{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "oriented-privilege",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the much needed stuff for training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "approved-spider",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_IpX</th>\n",
       "      <th>thumb_IpY</th>\n",
       "      <th>thumb_TipX</th>\n",
       "      <th>...</th>\n",
       "      <th>ring_TipX</th>\n",
       "      <th>ring_TipY</th>\n",
       "      <th>pinky_McpX</th>\n",
       "      <th>pinky_McpY</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>614.663780</td>\n",
       "      <td>905.806620</td>\n",
       "      <td>549.961701</td>\n",
       "      <td>910.559009</td>\n",
       "      <td>484.506458</td>\n",
       "      <td>895.301416</td>\n",
       "      <td>446.415693</td>\n",
       "      <td>872.791400</td>\n",
       "      <td>435.599104</td>\n",
       "      <td>...</td>\n",
       "      <td>597.933635</td>\n",
       "      <td>906.572645</td>\n",
       "      <td>621.966362</td>\n",
       "      <td>825.802557</td>\n",
       "      <td>617.009506</td>\n",
       "      <td>835.676272</td>\n",
       "      <td>628.267184</td>\n",
       "      <td>863.719888</td>\n",
       "      <td>636.857986</td>\n",
       "      <td>880.355752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>1586.340550</td>\n",
       "      <td>1973.989626</td>\n",
       "      <td>1321.859984</td>\n",
       "      <td>1894.983892</td>\n",
       "      <td>1098.946306</td>\n",
       "      <td>1643.605182</td>\n",
       "      <td>1026.106686</td>\n",
       "      <td>1423.268890</td>\n",
       "      <td>1142.311011</td>\n",
       "      <td>...</td>\n",
       "      <td>1599.906293</td>\n",
       "      <td>1636.526762</td>\n",
       "      <td>1844.856026</td>\n",
       "      <td>1433.191668</td>\n",
       "      <td>1677.424247</td>\n",
       "      <td>1448.975126</td>\n",
       "      <td>1652.730848</td>\n",
       "      <td>1627.551797</td>\n",
       "      <td>1734.164803</td>\n",
       "      <td>1648.017813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1586.658151</td>\n",
       "      <td>1899.797421</td>\n",
       "      <td>1347.672125</td>\n",
       "      <td>1834.859933</td>\n",
       "      <td>1150.592813</td>\n",
       "      <td>1586.523955</td>\n",
       "      <td>1096.266506</td>\n",
       "      <td>1351.857016</td>\n",
       "      <td>1162.683060</td>\n",
       "      <td>...</td>\n",
       "      <td>1619.031085</td>\n",
       "      <td>1549.667838</td>\n",
       "      <td>1807.934423</td>\n",
       "      <td>1360.078092</td>\n",
       "      <td>1668.181927</td>\n",
       "      <td>1352.136308</td>\n",
       "      <td>1661.550659</td>\n",
       "      <td>1533.032684</td>\n",
       "      <td>1746.261959</td>\n",
       "      <td>1546.756997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>1609.015491</td>\n",
       "      <td>1907.042126</td>\n",
       "      <td>1332.827688</td>\n",
       "      <td>1813.612083</td>\n",
       "      <td>1129.451521</td>\n",
       "      <td>1560.214301</td>\n",
       "      <td>1071.799619</td>\n",
       "      <td>1348.270438</td>\n",
       "      <td>1158.308507</td>\n",
       "      <td>...</td>\n",
       "      <td>1592.248868</td>\n",
       "      <td>1547.547122</td>\n",
       "      <td>1834.185079</td>\n",
       "      <td>1350.679505</td>\n",
       "      <td>1657.451330</td>\n",
       "      <td>1350.081768</td>\n",
       "      <td>1638.767816</td>\n",
       "      <td>1529.082674</td>\n",
       "      <td>1727.664467</td>\n",
       "      <td>1547.885221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>1650.991414</td>\n",
       "      <td>1806.711465</td>\n",
       "      <td>1384.922959</td>\n",
       "      <td>1740.992451</td>\n",
       "      <td>1179.722488</td>\n",
       "      <td>1506.995106</td>\n",
       "      <td>1119.851872</td>\n",
       "      <td>1279.860915</td>\n",
       "      <td>1177.426620</td>\n",
       "      <td>...</td>\n",
       "      <td>1628.596647</td>\n",
       "      <td>1483.385787</td>\n",
       "      <td>1807.114396</td>\n",
       "      <td>1288.052511</td>\n",
       "      <td>1687.552578</td>\n",
       "      <td>1268.021172</td>\n",
       "      <td>1683.299860</td>\n",
       "      <td>1451.865086</td>\n",
       "      <td>1756.480227</td>\n",
       "      <td>1475.592785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>Z</td>\n",
       "      <td>632.342100</td>\n",
       "      <td>1417.761445</td>\n",
       "      <td>511.266172</td>\n",
       "      <td>1452.934027</td>\n",
       "      <td>654.702663</td>\n",
       "      <td>1413.266897</td>\n",
       "      <td>992.956877</td>\n",
       "      <td>1352.535844</td>\n",
       "      <td>1304.301739</td>\n",
       "      <td>...</td>\n",
       "      <td>965.736210</td>\n",
       "      <td>1345.040798</td>\n",
       "      <td>1115.571737</td>\n",
       "      <td>1094.783783</td>\n",
       "      <td>1257.244349</td>\n",
       "      <td>1213.543057</td>\n",
       "      <td>1127.177358</td>\n",
       "      <td>1351.791143</td>\n",
       "      <td>998.408437</td>\n",
       "      <td>1384.162068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>Z</td>\n",
       "      <td>707.776546</td>\n",
       "      <td>1417.585611</td>\n",
       "      <td>533.494473</td>\n",
       "      <td>1449.532986</td>\n",
       "      <td>626.750469</td>\n",
       "      <td>1442.848206</td>\n",
       "      <td>935.930610</td>\n",
       "      <td>1423.299432</td>\n",
       "      <td>1238.717318</td>\n",
       "      <td>...</td>\n",
       "      <td>934.907556</td>\n",
       "      <td>1375.932932</td>\n",
       "      <td>1138.646483</td>\n",
       "      <td>1136.404514</td>\n",
       "      <td>1227.160931</td>\n",
       "      <td>1300.656319</td>\n",
       "      <td>1077.356815</td>\n",
       "      <td>1399.181604</td>\n",
       "      <td>964.920759</td>\n",
       "      <td>1406.220436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Z</td>\n",
       "      <td>484.730750</td>\n",
       "      <td>1203.294873</td>\n",
       "      <td>609.190941</td>\n",
       "      <td>1234.577775</td>\n",
       "      <td>845.985651</td>\n",
       "      <td>1188.504934</td>\n",
       "      <td>1064.718962</td>\n",
       "      <td>1230.792642</td>\n",
       "      <td>1245.759130</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.254124</td>\n",
       "      <td>1246.820688</td>\n",
       "      <td>937.975585</td>\n",
       "      <td>1005.628824</td>\n",
       "      <td>1131.682754</td>\n",
       "      <td>1035.978436</td>\n",
       "      <td>1161.693215</td>\n",
       "      <td>1084.606409</td>\n",
       "      <td>1170.148134</td>\n",
       "      <td>1053.281426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Z</td>\n",
       "      <td>445.235610</td>\n",
       "      <td>1298.323512</td>\n",
       "      <td>468.829751</td>\n",
       "      <td>1329.984903</td>\n",
       "      <td>677.648187</td>\n",
       "      <td>1299.878120</td>\n",
       "      <td>947.729409</td>\n",
       "      <td>1341.548443</td>\n",
       "      <td>1169.836998</td>\n",
       "      <td>...</td>\n",
       "      <td>903.057277</td>\n",
       "      <td>1266.618848</td>\n",
       "      <td>957.399011</td>\n",
       "      <td>1050.945640</td>\n",
       "      <td>1096.230984</td>\n",
       "      <td>1104.877949</td>\n",
       "      <td>1034.316659</td>\n",
       "      <td>1175.140142</td>\n",
       "      <td>995.093167</td>\n",
       "      <td>1144.306540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Z</td>\n",
       "      <td>550.345659</td>\n",
       "      <td>1311.316252</td>\n",
       "      <td>553.188920</td>\n",
       "      <td>1309.291124</td>\n",
       "      <td>761.222124</td>\n",
       "      <td>1277.625799</td>\n",
       "      <td>1033.439517</td>\n",
       "      <td>1299.137354</td>\n",
       "      <td>1267.425895</td>\n",
       "      <td>...</td>\n",
       "      <td>960.366845</td>\n",
       "      <td>1254.362702</td>\n",
       "      <td>1017.805338</td>\n",
       "      <td>1062.273979</td>\n",
       "      <td>1146.723390</td>\n",
       "      <td>1102.788806</td>\n",
       "      <td>1068.720341</td>\n",
       "      <td>1209.428310</td>\n",
       "      <td>998.680770</td>\n",
       "      <td>1223.415375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1033 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_type       wristX       wristY   thumb_CmcX   thumb_CmcY  \\\n",
       "0             A   614.663780   905.806620   549.961701   910.559009   \n",
       "1             A  1586.340550  1973.989626  1321.859984  1894.983892   \n",
       "2             A  1586.658151  1899.797421  1347.672125  1834.859933   \n",
       "3             A  1609.015491  1907.042126  1332.827688  1813.612083   \n",
       "4             A  1650.991414  1806.711465  1384.922959  1740.992451   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "1028          Z   632.342100  1417.761445   511.266172  1452.934027   \n",
       "1029          Z   707.776546  1417.585611   533.494473  1449.532986   \n",
       "1030          Z   484.730750  1203.294873   609.190941  1234.577775   \n",
       "1031          Z   445.235610  1298.323512   468.829751  1329.984903   \n",
       "1032          Z   550.345659  1311.316252   553.188920  1309.291124   \n",
       "\n",
       "       thumb_McpX   thumb_McpY    thumb_IpX    thumb_IpY   thumb_TipX  ...  \\\n",
       "0      484.506458   895.301416   446.415693   872.791400   435.599104  ...   \n",
       "1     1098.946306  1643.605182  1026.106686  1423.268890  1142.311011  ...   \n",
       "2     1150.592813  1586.523955  1096.266506  1351.857016  1162.683060  ...   \n",
       "3     1129.451521  1560.214301  1071.799619  1348.270438  1158.308507  ...   \n",
       "4     1179.722488  1506.995106  1119.851872  1279.860915  1177.426620  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1028   654.702663  1413.266897   992.956877  1352.535844  1304.301739  ...   \n",
       "1029   626.750469  1442.848206   935.930610  1423.299432  1238.717318  ...   \n",
       "1030   845.985651  1188.504934  1064.718962  1230.792642  1245.759130  ...   \n",
       "1031   677.648187  1299.878120   947.729409  1341.548443  1169.836998  ...   \n",
       "1032   761.222124  1277.625799  1033.439517  1299.137354  1267.425895  ...   \n",
       "\n",
       "        ring_TipX    ring_TipY   pinky_McpX   pinky_McpY   pinky_PipX  \\\n",
       "0      597.933635   906.572645   621.966362   825.802557   617.009506   \n",
       "1     1599.906293  1636.526762  1844.856026  1433.191668  1677.424247   \n",
       "2     1619.031085  1549.667838  1807.934423  1360.078092  1668.181927   \n",
       "3     1592.248868  1547.547122  1834.185079  1350.679505  1657.451330   \n",
       "4     1628.596647  1483.385787  1807.114396  1288.052511  1687.552578   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1028   965.736210  1345.040798  1115.571737  1094.783783  1257.244349   \n",
       "1029   934.907556  1375.932932  1138.646483  1136.404514  1227.160931   \n",
       "1030  1007.254124  1246.820688   937.975585  1005.628824  1131.682754   \n",
       "1031   903.057277  1266.618848   957.399011  1050.945640  1096.230984   \n",
       "1032   960.366845  1254.362702  1017.805338  1062.273979  1146.723390   \n",
       "\n",
       "       pinky_PipY   pinky_DipX   pinky_DipY   pinky_TipX   pinky_TipY  \n",
       "0      835.676272   628.267184   863.719888   636.857986   880.355752  \n",
       "1     1448.975126  1652.730848  1627.551797  1734.164803  1648.017813  \n",
       "2     1352.136308  1661.550659  1533.032684  1746.261959  1546.756997  \n",
       "3     1350.081768  1638.767816  1529.082674  1727.664467  1547.885221  \n",
       "4     1268.021172  1683.299860  1451.865086  1756.480227  1475.592785  \n",
       "...           ...          ...          ...          ...          ...  \n",
       "1028  1213.543057  1127.177358  1351.791143   998.408437  1384.162068  \n",
       "1029  1300.656319  1077.356815  1399.181604   964.920759  1406.220436  \n",
       "1030  1035.978436  1161.693215  1084.606409  1170.148134  1053.281426  \n",
       "1031  1104.877949  1034.316659  1175.140142   995.093167  1144.306540  \n",
       "1032  1102.788806  1068.720341  1209.428310   998.680770  1223.415375  \n",
       "\n",
       "[1033 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Training the model using Pandas\n",
    "df_train = pd.read_csv(\"hands_SIBI_training.csv\", header=0)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "julian-soldier",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_IpX</th>\n",
       "      <th>thumb_IpY</th>\n",
       "      <th>thumb_TipX</th>\n",
       "      <th>...</th>\n",
       "      <th>ring_TipX</th>\n",
       "      <th>ring_TipY</th>\n",
       "      <th>pinky_McpX</th>\n",
       "      <th>pinky_McpY</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1595.475198</td>\n",
       "      <td>1713.882845</td>\n",
       "      <td>1371.011287</td>\n",
       "      <td>1654.651115</td>\n",
       "      <td>1140.459070</td>\n",
       "      <td>1461.532063</td>\n",
       "      <td>1041.989141</td>\n",
       "      <td>1233.009396</td>\n",
       "      <td>1109.759125</td>\n",
       "      <td>...</td>\n",
       "      <td>1564.787345</td>\n",
       "      <td>1370.234713</td>\n",
       "      <td>1741.941414</td>\n",
       "      <td>1206.933278</td>\n",
       "      <td>1664.283545</td>\n",
       "      <td>1119.481146</td>\n",
       "      <td>1645.726819</td>\n",
       "      <td>1310.211016</td>\n",
       "      <td>1689.494687</td>\n",
       "      <td>1349.412327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>719.049096</td>\n",
       "      <td>1577.525616</td>\n",
       "      <td>508.609653</td>\n",
       "      <td>1392.832041</td>\n",
       "      <td>435.399145</td>\n",
       "      <td>1078.577757</td>\n",
       "      <td>453.572810</td>\n",
       "      <td>842.452109</td>\n",
       "      <td>486.912608</td>\n",
       "      <td>...</td>\n",
       "      <td>843.840003</td>\n",
       "      <td>1323.482871</td>\n",
       "      <td>1090.264320</td>\n",
       "      <td>1240.741253</td>\n",
       "      <td>1128.179789</td>\n",
       "      <td>1106.518865</td>\n",
       "      <td>1024.695754</td>\n",
       "      <td>1242.374778</td>\n",
       "      <td>977.687418</td>\n",
       "      <td>1327.082992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>757.780492</td>\n",
       "      <td>1601.105571</td>\n",
       "      <td>552.570701</td>\n",
       "      <td>1429.801345</td>\n",
       "      <td>481.703639</td>\n",
       "      <td>1114.173293</td>\n",
       "      <td>499.344498</td>\n",
       "      <td>878.003299</td>\n",
       "      <td>518.134713</td>\n",
       "      <td>...</td>\n",
       "      <td>877.598047</td>\n",
       "      <td>1347.837567</td>\n",
       "      <td>1113.693476</td>\n",
       "      <td>1258.446693</td>\n",
       "      <td>1158.711910</td>\n",
       "      <td>1131.860495</td>\n",
       "      <td>1060.676217</td>\n",
       "      <td>1270.025492</td>\n",
       "      <td>1010.870814</td>\n",
       "      <td>1352.582455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>615.760669</td>\n",
       "      <td>1031.698437</td>\n",
       "      <td>546.501115</td>\n",
       "      <td>1009.297512</td>\n",
       "      <td>496.653780</td>\n",
       "      <td>939.493802</td>\n",
       "      <td>477.223635</td>\n",
       "      <td>880.857238</td>\n",
       "      <td>457.223967</td>\n",
       "      <td>...</td>\n",
       "      <td>606.932983</td>\n",
       "      <td>927.758379</td>\n",
       "      <td>666.484147</td>\n",
       "      <td>879.113685</td>\n",
       "      <td>633.975327</td>\n",
       "      <td>843.605268</td>\n",
       "      <td>628.140047</td>\n",
       "      <td>895.670514</td>\n",
       "      <td>643.053487</td>\n",
       "      <td>913.554010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>322.910583</td>\n",
       "      <td>3155.554688</td>\n",
       "      <td>387.274112</td>\n",
       "      <td>3209.536560</td>\n",
       "      <td>392.819341</td>\n",
       "      <td>3277.465576</td>\n",
       "      <td>393.364716</td>\n",
       "      <td>3335.383026</td>\n",
       "      <td>401.043561</td>\n",
       "      <td>...</td>\n",
       "      <td>311.369056</td>\n",
       "      <td>3373.221863</td>\n",
       "      <td>272.978125</td>\n",
       "      <td>3264.753571</td>\n",
       "      <td>278.960250</td>\n",
       "      <td>3301.982941</td>\n",
       "      <td>294.675690</td>\n",
       "      <td>3319.776947</td>\n",
       "      <td>307.635242</td>\n",
       "      <td>3325.865021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Z</td>\n",
       "      <td>758.030560</td>\n",
       "      <td>2493.725372</td>\n",
       "      <td>555.423833</td>\n",
       "      <td>2410.149605</td>\n",
       "      <td>554.325989</td>\n",
       "      <td>2121.368992</td>\n",
       "      <td>785.251643</td>\n",
       "      <td>1939.650558</td>\n",
       "      <td>1009.190364</td>\n",
       "      <td>...</td>\n",
       "      <td>836.804911</td>\n",
       "      <td>1998.270870</td>\n",
       "      <td>878.059498</td>\n",
       "      <td>1776.177761</td>\n",
       "      <td>1103.563620</td>\n",
       "      <td>1768.856277</td>\n",
       "      <td>1011.302115</td>\n",
       "      <td>1940.822914</td>\n",
       "      <td>882.944364</td>\n",
       "      <td>1985.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Z</td>\n",
       "      <td>2576.749849</td>\n",
       "      <td>2868.969537</td>\n",
       "      <td>2432.240124</td>\n",
       "      <td>2800.074300</td>\n",
       "      <td>2270.718741</td>\n",
       "      <td>2749.498940</td>\n",
       "      <td>2145.062299</td>\n",
       "      <td>2784.001738</td>\n",
       "      <td>2058.201814</td>\n",
       "      <td>...</td>\n",
       "      <td>2296.587610</td>\n",
       "      <td>2895.431664</td>\n",
       "      <td>2441.918750</td>\n",
       "      <td>2826.890079</td>\n",
       "      <td>2278.390603</td>\n",
       "      <td>2947.817417</td>\n",
       "      <td>2309.023933</td>\n",
       "      <td>2981.885049</td>\n",
       "      <td>2355.511651</td>\n",
       "      <td>2958.717896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Z</td>\n",
       "      <td>1902.547185</td>\n",
       "      <td>2725.740898</td>\n",
       "      <td>1779.595316</td>\n",
       "      <td>2662.946720</td>\n",
       "      <td>1690.045661</td>\n",
       "      <td>2521.979805</td>\n",
       "      <td>1711.686770</td>\n",
       "      <td>2370.669079</td>\n",
       "      <td>1794.469457</td>\n",
       "      <td>...</td>\n",
       "      <td>1875.205433</td>\n",
       "      <td>2507.626522</td>\n",
       "      <td>2008.996679</td>\n",
       "      <td>2444.163872</td>\n",
       "      <td>1994.378469</td>\n",
       "      <td>2405.037231</td>\n",
       "      <td>1955.960378</td>\n",
       "      <td>2498.758759</td>\n",
       "      <td>1947.145678</td>\n",
       "      <td>2534.302185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Z</td>\n",
       "      <td>646.825790</td>\n",
       "      <td>1212.169051</td>\n",
       "      <td>674.274445</td>\n",
       "      <td>1242.027044</td>\n",
       "      <td>843.870282</td>\n",
       "      <td>1261.058211</td>\n",
       "      <td>1057.972789</td>\n",
       "      <td>1293.666124</td>\n",
       "      <td>1220.292091</td>\n",
       "      <td>...</td>\n",
       "      <td>1005.024076</td>\n",
       "      <td>1216.723204</td>\n",
       "      <td>1082.362056</td>\n",
       "      <td>1026.123166</td>\n",
       "      <td>1191.763639</td>\n",
       "      <td>1050.326347</td>\n",
       "      <td>1154.398918</td>\n",
       "      <td>1090.296507</td>\n",
       "      <td>1120.047092</td>\n",
       "      <td>1066.013694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Z</td>\n",
       "      <td>742.793560</td>\n",
       "      <td>1290.448785</td>\n",
       "      <td>756.768525</td>\n",
       "      <td>1313.876748</td>\n",
       "      <td>911.147416</td>\n",
       "      <td>1330.813885</td>\n",
       "      <td>1126.545787</td>\n",
       "      <td>1361.523628</td>\n",
       "      <td>1301.351786</td>\n",
       "      <td>...</td>\n",
       "      <td>1108.314395</td>\n",
       "      <td>1289.618134</td>\n",
       "      <td>1201.952457</td>\n",
       "      <td>1120.719910</td>\n",
       "      <td>1311.442971</td>\n",
       "      <td>1128.456712</td>\n",
       "      <td>1273.124337</td>\n",
       "      <td>1169.704914</td>\n",
       "      <td>1245.737553</td>\n",
       "      <td>1152.362823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_type       wristX       wristY   thumb_CmcX   thumb_CmcY  \\\n",
       "0            A  1595.475198  1713.882845  1371.011287  1654.651115   \n",
       "1            A   719.049096  1577.525616   508.609653  1392.832041   \n",
       "2            A   757.780492  1601.105571   552.570701  1429.801345   \n",
       "3            A   615.760669  1031.698437   546.501115  1009.297512   \n",
       "4            A   322.910583  3155.554688   387.274112  3209.536560   \n",
       "..         ...          ...          ...          ...          ...   \n",
       "199          Z   758.030560  2493.725372   555.423833  2410.149605   \n",
       "200          Z  2576.749849  2868.969537  2432.240124  2800.074300   \n",
       "201          Z  1902.547185  2725.740898  1779.595316  2662.946720   \n",
       "202          Z   646.825790  1212.169051   674.274445  1242.027044   \n",
       "203          Z   742.793560  1290.448785   756.768525  1313.876748   \n",
       "\n",
       "      thumb_McpX   thumb_McpY    thumb_IpX    thumb_IpY   thumb_TipX  ...  \\\n",
       "0    1140.459070  1461.532063  1041.989141  1233.009396  1109.759125  ...   \n",
       "1     435.399145  1078.577757   453.572810   842.452109   486.912608  ...   \n",
       "2     481.703639  1114.173293   499.344498   878.003299   518.134713  ...   \n",
       "3     496.653780   939.493802   477.223635   880.857238   457.223967  ...   \n",
       "4     392.819341  3277.465576   393.364716  3335.383026   401.043561  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "199   554.325989  2121.368992   785.251643  1939.650558  1009.190364  ...   \n",
       "200  2270.718741  2749.498940  2145.062299  2784.001738  2058.201814  ...   \n",
       "201  1690.045661  2521.979805  1711.686770  2370.669079  1794.469457  ...   \n",
       "202   843.870282  1261.058211  1057.972789  1293.666124  1220.292091  ...   \n",
       "203   911.147416  1330.813885  1126.545787  1361.523628  1301.351786  ...   \n",
       "\n",
       "       ring_TipX    ring_TipY   pinky_McpX   pinky_McpY   pinky_PipX  \\\n",
       "0    1564.787345  1370.234713  1741.941414  1206.933278  1664.283545   \n",
       "1     843.840003  1323.482871  1090.264320  1240.741253  1128.179789   \n",
       "2     877.598047  1347.837567  1113.693476  1258.446693  1158.711910   \n",
       "3     606.932983   927.758379   666.484147   879.113685   633.975327   \n",
       "4     311.369056  3373.221863   272.978125  3264.753571   278.960250   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "199   836.804911  1998.270870   878.059498  1776.177761  1103.563620   \n",
       "200  2296.587610  2895.431664  2441.918750  2826.890079  2278.390603   \n",
       "201  1875.205433  2507.626522  2008.996679  2444.163872  1994.378469   \n",
       "202  1005.024076  1216.723204  1082.362056  1026.123166  1191.763639   \n",
       "203  1108.314395  1289.618134  1201.952457  1120.719910  1311.442971   \n",
       "\n",
       "      pinky_PipY   pinky_DipX   pinky_DipY   pinky_TipX   pinky_TipY  \n",
       "0    1119.481146  1645.726819  1310.211016  1689.494687  1349.412327  \n",
       "1    1106.518865  1024.695754  1242.374778   977.687418  1327.082992  \n",
       "2    1131.860495  1060.676217  1270.025492  1010.870814  1352.582455  \n",
       "3     843.605268   628.140047   895.670514   643.053487   913.554010  \n",
       "4    3301.982941   294.675690  3319.776947   307.635242  3325.865021  \n",
       "..           ...          ...          ...          ...          ...  \n",
       "199  1768.856277  1011.302115  1940.822914   882.944364  1985.800049  \n",
       "200  2947.817417  2309.023933  2981.885049  2355.511651  2958.717896  \n",
       "201  2405.037231  1955.960378  2498.758759  1947.145678  2534.302185  \n",
       "202  1050.326347  1154.398918  1090.296507  1120.047092  1066.013694  \n",
       "203  1128.456712  1273.124337  1169.704914  1245.737553  1152.362823  \n",
       "\n",
       "[204 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Validation or Testing the Model using Pandas\n",
    "df_test = pd.read_csv(\"hands_SIBI_validation.csv\", header=0)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "assumed-catering",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Categorical using Pandas\n",
    "df_train[\"class_type\"] = pd.Categorical(df_train[\"class_type\"])\n",
    "df_train[\"class_type\"] = df_train.class_type.cat.codes\n",
    "\n",
    "df_test[\"class_type\"] = pd.Categorical(df_test[\"class_type\"])\n",
    "df_test[\"class_type\"] = df_test.class_type.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "suited-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Label and Feature for training\n",
    "y_train = df_train.pop(\"class_type\")\n",
    "x_train = df_train.copy()\n",
    "\n",
    "y_test = df_test.pop(\"class_type\")\n",
    "x_test = df_test.copy()\n",
    "\n",
    "# Copied Features turn to Array by using NumPy\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bigger-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033, 42)\n",
      "(204, 42)\n",
      "(1033, 42, 1)\n",
      "(204, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check Array Shape before transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Since the array shape is 1x1, we must turn it into 1x10x1 so we can feed it into the model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Check Array Shape after transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hollywood-prevention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[614.664]\n",
      " [905.807]\n",
      " [549.962]\n",
      " [910.559]\n",
      " [484.506]\n",
      " [895.301]\n",
      " [446.416]\n",
      " [872.791]\n",
      " [435.599]\n",
      " [842.709]\n",
      " [492.176]\n",
      " [845.125]\n",
      " [455.925]\n",
      " [844.86 ]\n",
      " [478.732]\n",
      " [890.223]\n",
      " [501.187]\n",
      " [922.558]\n",
      " [536.641]\n",
      " [826.821]\n",
      " [510.607]\n",
      " [840.276]\n",
      " [530.386]\n",
      " [889.723]\n",
      " [550.536]\n",
      " [916.596]\n",
      " [581.733]\n",
      " [822.1  ]\n",
      " [561.356]\n",
      " [835.389]\n",
      " [580.989]\n",
      " [882.792]\n",
      " [597.934]\n",
      " [906.573]\n",
      " [621.966]\n",
      " [825.803]\n",
      " [617.01 ]\n",
      " [835.676]\n",
      " [628.267]\n",
      " [863.72 ]\n",
      " [636.858]\n",
      " [880.356]]\n",
      "[[2667.218]\n",
      " [1947.497]\n",
      " [2643.249]\n",
      " [1810.535]\n",
      " [2597.547]\n",
      " [1667.531]\n",
      " [2558.04 ]\n",
      " [1543.697]\n",
      " [2516.062]\n",
      " [1443.765]\n",
      " [2626.635]\n",
      " [1594.4  ]\n",
      " [2726.339]\n",
      " [1509.7  ]\n",
      " [2755.825]\n",
      " [1631.71 ]\n",
      " [2742.401]\n",
      " [1712.601]\n",
      " [2634.353]\n",
      " [1601.582]\n",
      " [2737.23 ]\n",
      " [1524.637]\n",
      " [2755.177]\n",
      " [1665.086]\n",
      " [2742.176]\n",
      " [1736.505]\n",
      " [2640.166]\n",
      " [1617.782]\n",
      " [2727.282]\n",
      " [1520.309]\n",
      " [2741.725]\n",
      " [1637.303]\n",
      " [2723.755]\n",
      " [1704.204]\n",
      " [2640.379]\n",
      " [1641.465]\n",
      " [2662.197]\n",
      " [1533.31 ]\n",
      " [2656.062]\n",
      " [1516.998]\n",
      " [2638.775]\n",
      " [1495.61 ]]\n"
     ]
    }
   ],
   "source": [
    "# Check sample train and test features\n",
    "print(x_train[0])\n",
    "print(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "mobile-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes according standard Alphabets\n",
    "num_classes = 26\n",
    "\n",
    "# Using the Keras.Utils to put the label categorically \n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "nuclear-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 42, 32)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 42, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 21, 64)            10304     \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 10, 128)           41088     \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 10, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 5, 256)            164096    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 5, 256)            327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 927,354\n",
      "Trainable params: 927,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One Dimensional Convolutional Neural Network model, Train will be feed to 1 Dimension Convolutional Neural Network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "seven-crest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "33/33 [==============================] - 3s 99ms/step - loss: 3.5478 - accuracy: 0.0339 - val_loss: 3.2467 - val_accuracy: 0.0392\n",
      "Epoch 2/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 3.2047 - accuracy: 0.0445 - val_loss: 3.0821 - val_accuracy: 0.1078\n",
      "Epoch 3/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 2.9243 - accuracy: 0.1123 - val_loss: 2.7484 - val_accuracy: 0.1373\n",
      "Epoch 4/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 2.6231 - accuracy: 0.1704 - val_loss: 2.7129 - val_accuracy: 0.1471\n",
      "Epoch 5/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 2.5070 - accuracy: 0.2062 - val_loss: 2.4693 - val_accuracy: 0.1814\n",
      "Epoch 6/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 2.2272 - accuracy: 0.2740 - val_loss: 2.2658 - val_accuracy: 0.3088\n",
      "Epoch 7/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 1.9858 - accuracy: 0.3524 - val_loss: 2.0750 - val_accuracy: 0.3676loss: 2.0201 - accura\n",
      "Epoch 8/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 1.7336 - accuracy: 0.4308 - val_loss: 1.9942 - val_accuracy: 0.3873\n",
      "Epoch 9/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.6665 - accuracy: 0.4579 - val_loss: 1.7547 - val_accuracy: 0.4020\n",
      "Epoch 10/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.5486 - accuracy: 0.4734 - val_loss: 1.7299 - val_accuracy: 0.4461\n",
      "Epoch 11/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 1.4156 - accuracy: 0.5286 - val_loss: 1.8553 - val_accuracy: 0.4363\n",
      "Epoch 12/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 1.3846 - accuracy: 0.5382 - val_loss: 1.7431 - val_accuracy: 0.4657\n",
      "Epoch 13/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 1.2856 - accuracy: 0.5586 - val_loss: 1.4734 - val_accuracy: 0.5735\n",
      "Epoch 14/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 1.1975 - accuracy: 0.5944 - val_loss: 1.4018 - val_accuracy: 0.5392\n",
      "Epoch 15/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 1.0856 - accuracy: 0.6370 - val_loss: 1.4271 - val_accuracy: 0.5441\n",
      "Epoch 16/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.9982 - accuracy: 0.6592 - val_loss: 1.3457 - val_accuracy: 0.6275\n",
      "Epoch 17/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.9581 - accuracy: 0.6796 - val_loss: 1.3836 - val_accuracy: 0.6029\n",
      "Epoch 18/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.9179 - accuracy: 0.6854 - val_loss: 1.3867 - val_accuracy: 0.5931\n",
      "Epoch 19/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.8949 - accuracy: 0.7047 - val_loss: 1.2998 - val_accuracy: 0.6078\n",
      "Epoch 20/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.8314 - accuracy: 0.7125 - val_loss: 1.3957 - val_accuracy: 0.6029\n",
      "Epoch 21/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.8076 - accuracy: 0.7212 - val_loss: 1.2389 - val_accuracy: 0.6127\n",
      "Epoch 22/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.7630 - accuracy: 0.7444 - val_loss: 1.3336 - val_accuracy: 0.6324\n",
      "Epoch 23/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.6943 - accuracy: 0.7667 - val_loss: 1.3079 - val_accuracy: 0.6324\n",
      "Epoch 24/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.6447 - accuracy: 0.7841 - val_loss: 1.1646 - val_accuracy: 0.7059\n",
      "Epoch 25/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.6030 - accuracy: 0.7793 - val_loss: 1.3466 - val_accuracy: 0.6814\n",
      "Epoch 26/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.5961 - accuracy: 0.8006 - val_loss: 1.2499 - val_accuracy: 0.6471\n",
      "Epoch 27/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.6063 - accuracy: 0.7957 - val_loss: 1.3026 - val_accuracy: 0.6863\n",
      "Epoch 28/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.5673 - accuracy: 0.8064 - val_loss: 1.4928 - val_accuracy: 0.6373\n",
      "Epoch 29/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.5192 - accuracy: 0.8190 - val_loss: 1.3500 - val_accuracy: 0.6814\n",
      "Epoch 30/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.5943 - accuracy: 0.7890 - val_loss: 1.2138 - val_accuracy: 0.6863\n",
      "Epoch 31/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.4867 - accuracy: 0.8335 - val_loss: 1.2790 - val_accuracy: 0.6814\n",
      "Epoch 32/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.5890 - accuracy: 0.8035 - val_loss: 1.2236 - val_accuracy: 0.7059\n",
      "Epoch 33/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.5973 - accuracy: 0.8015 - val_loss: 1.3972 - val_accuracy: 0.6765\n",
      "Epoch 34/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.4633 - accuracy: 0.8287 - val_loss: 1.2123 - val_accuracy: 0.7059\n",
      "Epoch 35/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.4758 - accuracy: 0.8422 - val_loss: 1.3555 - val_accuracy: 0.6863\n",
      "Epoch 36/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.4354 - accuracy: 0.8451 - val_loss: 1.2742 - val_accuracy: 0.7255\n",
      "Epoch 37/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.3989 - accuracy: 0.8606 - val_loss: 1.3263 - val_accuracy: 0.7255\n",
      "Epoch 38/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.3608 - accuracy: 0.8819 - val_loss: 1.4288 - val_accuracy: 0.6765\n",
      "Epoch 39/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.4985 - accuracy: 0.8354 - val_loss: 1.4385 - val_accuracy: 0.6814\n",
      "Epoch 40/500\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.4702 - accuracy: 0.8364 - val_loss: 1.3519 - val_accuracy: 0.6961\n",
      "Epoch 41/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.3952 - accuracy: 0.8529 - val_loss: 1.4641 - val_accuracy: 0.7108\n",
      "Epoch 42/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.4554 - accuracy: 0.8403 - val_loss: 1.4677 - val_accuracy: 0.7353\n",
      "Epoch 43/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.3792 - accuracy: 0.8577 - val_loss: 1.2464 - val_accuracy: 0.7206\n",
      "Epoch 44/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.3088 - accuracy: 0.8896 - val_loss: 1.3400 - val_accuracy: 0.7549\n",
      "Epoch 45/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.3239 - accuracy: 0.8858 - val_loss: 1.5114 - val_accuracy: 0.7353\n",
      "Epoch 46/500\n",
      "33/33 [==============================] - 1s 41ms/step - loss: 0.2618 - accuracy: 0.9032 - val_loss: 1.5758 - val_accuracy: 0.7304\n",
      "Epoch 47/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.2469 - accuracy: 0.9187 - val_loss: 1.4385 - val_accuracy: 0.7402\n",
      "Epoch 48/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.2241 - accuracy: 0.9187 - val_loss: 1.6574 - val_accuracy: 0.7255\n",
      "Epoch 49/500\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2193 - accuracy: 0.9158 - val_loss: 1.6105 - val_accuracy: 0.7500\n",
      "Epoch 50/500\n",
      "33/33 [==============================] - 2s 46ms/step - loss: 0.2340 - accuracy: 0.9090 - val_loss: 1.6253 - val_accuracy: 0.7598\n",
      "Epoch 51/500\n",
      "33/33 [==============================] - 2s 47ms/step - loss: 0.2741 - accuracy: 0.9109 - val_loss: 1.6711 - val_accuracy: 0.7108\n",
      "Epoch 52/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.2663 - accuracy: 0.9129 - val_loss: 1.7330 - val_accuracy: 0.7059\n",
      "Epoch 53/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.3985 - accuracy: 0.8751 - val_loss: 1.5709 - val_accuracy: 0.7255\n",
      "Epoch 54/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.3616 - accuracy: 0.8838 - val_loss: 1.4637 - val_accuracy: 0.7108\n",
      "Epoch 55/500\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 0.2820 - accuracy: 0.9013 - val_loss: 1.5605 - val_accuracy: 0.7451\n",
      "Epoch 56/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1809 - accuracy: 0.9342 - val_loss: 2.0574 - val_accuracy: 0.7206\n",
      "Epoch 57/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.1760 - accuracy: 0.9351 - val_loss: 1.7864 - val_accuracy: 0.7549\n",
      "Epoch 58/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.2634 - accuracy: 0.9206 - val_loss: 1.9528 - val_accuracy: 0.7255\n",
      "Epoch 59/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.3845 - accuracy: 0.8761 - val_loss: 1.5495 - val_accuracy: 0.7108\n",
      "Epoch 60/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2991 - accuracy: 0.8935 - val_loss: 1.2759 - val_accuracy: 0.7353\n",
      "Epoch 61/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2355 - accuracy: 0.9167 - val_loss: 1.3797 - val_accuracy: 0.7206\n",
      "Epoch 62/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1958 - accuracy: 0.9332 - val_loss: 1.5872 - val_accuracy: 0.7549\n",
      "Epoch 63/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1535 - accuracy: 0.9409 - val_loss: 1.5417 - val_accuracy: 0.7647\n",
      "Epoch 64/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2024 - accuracy: 0.9380 - val_loss: 1.6960 - val_accuracy: 0.7010\n",
      "Epoch 65/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2476 - accuracy: 0.9284 - val_loss: 1.6559 - val_accuracy: 0.7500\n",
      "Epoch 66/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.2251 - accuracy: 0.9255 - val_loss: 1.4320 - val_accuracy: 0.7451\n",
      "Epoch 67/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1574 - accuracy: 0.9400 - val_loss: 1.4932 - val_accuracy: 0.7598\n",
      "Epoch 68/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1730 - accuracy: 0.9419 - val_loss: 1.5595 - val_accuracy: 0.7500\n",
      "Epoch 69/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.2507 - accuracy: 0.92 - 1s 31ms/step - loss: 0.2501 - accuracy: 0.9245 - val_loss: 1.6764 - val_accuracy: 0.7549\n",
      "Epoch 70/500\n",
      "33/33 [==============================] - 1s 41ms/step - loss: 0.2726 - accuracy: 0.9100 - val_loss: 1.7742 - val_accuracy: 0.7059\n",
      "Epoch 71/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.4008 - accuracy: 0.8751 - val_loss: 1.2205 - val_accuracy: 0.7402\n",
      "Epoch 72/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.2079 - accuracy: 0.9235 - val_loss: 1.6581 - val_accuracy: 0.7451\n",
      "Epoch 73/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1795 - accuracy: 0.9448 - val_loss: 1.6701 - val_accuracy: 0.7647\n",
      "Epoch 74/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1925 - accuracy: 0.9439 - val_loss: 1.8405 - val_accuracy: 0.7451\n",
      "Epoch 75/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2927 - accuracy: 0.9013 - val_loss: 1.6149 - val_accuracy: 0.7304\n",
      "Epoch 76/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2073 - accuracy: 0.9274 - val_loss: 2.0641 - val_accuracy: 0.7304\n",
      "Epoch 77/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.2304 - accuracy: 0.92 - 1s 29ms/step - loss: 0.2326 - accuracy: 0.9264 - val_loss: 1.5030 - val_accuracy: 0.7745\n",
      "Epoch 78/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2935 - accuracy: 0.9119 - val_loss: 1.5804 - val_accuracy: 0.7304\n",
      "Epoch 79/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.3641 - accuracy: 0.8838 - val_loss: 1.4259 - val_accuracy: 0.7304\n",
      "Epoch 80/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1470 - accuracy: 0.9458 - val_loss: 1.7034 - val_accuracy: 0.7647\n",
      "Epoch 81/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1098 - accuracy: 0.9564 - val_loss: 1.6122 - val_accuracy: 0.7353\n",
      "Epoch 82/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1294 - accuracy: 0.9497 - val_loss: 1.7095 - val_accuracy: 0.7549\n",
      "Epoch 83/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1857 - accuracy: 0.9351 - val_loss: 1.7038 - val_accuracy: 0.7549\n",
      "Epoch 84/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2325 - accuracy: 0.9332 - val_loss: 1.4005 - val_accuracy: 0.7647\n",
      "Epoch 85/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1645 - accuracy: 0.9458 - val_loss: 1.5804 - val_accuracy: 0.7451\n",
      "Epoch 86/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.1074 - accuracy: 0.9584 - val_loss: 1.7220 - val_accuracy: 0.7402\n",
      "Epoch 87/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1062 - accuracy: 0.9632 - val_loss: 1.6699 - val_accuracy: 0.7892ss: 0.1070 - accuracy: 0.96\n",
      "Epoch 88/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1361 - accuracy: 0.9506 - val_loss: 1.9641 - val_accuracy: 0.7941\n",
      "Epoch 89/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1523 - accuracy: 0.9477 - val_loss: 1.6918 - val_accuracy: 0.7500\n",
      "Epoch 90/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1516 - accuracy: 0.9458 - val_loss: 1.6151 - val_accuracy: 0.7500\n",
      "Epoch 91/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2084 - accuracy: 0.9361 - val_loss: 1.5833 - val_accuracy: 0.7206\n",
      "Epoch 92/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2341 - accuracy: 0.9342 - val_loss: 1.8446 - val_accuracy: 0.6961\n",
      "Epoch 93/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.2199 - accuracy: 0.9371 - val_loss: 1.7360 - val_accuracy: 0.7402\n",
      "Epoch 94/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1974 - accuracy: 0.9439 - val_loss: 1.7060 - val_accuracy: 0.6863\n",
      "Epoch 95/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2788 - accuracy: 0.9090 - val_loss: 1.3146 - val_accuracy: 0.7304\n",
      "Epoch 96/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.1518 - accuracy: 0.9584 - val_loss: 1.4296 - val_accuracy: 0.7598\n",
      "Epoch 97/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1896 - accuracy: 0.9371 - val_loss: 1.6803 - val_accuracy: 0.7696\n",
      "Epoch 98/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1640 - accuracy: 0.9477 - val_loss: 2.0115 - val_accuracy: 0.7451\n",
      "Epoch 99/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1635 - accuracy: 0.9448 - val_loss: 1.7167 - val_accuracy: 0.7598\n",
      "Epoch 100/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1527 - accuracy: 0.9468 - val_loss: 1.6601 - val_accuracy: 0.7549\n",
      "Epoch 101/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1277 - accuracy: 0.9574 - val_loss: 1.6157 - val_accuracy: 0.7598\n",
      "Epoch 102/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2944 - accuracy: 0.9206 - val_loss: 1.5042 - val_accuracy: 0.7696\n",
      "Epoch 103/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1929 - accuracy: 0.9351 - val_loss: 1.3847 - val_accuracy: 0.7598\n",
      "Epoch 104/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1779 - accuracy: 0.9409 - val_loss: 1.4513 - val_accuracy: 0.7892\n",
      "Epoch 105/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0877 - accuracy: 0.9729 - val_loss: 1.6420 - val_accuracy: 0.7843\n",
      "Epoch 106/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0738 - accuracy: 0.9710 - val_loss: 1.7238 - val_accuracy: 0.7843\n",
      "Epoch 107/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0733 - accuracy: 0.9710 - val_loss: 1.8030 - val_accuracy: 0.7843\n",
      "Epoch 108/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0648 - accuracy: 0.9768 - val_loss: 1.7769 - val_accuracy: 0.7843\n",
      "Epoch 109/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0619 - accuracy: 0.9748 - val_loss: 1.6962 - val_accuracy: 0.7941\n",
      "Epoch 110/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0899 - accuracy: 0.9671 - val_loss: 1.7417 - val_accuracy: 0.7843\n",
      "Epoch 111/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1410 - accuracy: 0.9506 - val_loss: 1.8903 - val_accuracy: 0.7745\n",
      "Epoch 112/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1735 - accuracy: 0.9574 - val_loss: 1.4513 - val_accuracy: 0.7990\n",
      "Epoch 113/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.1924 - accuracy: 0.9390 - val_loss: 1.8690 - val_accuracy: 0.7500\n",
      "Epoch 114/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2107 - accuracy: 0.9409 - val_loss: 1.3764 - val_accuracy: 0.7745\n",
      "Epoch 115/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0804 - accuracy: 0.9690 - val_loss: 1.6022 - val_accuracy: 0.7696\n",
      "Epoch 116/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1200 - accuracy: 0.9593 - val_loss: 1.7688 - val_accuracy: 0.7794\n",
      "Epoch 117/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0959 - accuracy: 0.9710 - val_loss: 1.7191 - val_accuracy: 0.7794\n",
      "Epoch 118/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2539 - accuracy: 0.9216 - val_loss: 1.6613 - val_accuracy: 0.7010\n",
      "Epoch 119/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.1948 - accuracy: 0.9322 - val_loss: 2.1744 - val_accuracy: 0.7598\n",
      "Epoch 120/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0819 - accuracy: 0.9681 - val_loss: 2.2715 - val_accuracy: 0.7647\n",
      "Epoch 121/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0591 - accuracy: 0.9806 - val_loss: 2.3270 - val_accuracy: 0.7696\n",
      "Epoch 122/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0509 - accuracy: 0.9826 - val_loss: 2.3393 - val_accuracy: 0.7500\n",
      "Epoch 123/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1694 - accuracy: 0.9506 - val_loss: 2.0474 - val_accuracy: 0.7402\n",
      "Epoch 124/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2119 - accuracy: 0.9313 - val_loss: 2.0929 - val_accuracy: 0.7304\n",
      "Epoch 125/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.2003 - accuracy: 0.9351 - val_loss: 2.0758 - val_accuracy: 0.7402\n",
      "Epoch 126/500\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1388 - accuracy: 0.9555 - val_loss: 1.9730 - val_accuracy: 0.7647\n",
      "Epoch 127/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.1245 - accuracy: 0.9593 - val_loss: 1.7918 - val_accuracy: 0.7794\n",
      "Epoch 128/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1588 - accuracy: 0.9555 - val_loss: 1.8712 - val_accuracy: 0.7745\n",
      "Epoch 129/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1525 - accuracy: 0.9526 - val_loss: 1.5665 - val_accuracy: 0.7941\n",
      "Epoch 130/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1536 - accuracy: 0.9545 - val_loss: 1.9471 - val_accuracy: 0.7255\n",
      "Epoch 131/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1624 - accuracy: 0.9419 - val_loss: 2.9640 - val_accuracy: 0.7206\n",
      "Epoch 132/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.3025 - accuracy: 0.92 - 1s 31ms/step - loss: 0.3025 - accuracy: 0.9235 - val_loss: 1.6434 - val_accuracy: 0.7255\n",
      "Epoch 133/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.3408 - accuracy: 0.9051 - val_loss: 1.5080 - val_accuracy: 0.7647\n",
      "Epoch 134/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1735 - accuracy: 0.9477 - val_loss: 1.6818 - val_accuracy: 0.7794\n",
      "Epoch 135/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2831 - accuracy: 0.9255 - val_loss: 1.7328 - val_accuracy: 0.7255\n",
      "Epoch 136/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2569 - accuracy: 0.9235 - val_loss: 2.0270 - val_accuracy: 0.7304\n",
      "Epoch 137/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1490 - accuracy: 0.9458 - val_loss: 1.4842 - val_accuracy: 0.7794\n",
      "Epoch 138/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1123 - accuracy: 0.9593 - val_loss: 1.9217 - val_accuracy: 0.7500\n",
      "Epoch 139/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.0928 - accuracy: 0.9710 - val_loss: 1.7195 - val_accuracy: 0.7843\n",
      "Epoch 140/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1531 - accuracy: 0.9545 - val_loss: 1.7605 - val_accuracy: 0.7745\n",
      "Epoch 141/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1416 - accuracy: 0.9458 - val_loss: 1.8437 - val_accuracy: 0.7745\n",
      "Epoch 142/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1867 - accuracy: 0.9477 - val_loss: 1.7457 - val_accuracy: 0.7402\n",
      "Epoch 143/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1745 - accuracy: 0.9429 - val_loss: 1.7299 - val_accuracy: 0.7255\n",
      "Epoch 144/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2041 - accuracy: 0.9313 - val_loss: 1.5287 - val_accuracy: 0.7745\n",
      "Epoch 145/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1326 - accuracy: 0.9516 - val_loss: 1.6881 - val_accuracy: 0.7500\n",
      "Epoch 146/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1162 - accuracy: 0.9613 - val_loss: 1.7977 - val_accuracy: 0.7647\n",
      "Epoch 147/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1287 - accuracy: 0.9613 - val_loss: 1.5960 - val_accuracy: 0.7549\n",
      "Epoch 148/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1133 - accuracy: 0.9652 - val_loss: 1.6581 - val_accuracy: 0.7745\n",
      "Epoch 149/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0648 - accuracy: 0.9835 - val_loss: 1.7745 - val_accuracy: 0.7843\n",
      "Epoch 150/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0554 - accuracy: 0.9787 - val_loss: 1.8499 - val_accuracy: 0.7745\n",
      "Epoch 151/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0444 - accuracy: 0.9816 - val_loss: 2.0055 - val_accuracy: 0.7745\n",
      "Epoch 152/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0684 - accuracy: 0.9777 - val_loss: 1.6393 - val_accuracy: 0.7990\n",
      "Epoch 153/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0745 - accuracy: 0.9758 - val_loss: 1.5056 - val_accuracy: 0.7843\n",
      "Epoch 154/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.1079 - accuracy: 0.9671 - val_loss: 1.6079 - val_accuracy: 0.7843\n",
      "Epoch 155/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1822 - accuracy: 0.9516 - val_loss: 1.6665 - val_accuracy: 0.7500\n",
      "Epoch 156/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1127 - accuracy: 0.9555 - val_loss: 1.7163 - val_accuracy: 0.7598\n",
      "Epoch 157/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0757 - accuracy: 0.9700 - val_loss: 1.8816 - val_accuracy: 0.7892\n",
      "Epoch 158/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0537 - accuracy: 0.9777 - val_loss: 1.7306 - val_accuracy: 0.7990\n",
      "Epoch 159/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0632 - accuracy: 0.9797 - val_loss: 2.0303 - val_accuracy: 0.7647acy: 0.97\n",
      "Epoch 160/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0718 - accuracy: 0.9690 - val_loss: 1.9989 - val_accuracy: 0.7549\n",
      "Epoch 161/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0883 - accuracy: 0.9719 - val_loss: 2.6003 - val_accuracy: 0.7598\n",
      "Epoch 162/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2430 - accuracy: 0.9390 - val_loss: 1.6904 - val_accuracy: 0.7255\n",
      "Epoch 163/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.3182 - accuracy: 0.9119 - val_loss: 1.3427 - val_accuracy: 0.7941\n",
      "Epoch 164/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.2009 - accuracy: 0.9497 - val_loss: 1.5225 - val_accuracy: 0.7696\n",
      "Epoch 165/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1079 - accuracy: 0.9671 - val_loss: 1.7396 - val_accuracy: 0.7451\n",
      "Epoch 166/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0872 - accuracy: 0.9748 - val_loss: 1.6392 - val_accuracy: 0.7892\n",
      "Epoch 167/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0602 - accuracy: 0.9768 - val_loss: 1.6111 - val_accuracy: 0.7843\n",
      "Epoch 168/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0911 - accuracy: 0.96 - 1s 29ms/step - loss: 0.0904 - accuracy: 0.9671 - val_loss: 1.8206 - val_accuracy: 0.7451\n",
      "Epoch 169/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1195 - accuracy: 0.9700 - val_loss: 1.8033 - val_accuracy: 0.7451\n",
      "Epoch 170/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0929 - accuracy: 0.9700 - val_loss: 1.8683 - val_accuracy: 0.7451\n",
      "Epoch 171/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1406 - accuracy: 0.9622 - val_loss: 1.5388 - val_accuracy: 0.7402\n",
      "Epoch 172/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1941 - accuracy: 0.9458 - val_loss: 1.6884 - val_accuracy: 0.7598\n",
      "Epoch 173/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.1054 - accuracy: 0.9642 - val_loss: 1.7940 - val_accuracy: 0.7500\n",
      "Epoch 174/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1505 - accuracy: 0.9545 - val_loss: 1.5255 - val_accuracy: 0.7696\n",
      "Epoch 175/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.0901 - accuracy: 0.9690 - val_loss: 1.6329 - val_accuracy: 0.7549\n",
      "Epoch 176/500\n",
      "33/33 [==============================] - 2s 48ms/step - loss: 0.1629 - accuracy: 0.9593 - val_loss: 1.6016 - val_accuracy: 0.7353\n",
      "Epoch 177/500\n",
      "33/33 [==============================] - 1s 45ms/step - loss: 0.1751 - accuracy: 0.9419 - val_loss: 1.8146 - val_accuracy: 0.7647\n",
      "Epoch 178/500\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.1491 - accuracy: 0.9497 - val_loss: 1.7257 - val_accuracy: 0.7500\n",
      "Epoch 179/500\n",
      "33/33 [==============================] - 2s 68ms/step - loss: 0.1094 - accuracy: 0.9690 - val_loss: 1.9316 - val_accuracy: 0.7598\n",
      "Epoch 180/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0603 - accuracy: 0.9787 - val_loss: 1.8576 - val_accuracy: 0.7549\n",
      "Epoch 181/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1413 - accuracy: 0.9584 - val_loss: 1.9718 - val_accuracy: 0.7647\n",
      "Epoch 182/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.2031 - accuracy: 0.9448 - val_loss: 2.1045 - val_accuracy: 0.7500\n",
      "Epoch 183/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1094 - accuracy: 0.9584 - val_loss: 2.0861 - val_accuracy: 0.7549\n",
      "Epoch 184/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0727 - accuracy: 0.9777 - val_loss: 2.0092 - val_accuracy: 0.7696\n",
      "Epoch 185/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0543 - accuracy: 0.9826 - val_loss: 2.2770 - val_accuracy: 0.7598 0s - loss: 0.0577 - accura\n",
      "Epoch 186/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1068 - accuracy: 0.9661 - val_loss: 2.1454 - val_accuracy: 0.7647\n",
      "Epoch 187/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1584 - accuracy: 0.9652 - val_loss: 1.4425 - val_accuracy: 0.7794\n",
      "Epoch 188/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.1285 - accuracy: 0.9574 - val_loss: 1.6551 - val_accuracy: 0.7745\n",
      "Epoch 189/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1555 - accuracy: 0.9535 - val_loss: 1.6080 - val_accuracy: 0.7549\n",
      "Epoch 190/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.1035 - accuracy: 0.9661 - val_loss: 1.8290 - val_accuracy: 0.7451\n",
      "Epoch 191/500\n",
      "33/33 [==============================] - 3s 84ms/step - loss: 0.0958 - accuracy: 0.9690 - val_loss: 1.7556 - val_accuracy: 0.7745\n",
      "Epoch 192/500\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.1383 - accuracy: 0.9642 - val_loss: 1.7416 - val_accuracy: 0.7598\n",
      "Epoch 193/500\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.1502 - accuracy: 0.9574 - val_loss: 1.3803 - val_accuracy: 0.7696\n",
      "Epoch 194/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.1337 - accuracy: 0.9671 - val_loss: 1.3485 - val_accuracy: 0.7794\n",
      "Epoch 195/500\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0733 - accuracy: 0.9681 - val_loss: 1.5375 - val_accuracy: 0.7598\n",
      "Epoch 196/500\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0503 - accuracy: 0.9835 - val_loss: 1.8254 - val_accuracy: 0.7647\n",
      "Epoch 197/500\n",
      "33/33 [==============================] - 1s 45ms/step - loss: 0.0333 - accuracy: 0.9874 - val_loss: 1.8928 - val_accuracy: 0.7647\n",
      "Epoch 198/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.0316 - accuracy: 0.9874 - val_loss: 1.9435 - val_accuracy: 0.7549\n",
      "Epoch 199/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.1179 - accuracy: 0.9739 - val_loss: 1.7299 - val_accuracy: 0.7745\n",
      "Epoch 200/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1153 - accuracy: 0.9681 - val_loss: 1.7710 - val_accuracy: 0.7647\n",
      "Epoch 201/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0762 - accuracy: 0.9758 - val_loss: 1.7561 - val_accuracy: 0.7745\n",
      "Epoch 202/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0595 - accuracy: 0.9787 - val_loss: 1.7815 - val_accuracy: 0.7745\n",
      "Epoch 203/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0488 - accuracy: 0.9816 - val_loss: 1.6583 - val_accuracy: 0.8186\n",
      "Epoch 204/500\n",
      "33/33 [==============================] - 2s 47ms/step - loss: 0.0778 - accuracy: 0.9739 - val_loss: 1.6986 - val_accuracy: 0.7598\n",
      "Epoch 205/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0480 - accuracy: 0.9835 - val_loss: 1.8503 - val_accuracy: 0.7843\n",
      "Epoch 206/500\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0376 - accuracy: 0.9874 - val_loss: 1.8561 - val_accuracy: 0.7696\n",
      "Epoch 207/500\n",
      "33/33 [==============================] - 1s 41ms/step - loss: 0.0398 - accuracy: 0.9864 - val_loss: 1.7833 - val_accuracy: 0.7745\n",
      "Epoch 208/500\n",
      "33/33 [==============================] - 2s 70ms/step - loss: 0.0383 - accuracy: 0.9855 - val_loss: 1.7563 - val_accuracy: 0.7990\n",
      "Epoch 209/500\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0329 - accuracy: 0.9894 - val_loss: 1.9099 - val_accuracy: 0.7941\n",
      "Epoch 210/500\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 0.0338 - accuracy: 0.9884 - val_loss: 2.0297 - val_accuracy: 0.7990\n",
      "Epoch 211/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0837 - accuracy: 0.9758 - val_loss: 1.7351 - val_accuracy: 0.7647\n",
      "Epoch 212/500\n",
      "33/33 [==============================] - 1s 44ms/step - loss: 0.0981 - accuracy: 0.9661 - val_loss: 1.9627 - val_accuracy: 0.7843\n",
      "Epoch 213/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0929 - accuracy: 0.9816 - val_loss: 2.1000 - val_accuracy: 0.7451\n",
      "Epoch 214/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.2461 - accuracy: 0.9409 - val_loss: 1.6096 - val_accuracy: 0.7353\n",
      "Epoch 215/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1487 - accuracy: 0.9613 - val_loss: 1.8471 - val_accuracy: 0.7598\n",
      "Epoch 216/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1526 - accuracy: 0.9535 - val_loss: 1.6819 - val_accuracy: 0.7745\n",
      "Epoch 217/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0733 - accuracy: 0.9710 - val_loss: 1.9175 - val_accuracy: 0.7549\n",
      "Epoch 218/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2184 - accuracy: 0.9429 - val_loss: 2.2883 - val_accuracy: 0.7255\n",
      "Epoch 219/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.3094 - accuracy: 0.9197 - val_loss: 1.4665 - val_accuracy: 0.7402\n",
      "Epoch 220/500\n",
      "33/33 [==============================] - 1s 45ms/step - loss: 0.2867 - accuracy: 0.9206 - val_loss: 1.7389 - val_accuracy: 0.7696\n",
      "Epoch 221/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.1321 - accuracy: 0.9526 - val_loss: 1.8020 - val_accuracy: 0.7843\n",
      "Epoch 222/500\n",
      "33/33 [==============================] - 2s 47ms/step - loss: 0.0955 - accuracy: 0.9661 - val_loss: 1.8731 - val_accuracy: 0.7696\n",
      "Epoch 223/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0951 - accuracy: 0.9661 - val_loss: 1.7327 - val_accuracy: 0.7696\n",
      "Epoch 224/500\n",
      "33/33 [==============================] - 2s 47ms/step - loss: 0.1530 - accuracy: 0.9526 - val_loss: 1.9354 - val_accuracy: 0.7500\n",
      "Epoch 225/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.1756 - accuracy: 0.9526 - val_loss: 1.6701 - val_accuracy: 0.7500\n",
      "Epoch 226/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1067 - accuracy: 0.9661 - val_loss: 1.8580 - val_accuracy: 0.7451\n",
      "Epoch 227/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0884 - accuracy: 0.9739 - val_loss: 1.8174 - val_accuracy: 0.7500\n",
      "Epoch 228/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0671 - accuracy: 0.9768 - val_loss: 1.9415 - val_accuracy: 0.7598\n",
      "Epoch 229/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0464 - accuracy: 0.9845 - val_loss: 1.9112 - val_accuracy: 0.7500\n",
      "Epoch 230/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0800 - accuracy: 0.9787 - val_loss: 2.0606 - val_accuracy: 0.7549\n",
      "Epoch 231/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0444 - accuracy: 0.9864 - val_loss: 1.9548 - val_accuracy: 0.7549\n",
      "Epoch 232/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0441 - accuracy: 0.9874 - val_loss: 1.8458 - val_accuracy: 0.7794\n",
      "Epoch 233/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0204 - accuracy: 0.9923 - val_loss: 1.9121 - val_accuracy: 0.7500\n",
      "Epoch 234/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0246 - accuracy: 0.9913 - val_loss: 1.9553 - val_accuracy: 0.7745\n",
      "Epoch 235/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0255 - accuracy: 0.9903 - val_loss: 2.2846 - val_accuracy: 0.7794\n",
      "Epoch 236/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0454 - accuracy: 0.9845 - val_loss: 1.9798 - val_accuracy: 0.7647\n",
      "Epoch 237/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0695 - accuracy: 0.9816 - val_loss: 2.0825 - val_accuracy: 0.7745\n",
      "Epoch 238/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0483 - accuracy: 0.9855 - val_loss: 1.8553 - val_accuracy: 0.7843\n",
      "Epoch 239/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0842 - accuracy: 0.9768 - val_loss: 1.9238 - val_accuracy: 0.7745\n",
      "Epoch 240/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0703 - accuracy: 0.9787 - val_loss: 1.8093 - val_accuracy: 0.7549\n",
      "Epoch 241/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0688 - accuracy: 0.9797 - val_loss: 2.0667 - val_accuracy: 0.7549\n",
      "Epoch 242/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 1.9461 - val_accuracy: 0.7598\n",
      "Epoch 243/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0415 - accuracy: 0.9884 - val_loss: 1.7979 - val_accuracy: 0.8039\n",
      "Epoch 244/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0306 - accuracy: 0.9884 - val_loss: 1.9233 - val_accuracy: 0.7647\n",
      "Epoch 245/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0462 - accuracy: 0.9835 - val_loss: 1.9699 - val_accuracy: 0.7794\n",
      "Epoch 246/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0684 - accuracy: 0.9826 - val_loss: 1.9011 - val_accuracy: 0.7745\n",
      "Epoch 247/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0461 - accuracy: 0.9835 - val_loss: 2.4120 - val_accuracy: 0.7647\n",
      "Epoch 248/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0522 - accuracy: 0.9806 - val_loss: 1.7442 - val_accuracy: 0.7843\n",
      "Epoch 249/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1022 - accuracy: 0.9729 - val_loss: 1.9522 - val_accuracy: 0.7745\n",
      "Epoch 250/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2321 - accuracy: 0.9371 - val_loss: 1.8431 - val_accuracy: 0.7451\n",
      "Epoch 251/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.2147 - accuracy: 0.9390 - val_loss: 1.6317 - val_accuracy: 0.7745\n",
      "Epoch 252/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1963 - accuracy: 0.9497 - val_loss: 1.7641 - val_accuracy: 0.7843\n",
      "Epoch 253/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2515 - accuracy: 0.9361 - val_loss: 1.7431 - val_accuracy: 0.7500\n",
      "Epoch 254/500\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1934 - accuracy: 0.9448 - val_loss: 1.5897 - val_accuracy: 0.7647\n",
      "Epoch 255/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.3326 - accuracy: 0.9177 - val_loss: 1.6872 - val_accuracy: 0.7647\n",
      "Epoch 256/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.1713 - accuracy: 0.9487 - val_loss: 1.5502 - val_accuracy: 0.7451\n",
      "Epoch 257/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1014 - accuracy: 0.9719 - val_loss: 2.1459 - val_accuracy: 0.7745\n",
      "Epoch 258/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1020 - accuracy: 0.9748 - val_loss: 1.8257 - val_accuracy: 0.7696\n",
      "Epoch 259/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.1388 - accuracy: 0.9681 - val_loss: 1.8481 - val_accuracy: 0.7255\n",
      "Epoch 260/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2091 - accuracy: 0.9371 - val_loss: 1.6015 - val_accuracy: 0.7745\n",
      "Epoch 261/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1412 - accuracy: 0.9632 - val_loss: 1.5835 - val_accuracy: 0.7647\n",
      "Epoch 262/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0391 - accuracy: 0.9845 - val_loss: 1.7844 - val_accuracy: 0.7990\n",
      "Epoch 263/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0184 - accuracy: 0.9932 - val_loss: 1.8114 - val_accuracy: 0.8088\n",
      "Epoch 264/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0179 - accuracy: 0.9961 - val_loss: 1.9265 - val_accuracy: 0.8039\n",
      "Epoch 265/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0127 - accuracy: 0.9961 - val_loss: 2.0808 - val_accuracy: 0.7990\n",
      "Epoch 266/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0219 - accuracy: 0.9913 - val_loss: 2.2620 - val_accuracy: 0.7892\n",
      "Epoch 267/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0116 - accuracy: 0.9932 - val_loss: 2.1776 - val_accuracy: 0.8039\n",
      "Epoch 268/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0187 - accuracy: 0.9952 - val_loss: 2.2639 - val_accuracy: 0.8088\n",
      "Epoch 269/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0143 - accuracy: 0.9942 - val_loss: 2.2528 - val_accuracy: 0.8039\n",
      "Epoch 270/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0217 - accuracy: 0.9913 - val_loss: 2.3105 - val_accuracy: 0.8039\n",
      "Epoch 271/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 2.4044 - val_accuracy: 0.8039\n",
      "Epoch 272/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0341 - accuracy: 0.9874 - val_loss: 1.9936 - val_accuracy: 0.7990\n",
      "Epoch 273/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0883 - accuracy: 0.9806 - val_loss: 2.8439 - val_accuracy: 0.7157\n",
      "Epoch 274/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.4800 - accuracy: 0.8916 - val_loss: 1.5877 - val_accuracy: 0.7500\n",
      "Epoch 275/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.3140 - accuracy: 0.9100 - val_loss: 1.6333 - val_accuracy: 0.7843\n",
      "Epoch 276/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1492 - accuracy: 0.9613 - val_loss: 1.8116 - val_accuracy: 0.7647\n",
      "Epoch 277/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1223 - accuracy: 0.9671 - val_loss: 2.1026 - val_accuracy: 0.7549\n",
      "Epoch 278/500\n",
      "33/33 [==============================] - 2s 46ms/step - loss: 0.1408 - accuracy: 0.9681 - val_loss: 1.9766 - val_accuracy: 0.7402\n",
      "Epoch 279/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1777 - accuracy: 0.9506 - val_loss: 1.8618 - val_accuracy: 0.7500\n",
      "Epoch 280/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0886 - accuracy: 0.9739 - val_loss: 1.8105 - val_accuracy: 0.7353\n",
      "Epoch 281/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0736 - accuracy: 0.9758 - val_loss: 1.9445 - val_accuracy: 0.7794\n",
      "Epoch 282/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0631 - accuracy: 0.9777 - val_loss: 2.1108 - val_accuracy: 0.7500\n",
      "Epoch 283/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0575 - accuracy: 0.9787 - val_loss: 1.8943 - val_accuracy: 0.7647\n",
      "Epoch 284/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0472 - accuracy: 0.9826 - val_loss: 2.1102 - val_accuracy: 0.7794\n",
      "Epoch 285/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0391 - accuracy: 0.9874 - val_loss: 2.1156 - val_accuracy: 0.7794\n",
      "Epoch 286/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0399 - accuracy: 0.9874 - val_loss: 2.1223 - val_accuracy: 0.7843\n",
      "Epoch 287/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 2.0928 - val_accuracy: 0.7892\n",
      "Epoch 288/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0528 - accuracy: 0.9864 - val_loss: 1.9115 - val_accuracy: 0.7892\n",
      "Epoch 289/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0320 - accuracy: 0.9894 - val_loss: 1.9086 - val_accuracy: 0.7892\n",
      "Epoch 290/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 1.8698 - val_accuracy: 0.7843\n",
      "Epoch 291/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 1.9592 - val_accuracy: 0.7990\n",
      "Epoch 292/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0047 - accuracy: 0.9990 - val_loss: 2.0820 - val_accuracy: 0.7892\n",
      "Epoch 293/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.1207 - val_accuracy: 0.7990\n",
      "Epoch 294/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0059 - accuracy: 0.9961 - val_loss: 2.1729 - val_accuracy: 0.7892\n",
      "Epoch 295/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0132 - accuracy: 0.9952 - val_loss: 2.2298 - val_accuracy: 0.7892\n",
      "Epoch 296/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0118 - accuracy: 0.9961 - val_loss: 2.2207 - val_accuracy: 0.7941\n",
      "Epoch 297/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0094 - accuracy: 0.9990 - val_loss: 2.2823 - val_accuracy: 0.7794\n",
      "Epoch 298/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0122 - accuracy: 0.9942 - val_loss: 2.5315 - val_accuracy: 0.7794\n",
      "Epoch 299/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1066 - accuracy: 0.9806 - val_loss: 1.9107 - val_accuracy: 0.7647\n",
      "Epoch 300/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0652 - accuracy: 0.9768 - val_loss: 2.3464 - val_accuracy: 0.7794\n",
      "Epoch 301/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0619 - accuracy: 0.9845 - val_loss: 2.1133 - val_accuracy: 0.7745\n",
      "Epoch 302/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.3096 - accuracy: 0.9351 - val_loss: 1.2669 - val_accuracy: 0.7745\n",
      "Epoch 303/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1446 - accuracy: 0.9632 - val_loss: 1.4004 - val_accuracy: 0.7892\n",
      "Epoch 304/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0613 - accuracy: 0.9826 - val_loss: 1.7412 - val_accuracy: 0.7598\n",
      "Epoch 305/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0906 - accuracy: 0.9652 - val_loss: 2.0345 - val_accuracy: 0.7304\n",
      "Epoch 306/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1229 - accuracy: 0.9690 - val_loss: 1.8928 - val_accuracy: 0.7843\n",
      "Epoch 307/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.3668 - accuracy: 0.9264 - val_loss: 1.5873 - val_accuracy: 0.7402\n",
      "Epoch 308/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1294 - accuracy: 0.9564 - val_loss: 1.9385 - val_accuracy: 0.7059\n",
      "Epoch 309/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1342 - accuracy: 0.9584 - val_loss: 1.8576 - val_accuracy: 0.7451\n",
      "Epoch 310/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.3102 - accuracy: 0.9419 - val_loss: 1.9150 - val_accuracy: 0.7402\n",
      "Epoch 311/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1418 - accuracy: 0.9729 - val_loss: 1.7152 - val_accuracy: 0.7304\n",
      "Epoch 312/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0458 - accuracy: 0.9835 - val_loss: 1.7648 - val_accuracy: 0.7892\n",
      "Epoch 313/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0436 - accuracy: 0.9855 - val_loss: 1.9337 - val_accuracy: 0.7696\n",
      "Epoch 314/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.0501 - accuracy: 0.9845 - val_loss: 1.9181 - val_accuracy: 0.7794\n",
      "Epoch 315/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0376 - accuracy: 0.9903 - val_loss: 1.8384 - val_accuracy: 0.7990\n",
      "Epoch 316/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0468 - accuracy: 0.9845 - val_loss: 2.0527 - val_accuracy: 0.7843\n",
      "Epoch 317/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0602 - accuracy: 0.9855 - val_loss: 1.7353 - val_accuracy: 0.7745\n",
      "Epoch 318/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1213 - accuracy: 0.9681 - val_loss: 1.6656 - val_accuracy: 0.7794\n",
      "Epoch 319/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0656 - accuracy: 0.9787 - val_loss: 1.6214 - val_accuracy: 0.7892\n",
      "Epoch 320/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0347 - accuracy: 0.9894 - val_loss: 1.9006 - val_accuracy: 0.7892\n",
      "Epoch 321/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0507 - accuracy: 0.9826 - val_loss: 2.2368 - val_accuracy: 0.7843\n",
      "Epoch 322/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0325 - accuracy: 0.9913 - val_loss: 2.1315 - val_accuracy: 0.7843\n",
      "Epoch 323/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0371 - accuracy: 0.9884 - val_loss: 2.0782 - val_accuracy: 0.7892\n",
      "Epoch 324/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0240 - accuracy: 0.9913 - val_loss: 2.0400 - val_accuracy: 0.7941\n",
      "Epoch 325/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0528 - accuracy: 0.9845 - val_loss: 2.0899 - val_accuracy: 0.8039\n",
      "Epoch 326/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0724 - accuracy: 0.9768 - val_loss: 1.6622 - val_accuracy: 0.8088\n",
      "Epoch 327/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0409 - accuracy: 0.9884 - val_loss: 1.7067 - val_accuracy: 0.8039\n",
      "Epoch 328/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0430 - accuracy: 0.9835 - val_loss: 1.8735 - val_accuracy: 0.8039\n",
      "Epoch 329/500\n",
      "33/33 [==============================] - 2s 48ms/step - loss: 0.0341 - accuracy: 0.9894 - val_loss: 2.0257 - val_accuracy: 0.8039\n",
      "Epoch 330/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0787 - accuracy: 0.9797 - val_loss: 1.7399 - val_accuracy: 0.8186\n",
      "Epoch 331/500\n",
      "33/33 [==============================] - 2s 48ms/step - loss: 0.0435 - accuracy: 0.9826 - val_loss: 1.7280 - val_accuracy: 0.7990\n",
      "Epoch 332/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0431 - accuracy: 0.9855 - val_loss: 1.9154 - val_accuracy: 0.7990\n",
      "Epoch 333/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0296 - accuracy: 0.9913 - val_loss: 2.1515 - val_accuracy: 0.7843\n",
      "Epoch 334/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0299 - accuracy: 0.9864 - val_loss: 1.9856 - val_accuracy: 0.7892\n",
      "Epoch 335/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0809 - accuracy: 0.9777 - val_loss: 1.8406 - val_accuracy: 0.8137\n",
      "Epoch 336/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0819 - accuracy: 0.9787 - val_loss: 2.1939 - val_accuracy: 0.7647\n",
      "Epoch 337/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1519 - accuracy: 0.9652 - val_loss: 1.8466 - val_accuracy: 0.7549\n",
      "Epoch 338/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2566 - accuracy: 0.9458 - val_loss: 2.4017 - val_accuracy: 0.7549\n",
      "Epoch 339/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1584 - accuracy: 0.9545 - val_loss: 2.1646 - val_accuracy: 0.7745\n",
      "Epoch 340/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1487 - accuracy: 0.9652 - val_loss: 1.9145 - val_accuracy: 0.7843\n",
      "Epoch 341/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1429 - accuracy: 0.9642 - val_loss: 1.9221 - val_accuracy: 0.7745\n",
      "Epoch 342/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0704 - accuracy: 0.9806 - val_loss: 2.1118 - val_accuracy: 0.7745\n",
      "Epoch 343/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0313 - accuracy: 0.9923 - val_loss: 1.9117 - val_accuracy: 0.7941\n",
      "Epoch 344/500\n",
      "33/33 [==============================] - 1s 41ms/step - loss: 0.0214 - accuracy: 0.9923 - val_loss: 1.8990 - val_accuracy: 0.8088\n",
      "Epoch 345/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0129 - accuracy: 0.9961 - val_loss: 2.0753 - val_accuracy: 0.7892\n",
      "Epoch 346/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0162 - accuracy: 0.9961 - val_loss: 1.9498 - val_accuracy: 0.7843\n",
      "Epoch 347/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0162 - accuracy: 0.9932 - val_loss: 1.9466 - val_accuracy: 0.7941\n",
      "Epoch 348/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0113 - accuracy: 0.9961 - val_loss: 1.9659 - val_accuracy: 0.8039\n",
      "Epoch 349/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0070 - accuracy: 0.9990 - val_loss: 1.9979 - val_accuracy: 0.8039\n",
      "Epoch 350/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0074 - accuracy: 0.9971 - val_loss: 2.2268 - val_accuracy: 0.7941\n",
      "Epoch 351/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0253 - accuracy: 0.9981 - val_loss: 2.0877 - val_accuracy: 0.8039\n",
      "Epoch 352/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0413 - accuracy: 0.9894 - val_loss: 1.9857 - val_accuracy: 0.7794\n",
      "Epoch 353/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0615 - accuracy: 0.9855 - val_loss: 1.9959 - val_accuracy: 0.7500\n",
      "Epoch 354/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1137 - accuracy: 0.9719 - val_loss: 1.9325 - val_accuracy: 0.7647\n",
      "Epoch 355/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1561 - accuracy: 0.9661 - val_loss: 1.5504 - val_accuracy: 0.7843\n",
      "Epoch 356/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0791 - accuracy: 0.9826 - val_loss: 1.7396 - val_accuracy: 0.7745\n",
      "Epoch 357/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0615 - accuracy: 0.9845 - val_loss: 2.0085 - val_accuracy: 0.7745\n",
      "Epoch 358/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.0511 - accuracy: 0.9894 - val_loss: 2.0303 - val_accuracy: 0.7892\n",
      "Epoch 359/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0595 - accuracy: 0.9816 - val_loss: 1.8526 - val_accuracy: 0.7696\n",
      "Epoch 360/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1288 - accuracy: 0.9661 - val_loss: 1.9467 - val_accuracy: 0.7598\n",
      "Epoch 361/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0854 - accuracy: 0.9739 - val_loss: 1.7107 - val_accuracy: 0.7304\n",
      "Epoch 362/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1391 - accuracy: 0.9642 - val_loss: 1.9920 - val_accuracy: 0.7157\n",
      "Epoch 363/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.6038 - accuracy: 0.8674 - val_loss: 1.3915 - val_accuracy: 0.7794\n",
      "Epoch 364/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.3174 - accuracy: 0.9051 - val_loss: 1.4963 - val_accuracy: 0.7500\n",
      "Epoch 365/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2080 - accuracy: 0.9468 - val_loss: 2.2336 - val_accuracy: 0.7598\n",
      "Epoch 366/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1650 - accuracy: 0.9545 - val_loss: 2.0212 - val_accuracy: 0.7353\n",
      "Epoch 367/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0947 - accuracy: 0.9748 - val_loss: 2.0151 - val_accuracy: 0.7745\n",
      "Epoch 368/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0530 - accuracy: 0.9845 - val_loss: 2.0874 - val_accuracy: 0.7598\n",
      "Epoch 369/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0383 - accuracy: 0.9913 - val_loss: 2.1199 - val_accuracy: 0.7794\n",
      "Epoch 370/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0365 - accuracy: 0.9923 - val_loss: 2.0961 - val_accuracy: 0.7794\n",
      "Epoch 371/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.0169 - accuracy: 0.9981 - val_loss: 2.1459 - val_accuracy: 0.7892\n",
      "Epoch 372/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0061 - accuracy: 0.9990 - val_loss: 2.2744 - val_accuracy: 0.7794\n",
      "Epoch 373/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0036 - accuracy: 0.9990 - val_loss: 2.3033 - val_accuracy: 0.7990\n",
      "Epoch 374/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0686 - accuracy: 0.9913 - val_loss: 2.2152 - val_accuracy: 0.7696\n",
      "Epoch 375/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1199 - accuracy: 0.9787 - val_loss: 1.8866 - val_accuracy: 0.7794\n",
      "Epoch 376/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0686 - accuracy: 0.9729 - val_loss: 1.6973 - val_accuracy: 0.7843\n",
      "Epoch 377/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0600 - accuracy: 0.9826 - val_loss: 1.6530 - val_accuracy: 0.7696\n",
      "Epoch 378/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0853 - accuracy: 0.9777 - val_loss: 2.0177 - val_accuracy: 0.7794\n",
      "Epoch 379/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1273 - accuracy: 0.9719 - val_loss: 1.9533 - val_accuracy: 0.7598\n",
      "Epoch 380/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.2224 - accuracy: 0.9468 - val_loss: 1.7193 - val_accuracy: 0.7647\n",
      "Epoch 381/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1281 - accuracy: 0.9622 - val_loss: 1.6327 - val_accuracy: 0.7892\n",
      "Epoch 382/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0691 - accuracy: 0.9806 - val_loss: 1.7141 - val_accuracy: 0.7892\n",
      "Epoch 383/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0657 - accuracy: 0.9816 - val_loss: 1.7299 - val_accuracy: 0.7843\n",
      "Epoch 384/500\n",
      "33/33 [==============================] - 2s 51ms/step - loss: 0.0416 - accuracy: 0.9864 - val_loss: 1.7650 - val_accuracy: 0.7892\n",
      "Epoch 385/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0401 - accuracy: 0.9884 - val_loss: 1.7757 - val_accuracy: 0.7843\n",
      "Epoch 386/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0502 - accuracy: 0.9874 - val_loss: 1.5873 - val_accuracy: 0.8088\n",
      "Epoch 387/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 1.7383 - val_accuracy: 0.7892\n",
      "Epoch 388/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0165 - accuracy: 0.9961 - val_loss: 1.7157 - val_accuracy: 0.7941\n",
      "Epoch 389/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0266 - accuracy: 0.9923 - val_loss: 1.6379 - val_accuracy: 0.8039\n",
      "Epoch 390/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0338 - accuracy: 0.9942 - val_loss: 1.5305 - val_accuracy: 0.8186\n",
      "Epoch 391/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0308 - accuracy: 0.9923 - val_loss: 1.7417 - val_accuracy: 0.7892\n",
      "Epoch 392/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0496 - accuracy: 0.9913 - val_loss: 1.9781 - val_accuracy: 0.7745\n",
      "Epoch 393/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0804 - accuracy: 0.9826 - val_loss: 1.6560 - val_accuracy: 0.7598\n",
      "Epoch 394/500\n",
      "33/33 [==============================] - 1s 45ms/step - loss: 0.0494 - accuracy: 0.9845 - val_loss: 1.9359 - val_accuracy: 0.7794\n",
      "Epoch 395/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0137 - accuracy: 0.9942 - val_loss: 1.9061 - val_accuracy: 0.7941\n",
      "Epoch 396/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0148 - accuracy: 0.9981 - val_loss: 1.9816 - val_accuracy: 0.7794\n",
      "Epoch 397/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0138 - accuracy: 0.9942 - val_loss: 1.9459 - val_accuracy: 0.7794\n",
      "Epoch 398/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0078 - accuracy: 0.9971 - val_loss: 2.0609 - val_accuracy: 0.7745\n",
      "Epoch 399/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0108 - accuracy: 0.9961 - val_loss: 2.1760 - val_accuracy: 0.7794\n",
      "Epoch 400/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0306 - accuracy: 0.9932 - val_loss: 2.7190 - val_accuracy: 0.7402\n",
      "Epoch 401/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.2119 - accuracy: 0.9516 - val_loss: 2.0800 - val_accuracy: 0.7353\n",
      "Epoch 402/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.2661 - accuracy: 0.9458 - val_loss: 1.9116 - val_accuracy: 0.7304\n",
      "Epoch 403/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1227 - accuracy: 0.9642 - val_loss: 1.9456 - val_accuracy: 0.7598\n",
      "Epoch 404/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0727 - accuracy: 0.9758 - val_loss: 2.0170 - val_accuracy: 0.7451\n",
      "Epoch 405/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1398 - accuracy: 0.9661 - val_loss: 2.0766 - val_accuracy: 0.7402\n",
      "Epoch 406/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1516 - accuracy: 0.9661 - val_loss: 1.9059 - val_accuracy: 0.7794\n",
      "Epoch 407/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0899 - accuracy: 0.9797 - val_loss: 2.5308 - val_accuracy: 0.7843\n",
      "Epoch 408/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1894 - accuracy: 0.9593 - val_loss: 1.9566 - val_accuracy: 0.7402\n",
      "Epoch 409/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1481 - accuracy: 0.9671 - val_loss: 1.9696 - val_accuracy: 0.7549\n",
      "Epoch 410/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.1825 - accuracy: 0.9700 - val_loss: 1.7914 - val_accuracy: 0.7647\n",
      "Epoch 411/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0721 - accuracy: 0.9826 - val_loss: 1.6821 - val_accuracy: 0.7941\n",
      "Epoch 412/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0259 - accuracy: 0.9923 - val_loss: 1.6858 - val_accuracy: 0.8137\n",
      "Epoch 413/500\n",
      "33/33 [==============================] - 2s 67ms/step - loss: 0.0765 - accuracy: 0.9826 - val_loss: 1.7973 - val_accuracy: 0.7745\n",
      "Epoch 414/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.0851 - accuracy: 0.9768 - val_loss: 1.5522 - val_accuracy: 0.8039\n",
      "Epoch 415/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.0826 - accuracy: 0.9758 - val_loss: 1.8665 - val_accuracy: 0.7794\n",
      "Epoch 416/500\n",
      "33/33 [==============================] - 1s 45ms/step - loss: 0.0757 - accuracy: 0.9777 - val_loss: 1.6810 - val_accuracy: 0.7990\n",
      "Epoch 417/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0339 - accuracy: 0.9884 - val_loss: 1.7469 - val_accuracy: 0.7941\n",
      "Epoch 418/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0136 - accuracy: 0.9961 - val_loss: 1.8676 - val_accuracy: 0.7941\n",
      "Epoch 419/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 1.8921 - val_accuracy: 0.7941\n",
      "Epoch 420/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0082 - accuracy: 0.9971 - val_loss: 1.9545 - val_accuracy: 0.7892\n",
      "Epoch 421/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0180 - accuracy: 0.9961 - val_loss: 2.0527 - val_accuracy: 0.7892\n",
      "Epoch 422/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0259 - accuracy: 0.9913 - val_loss: 1.9291 - val_accuracy: 0.7794\n",
      "Epoch 423/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0218 - accuracy: 0.9884 - val_loss: 1.9745 - val_accuracy: 0.7794\n",
      "Epoch 424/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0241 - accuracy: 0.9903 - val_loss: 1.9187 - val_accuracy: 0.7892\n",
      "Epoch 425/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0116 - accuracy: 0.9952 - val_loss: 1.9677 - val_accuracy: 0.7794\n",
      "Epoch 426/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0075 - accuracy: 0.9971 - val_loss: 2.0131 - val_accuracy: 0.7892\n",
      "Epoch 427/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0119 - accuracy: 0.9961 - val_loss: 2.0796 - val_accuracy: 0.7745\n",
      "Epoch 428/500\n",
      "33/33 [==============================] - 1s 41ms/step - loss: 0.0130 - accuracy: 0.9942 - val_loss: 2.0684 - val_accuracy: 0.8039\n",
      "Epoch 429/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0132 - accuracy: 0.9971 - val_loss: 2.0426 - val_accuracy: 0.7745\n",
      "Epoch 430/500\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 0.0189 - accuracy: 0.9942 - val_loss: 2.0199 - val_accuracy: 0.7696\n",
      "Epoch 431/500\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.0210 - accuracy: 0.9913 - val_loss: 2.0484 - val_accuracy: 0.7745\n",
      "Epoch 432/500\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0081 - accuracy: 0.9961 - val_loss: 2.0042 - val_accuracy: 0.7843\n",
      "Epoch 433/500\n",
      "33/33 [==============================] - 3s 97ms/step - loss: 0.0063 - accuracy: 0.9981 - val_loss: 2.1449 - val_accuracy: 0.7843\n",
      "Epoch 434/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0083 - accuracy: 0.9961 - val_loss: 2.1919 - val_accuracy: 0.7892\n",
      "Epoch 435/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0246 - accuracy: 0.9903 - val_loss: 2.3671 - val_accuracy: 0.7696\n",
      "Epoch 436/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0197 - accuracy: 0.9932 - val_loss: 2.9531 - val_accuracy: 0.7304\n",
      "Epoch 437/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0891 - accuracy: 0.9797 - val_loss: 2.4229 - val_accuracy: 0.7892\n",
      "Epoch 438/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0320 - accuracy: 0.9864 - val_loss: 2.3262 - val_accuracy: 0.7598\n",
      "Epoch 439/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0311 - accuracy: 0.9923 - val_loss: 2.3339 - val_accuracy: 0.7892\n",
      "Epoch 440/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0294 - accuracy: 0.9864 - val_loss: 2.2782 - val_accuracy: 0.7892\n",
      "Epoch 441/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0450 - accuracy: 0.9874 - val_loss: 2.2779 - val_accuracy: 0.7598\n",
      "Epoch 442/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.1162 - accuracy: 0.9797 - val_loss: 2.0401 - val_accuracy: 0.7794\n",
      "Epoch 443/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.1870 - accuracy: 0.9526 - val_loss: 2.0664 - val_accuracy: 0.7843\n",
      "Epoch 444/500\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.2262 - accuracy: 0.9342 - val_loss: 2.1988 - val_accuracy: 0.7157\n",
      "Epoch 445/500\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 0.2122 - accuracy: 0.9506 - val_loss: 1.8356 - val_accuracy: 0.7696\n",
      "Epoch 446/500\n",
      "33/33 [==============================] - 2s 53ms/step - loss: 0.2781 - accuracy: 0.9468 - val_loss: 1.8104 - val_accuracy: 0.7549\n",
      "Epoch 447/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.2046 - accuracy: 0.9448 - val_loss: 1.8339 - val_accuracy: 0.7598\n",
      "Epoch 448/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 0.0978 - accuracy: 0.9768 - val_loss: 2.0039 - val_accuracy: 0.7647\n",
      "Epoch 449/500\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.0543 - accuracy: 0.9845 - val_loss: 1.9496 - val_accuracy: 0.7647\n",
      "Epoch 450/500\n",
      "33/33 [==============================] - 2s 50ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 2.1967 - val_accuracy: 0.7892\n",
      "Epoch 451/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0499 - accuracy: 0.9874 - val_loss: 1.9699 - val_accuracy: 0.7941\n",
      "Epoch 452/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0223 - accuracy: 0.9923 - val_loss: 2.2333 - val_accuracy: 0.7892\n",
      "Epoch 453/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0501 - accuracy: 0.9835 - val_loss: 2.0554 - val_accuracy: 0.7941\n",
      "Epoch 454/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0489 - accuracy: 0.9884 - val_loss: 2.0127 - val_accuracy: 0.7892\n",
      "Epoch 455/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0400 - accuracy: 0.9864 - val_loss: 1.9543 - val_accuracy: 0.7941\n",
      "Epoch 456/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0186 - accuracy: 0.9961 - val_loss: 2.0179 - val_accuracy: 0.8039\n",
      "Epoch 457/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0081 - accuracy: 0.9981 - val_loss: 1.9172 - val_accuracy: 0.7990\n",
      "Epoch 458/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0195 - accuracy: 0.9942 - val_loss: 2.2509 - val_accuracy: 0.7696\n",
      "Epoch 459/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1136 - accuracy: 0.9777 - val_loss: 2.4766 - val_accuracy: 0.7745\n",
      "Epoch 460/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1902 - accuracy: 0.9584 - val_loss: 2.2085 - val_accuracy: 0.7451\n",
      "Epoch 461/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0698 - accuracy: 0.9797 - val_loss: 2.2277 - val_accuracy: 0.7647\n",
      "Epoch 462/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0797 - accuracy: 0.9719 - val_loss: 2.5000 - val_accuracy: 0.7549\n",
      "Epoch 463/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2494 - accuracy: 0.9429 - val_loss: 2.3187 - val_accuracy: 0.7500\n",
      "Epoch 464/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1828 - accuracy: 0.9584 - val_loss: 1.8098 - val_accuracy: 0.7843\n",
      "Epoch 465/500\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.1480 - accuracy: 0.9719 - val_loss: 2.2217 - val_accuracy: 0.7794\n",
      "Epoch 466/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0840 - accuracy: 0.9739 - val_loss: 1.6739 - val_accuracy: 0.8137\n",
      "Epoch 467/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0465 - accuracy: 0.9874 - val_loss: 1.5490 - val_accuracy: 0.7843\n",
      "Epoch 468/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0340 - accuracy: 0.9932 - val_loss: 1.7940 - val_accuracy: 0.7843\n",
      "Epoch 469/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0134 - accuracy: 0.9981 - val_loss: 1.8202 - val_accuracy: 0.7990\n",
      "Epoch 470/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0095 - accuracy: 0.9971 - val_loss: 1.8741 - val_accuracy: 0.8088\n",
      "Epoch 471/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0139 - accuracy: 0.9971 - val_loss: 1.9412 - val_accuracy: 0.7990\n",
      "Epoch 472/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0056 - accuracy: 0.9990 - val_loss: 1.9804 - val_accuracy: 0.8039\n",
      "Epoch 473/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 2.0626 - val_accuracy: 0.8137\n",
      "Epoch 474/500\n",
      "33/33 [==============================] - 1s 36ms/step - loss: 0.0112 - accuracy: 0.9981 - val_loss: 2.1088 - val_accuracy: 0.8039\n",
      "Epoch 475/500\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0058 - accuracy: 0.9981 - val_loss: 2.3072 - val_accuracy: 0.7892\n",
      "Epoch 476/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0125 - accuracy: 0.9971 - val_loss: 2.0673 - val_accuracy: 0.7892\n",
      "Epoch 477/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0070 - accuracy: 0.9981 - val_loss: 2.1682 - val_accuracy: 0.7892\n",
      "Epoch 478/500\n",
      "33/33 [==============================] - 2s 52ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 2.0987 - val_accuracy: 0.7990\n",
      "Epoch 479/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.0100 - accuracy: 0.9981 - val_loss: 2.3098 - val_accuracy: 0.8137\n",
      "Epoch 480/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0175 - accuracy: 0.9932 - val_loss: 2.0783 - val_accuracy: 0.8088\n",
      "Epoch 481/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0222 - accuracy: 0.9942 - val_loss: 2.1334 - val_accuracy: 0.8088\n",
      "Epoch 482/500\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.0814 - accuracy: 0.9787 - val_loss: 1.9467 - val_accuracy: 0.7794\n",
      "Epoch 483/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1037 - accuracy: 0.9661 - val_loss: 1.8182 - val_accuracy: 0.8088\n",
      "Epoch 484/500\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.0859 - accuracy: 0.9768 - val_loss: 1.9730 - val_accuracy: 0.7794\n",
      "Epoch 485/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0384 - accuracy: 0.9855 - val_loss: 2.0728 - val_accuracy: 0.7696\n",
      "Epoch 486/500\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0320 - accuracy: 0.99 - 1s 40ms/step - loss: 0.0320 - accuracy: 0.9903 - val_loss: 2.2405 - val_accuracy: 0.7843\n",
      "Epoch 487/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0300 - accuracy: 0.9913 - val_loss: 2.0427 - val_accuracy: 0.7696\n",
      "Epoch 488/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0214 - accuracy: 0.9903 - val_loss: 2.1142 - val_accuracy: 0.7794\n",
      "Epoch 489/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0172 - accuracy: 0.9894 - val_loss: 2.2661 - val_accuracy: 0.7941\n",
      "Epoch 490/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0058 - accuracy: 0.9990 - val_loss: 2.4058 - val_accuracy: 0.7843\n",
      "Epoch 491/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.0036 - accuracy: 0.9981 - val_loss: 2.6636 - val_accuracy: 0.7892\n",
      "Epoch 492/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 2.6314 - val_accuracy: 0.7990\n",
      "Epoch 493/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 2.7117 - val_accuracy: 0.8039\n",
      "Epoch 494/500\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 7.6336e-04 - accuracy: 1.0000 - val_loss: 2.7330 - val_accuracy: 0.7990\n",
      "Epoch 495/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 7.8326e-04 - accuracy: 1.0000 - val_loss: 2.7412 - val_accuracy: 0.7941\n",
      "Epoch 496/500\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 3.9671e-04 - accuracy: 1.0000 - val_loss: 2.8947 - val_accuracy: 0.7941\n",
      "Epoch 497/500\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 1.7477e-04 - accuracy: 1.0000 - val_loss: 2.9111 - val_accuracy: 0.7941\n",
      "Epoch 498/500\n",
      "33/33 [==============================] - 1s 38ms/step - loss: 1.4331e-04 - accuracy: 1.0000 - val_loss: 2.9450 - val_accuracy: 0.7941\n",
      "Epoch 499/500\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 6.9718e-04 - accuracy: 1.0000 - val_loss: 3.0605 - val_accuracy: 0.7892\n",
      "Epoch 500/500\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0141 - accuracy: 0.9971 - val_loss: 2.6575 - val_accuracy: 0.7990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x161533e8850>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the Model\n",
    "model.fit(x_train, y_train, epochs=500, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "finished-compilation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved into model_SIBI.h5\n"
     ]
    }
   ],
   "source": [
    "#Saving the model into H5 system file\n",
    "save_model = \"model_SIBI.h5\"\n",
    "model.save(save_model)\n",
    "print(\"Model Saved into\", save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wanted-vision",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model for TF-Serving Type\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"server_model_SIBI\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    print('\\nAlready saved a model, cleaning up\\n')\n",
    "    !rm -r {export_path}\n",
    "\n",
    "model.save(export_path, save_format=\"tf\")\n",
    "\n",
    "print('\\nexport_path = {}'.format(export_path))\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "intellectual-today",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 42, 1)\n",
      "[[[1103.327]\n",
      "  [1540.716]\n",
      "  [ 855.426]\n",
      "  [1411.774]\n",
      "  [ 683.308]\n",
      "  [1153.357]\n",
      "  [ 558.997]\n",
      "  [ 959.797]\n",
      "  [ 387.358]\n",
      "  [ 854.781]\n",
      "  [ 934.849]\n",
      "  [ 893.382]\n",
      "  [ 925.604]\n",
      "  [ 639.385]\n",
      "  [ 920.373]\n",
      "  [ 475.725]\n",
      "  [ 924.981]\n",
      "  [ 324.413]\n",
      "  [1102.588]\n",
      "  [ 946.422]\n",
      "  [1137.946]\n",
      "  [ 746.217]\n",
      "  [1086.862]\n",
      "  [1009.154]\n",
      "  [1076.11 ]\n",
      "  [1152.22 ]\n",
      "  [1254.827]\n",
      "  [1021.595]\n",
      "  [1289.853]\n",
      "  [ 855.331]\n",
      "  [1201.73 ]\n",
      "  [1101.462]\n",
      "  [1185.479]\n",
      "  [1216.182]\n",
      "  [1399.912]\n",
      "  [1116.804]\n",
      "  [1421.615]\n",
      "  [ 964.66 ]\n",
      "  [1333.269]\n",
      "  [1121.019]\n",
      "  [1310.715]\n",
      "  [1211.711]]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "input_test = [[[1103.32715511], [1540.71593285],\n",
    "               [ 855.42565584],[1411.77392006],[ 683.30776691],[1153.35738659],[ 558.99703503],[ 959.79690552],[ 387.35830784],[ 854.78103161],\n",
    "               [ 934.84920263],[ 893.38219166],[ 925.60398579],[ 639.38450813],[ 920.37254572],[ 475.72478652],[ 924.98147488],[ 324.41276312],\n",
    "               [1102.58769989],[ 946.42174244],[1137.94636726],[ 746.21725082],[1086.86184883],[1009.15443897],[1076.11000538],[1152.21977234],\n",
    "               [1254.82654572],[1021.59452438],[1289.85261917],[ 855.33124208],[1201.73048973],[1101.46188736],[1185.47868729],[1216.18199348],\n",
    "               [1399.91247654],[1116.8037653 ],[1421.61536217],[ 964.65975046],[1333.26935768],[1121.01900578],[1310.71531773],[1211.71069145]]]\n",
    "input_test = np.array(input_test)\n",
    "input = np.reshape(input_test, (input_test.shape[0], input_test.shape[1], 1))\n",
    "print(input_test.shape)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "yellow-oregon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "WARNING:tensorflow:From <ipython-input-16-ce52a49e73f2>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "#Print the Prediction\n",
    "print(model.predict(input_test))\n",
    "print(model.predict_classes(input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proper-radio",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n"
     ]
    }
   ],
   "source": [
    "classes = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6,\n",
    "    'H': 7,\n",
    "    'I': 8,\n",
    "    'J': 9,\n",
    "    'K': 10,\n",
    "    'L': 11,\n",
    "    'M': 12,\n",
    "    'N': 13,\n",
    "    'O': 14,\n",
    "    'P': 15,\n",
    "    'Q': 16,\n",
    "    'R': 17,\n",
    "    'S': 18,\n",
    "    'T': 19,\n",
    "    'U': 20,\n",
    "    'V': 21,\n",
    "    'W': 22,\n",
    "    'X': 23,\n",
    "    'Y': 24,\n",
    "    'Z': 25\n",
    "}\n",
    "\n",
    "predictions = model.predict_classes(input_test)\n",
    "for alphabets, values in classes.items():\n",
    "    if values == predictions[0] :\n",
    "        print(alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-mechanism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
