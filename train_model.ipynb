{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cellular-proportion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the much needed stuff for training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "abroad-archives",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>wristZ</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_CmcZ</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_McpZ</th>\n",
       "      <th>...</th>\n",
       "      <th>pinky_McpZ</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_PipZ</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_DipZ</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "      <th>pinky_TipZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>633.079380</td>\n",
       "      <td>1031.794727</td>\n",
       "      <td>-1.029254e-06</td>\n",
       "      <td>552.742794</td>\n",
       "      <td>1014.596709</td>\n",
       "      <td>-0.016872</td>\n",
       "      <td>488.761693</td>\n",
       "      <td>950.070965</td>\n",
       "      <td>-0.015831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014530</td>\n",
       "      <td>625.414029</td>\n",
       "      <td>841.904812</td>\n",
       "      <td>-0.030691</td>\n",
       "      <td>621.901855</td>\n",
       "      <td>885.472149</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>634.784594</td>\n",
       "      <td>893.365762</td>\n",
       "      <td>0.015983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>1591.784213</td>\n",
       "      <td>2099.707924</td>\n",
       "      <td>-1.857942e-06</td>\n",
       "      <td>1318.607858</td>\n",
       "      <td>1917.100941</td>\n",
       "      <td>-0.034396</td>\n",
       "      <td>1120.267701</td>\n",
       "      <td>1682.855012</td>\n",
       "      <td>-0.059530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074243</td>\n",
       "      <td>1683.593624</td>\n",
       "      <td>1443.143311</td>\n",
       "      <td>-0.114158</td>\n",
       "      <td>1637.900345</td>\n",
       "      <td>1626.554544</td>\n",
       "      <td>-0.080055</td>\n",
       "      <td>1703.259765</td>\n",
       "      <td>1651.284102</td>\n",
       "      <td>-0.043401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1583.454045</td>\n",
       "      <td>1993.154838</td>\n",
       "      <td>-1.932337e-06</td>\n",
       "      <td>1328.071508</td>\n",
       "      <td>1836.134789</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>1136.303083</td>\n",
       "      <td>1590.329483</td>\n",
       "      <td>-0.039298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075024</td>\n",
       "      <td>1680.113028</td>\n",
       "      <td>1334.217393</td>\n",
       "      <td>-0.118665</td>\n",
       "      <td>1636.022493</td>\n",
       "      <td>1532.326854</td>\n",
       "      <td>-0.082507</td>\n",
       "      <td>1701.786802</td>\n",
       "      <td>1574.371294</td>\n",
       "      <td>-0.043058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>1593.135340</td>\n",
       "      <td>2003.049824</td>\n",
       "      <td>-1.917325e-06</td>\n",
       "      <td>1329.626807</td>\n",
       "      <td>1846.083207</td>\n",
       "      <td>-0.025695</td>\n",
       "      <td>1135.570690</td>\n",
       "      <td>1601.084763</td>\n",
       "      <td>-0.042078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071267</td>\n",
       "      <td>1678.341120</td>\n",
       "      <td>1337.785852</td>\n",
       "      <td>-0.110229</td>\n",
       "      <td>1631.677766</td>\n",
       "      <td>1530.720809</td>\n",
       "      <td>-0.074036</td>\n",
       "      <td>1692.048390</td>\n",
       "      <td>1576.065932</td>\n",
       "      <td>-0.036066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>1615.659772</td>\n",
       "      <td>1899.973072</td>\n",
       "      <td>-1.697942e-06</td>\n",
       "      <td>1366.676348</td>\n",
       "      <td>1755.681619</td>\n",
       "      <td>-0.028110</td>\n",
       "      <td>1173.298271</td>\n",
       "      <td>1514.212327</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053522</td>\n",
       "      <td>1706.582711</td>\n",
       "      <td>1260.215963</td>\n",
       "      <td>-0.096525</td>\n",
       "      <td>1669.381164</td>\n",
       "      <td>1456.343893</td>\n",
       "      <td>-0.064412</td>\n",
       "      <td>1727.129797</td>\n",
       "      <td>1498.052231</td>\n",
       "      <td>-0.027742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>Z</td>\n",
       "      <td>472.966313</td>\n",
       "      <td>1540.549994</td>\n",
       "      <td>1.156611e-06</td>\n",
       "      <td>522.023559</td>\n",
       "      <td>1514.556885</td>\n",
       "      <td>-0.276131</td>\n",
       "      <td>711.459756</td>\n",
       "      <td>1421.978235</td>\n",
       "      <td>-0.379206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041158</td>\n",
       "      <td>1253.675699</td>\n",
       "      <td>1319.325686</td>\n",
       "      <td>-0.154479</td>\n",
       "      <td>1103.553176</td>\n",
       "      <td>1493.587255</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>963.837624</td>\n",
       "      <td>1532.385945</td>\n",
       "      <td>-0.143554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Z</td>\n",
       "      <td>511.085093</td>\n",
       "      <td>1538.953304</td>\n",
       "      <td>1.230672e-06</td>\n",
       "      <td>485.632837</td>\n",
       "      <td>1530.085564</td>\n",
       "      <td>-0.252264</td>\n",
       "      <td>638.235927</td>\n",
       "      <td>1447.455406</td>\n",
       "      <td>-0.371612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123394</td>\n",
       "      <td>1241.890192</td>\n",
       "      <td>1443.705082</td>\n",
       "      <td>-0.246548</td>\n",
       "      <td>1078.716993</td>\n",
       "      <td>1598.790646</td>\n",
       "      <td>-0.245476</td>\n",
       "      <td>946.605921</td>\n",
       "      <td>1613.921404</td>\n",
       "      <td>-0.219498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Z</td>\n",
       "      <td>289.586902</td>\n",
       "      <td>1389.938593</td>\n",
       "      <td>1.943169e-07</td>\n",
       "      <td>592.706800</td>\n",
       "      <td>1311.617374</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>900.285482</td>\n",
       "      <td>1261.919737</td>\n",
       "      <td>-0.125441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223034</td>\n",
       "      <td>1054.138541</td>\n",
       "      <td>1293.740630</td>\n",
       "      <td>0.159143</td>\n",
       "      <td>914.128363</td>\n",
       "      <td>1380.347967</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>786.959291</td>\n",
       "      <td>1369.079351</td>\n",
       "      <td>0.106502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Z</td>\n",
       "      <td>265.467346</td>\n",
       "      <td>1484.784365</td>\n",
       "      <td>2.655130e-07</td>\n",
       "      <td>447.429001</td>\n",
       "      <td>1383.484006</td>\n",
       "      <td>-0.221616</td>\n",
       "      <td>732.457399</td>\n",
       "      <td>1315.951228</td>\n",
       "      <td>-0.233599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175156</td>\n",
       "      <td>1110.132456</td>\n",
       "      <td>1363.412261</td>\n",
       "      <td>0.080964</td>\n",
       "      <td>984.978139</td>\n",
       "      <td>1486.548662</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>876.057625</td>\n",
       "      <td>1501.790762</td>\n",
       "      <td>0.041642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Z</td>\n",
       "      <td>413.485765</td>\n",
       "      <td>1470.117807</td>\n",
       "      <td>2.890906e-07</td>\n",
       "      <td>521.474183</td>\n",
       "      <td>1376.033545</td>\n",
       "      <td>-0.238790</td>\n",
       "      <td>766.105592</td>\n",
       "      <td>1297.483921</td>\n",
       "      <td>-0.279074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094886</td>\n",
       "      <td>1289.921880</td>\n",
       "      <td>1314.355969</td>\n",
       "      <td>-0.003062</td>\n",
       "      <td>1141.063213</td>\n",
       "      <td>1440.321207</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>1006.802082</td>\n",
       "      <td>1456.267476</td>\n",
       "      <td>-0.022182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_type       wristX       wristY        wristZ   thumb_CmcX  \\\n",
       "0             A   633.079380  1031.794727 -1.029254e-06   552.742794   \n",
       "1             A  1591.784213  2099.707924 -1.857942e-06  1318.607858   \n",
       "2             A  1583.454045  1993.154838 -1.932337e-06  1328.071508   \n",
       "3             A  1593.135340  2003.049824 -1.917325e-06  1329.626807   \n",
       "4             A  1615.659772  1899.973072 -1.697942e-06  1366.676348   \n",
       "...         ...          ...          ...           ...          ...   \n",
       "1051          Z   472.966313  1540.549994  1.156611e-06   522.023559   \n",
       "1052          Z   511.085093  1538.953304  1.230672e-06   485.632837   \n",
       "1053          Z   289.586902  1389.938593  1.943169e-07   592.706800   \n",
       "1054          Z   265.467346  1484.784365  2.655130e-07   447.429001   \n",
       "1055          Z   413.485765  1470.117807  2.890906e-07   521.474183   \n",
       "\n",
       "       thumb_CmcY  thumb_CmcZ   thumb_McpX   thumb_McpY  thumb_McpZ  ...  \\\n",
       "0     1014.596709   -0.016872   488.761693   950.070965   -0.015831  ...   \n",
       "1     1917.100941   -0.034396  1120.267701  1682.855012   -0.059530  ...   \n",
       "2     1836.134789   -0.023637  1136.303083  1590.329483   -0.039298  ...   \n",
       "3     1846.083207   -0.025695  1135.570690  1601.084763   -0.042078  ...   \n",
       "4     1755.681619   -0.028110  1173.298271  1514.212327   -0.040185  ...   \n",
       "...           ...         ...          ...          ...         ...  ...   \n",
       "1051  1514.556885   -0.276131   711.459756  1421.978235   -0.379206  ...   \n",
       "1052  1530.085564   -0.252264   638.235927  1447.455406   -0.371612  ...   \n",
       "1053  1311.617374   -0.152483   900.285482  1261.919737   -0.125441  ...   \n",
       "1054  1383.484006   -0.221616   732.457399  1315.951228   -0.233599  ...   \n",
       "1055  1376.033545   -0.238790   766.105592  1297.483921   -0.279074  ...   \n",
       "\n",
       "      pinky_McpZ   pinky_PipX   pinky_PipY  pinky_PipZ   pinky_DipX  \\\n",
       "0      -0.014530   625.414029   841.904812   -0.030691   621.901855   \n",
       "1      -0.074243  1683.593624  1443.143311   -0.114158  1637.900345   \n",
       "2      -0.075024  1680.113028  1334.217393   -0.118665  1636.022493   \n",
       "3      -0.071267  1678.341120  1337.785852   -0.110229  1631.677766   \n",
       "4      -0.053522  1706.582711  1260.215963   -0.096525  1669.381164   \n",
       "...          ...          ...          ...         ...          ...   \n",
       "1051   -0.041158  1253.675699  1319.325686   -0.154479  1103.553176   \n",
       "1052   -0.123394  1241.890192  1443.705082   -0.246548  1078.716993   \n",
       "1053    0.223034  1054.138541  1293.740630    0.159143   914.128363   \n",
       "1054    0.175156  1110.132456  1363.412261    0.080964   984.978139   \n",
       "1055    0.094886  1289.921880  1314.355969   -0.003062  1141.063213   \n",
       "\n",
       "       pinky_DipY  pinky_DipZ   pinky_TipX   pinky_TipY  pinky_TipZ  \n",
       "0      885.472149   -0.008086   634.784594   893.365762    0.015983  \n",
       "1     1626.554544   -0.080055  1703.259765  1651.284102   -0.043401  \n",
       "2     1532.326854   -0.082507  1701.786802  1574.371294   -0.043058  \n",
       "3     1530.720809   -0.074036  1692.048390  1576.065932   -0.036066  \n",
       "4     1456.343893   -0.064412  1727.129797  1498.052231   -0.027742  \n",
       "...           ...         ...          ...          ...         ...  \n",
       "1051  1493.587255   -0.163301   963.837624  1532.385945   -0.143554  \n",
       "1052  1598.790646   -0.245476   946.605921  1613.921404   -0.219498  \n",
       "1053  1380.347967    0.114000   786.959291  1369.079351    0.106502  \n",
       "1054  1486.548662    0.038428   876.057625  1501.790762    0.041642  \n",
       "1055  1440.321207   -0.032022  1006.802082  1456.267476   -0.022182  \n",
       "\n",
       "[1056 rows x 64 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Training the model using Pandas\n",
    "df_train = pd.read_csv(\"hands_SIBI_training.csv\", header=0)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "overall-pharmaceutical",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>wristZ</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_CmcZ</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_McpZ</th>\n",
       "      <th>...</th>\n",
       "      <th>pinky_McpZ</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_PipZ</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_DipZ</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "      <th>pinky_TipZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1610.007216</td>\n",
       "      <td>1784.244035</td>\n",
       "      <td>-1.749235e-06</td>\n",
       "      <td>1369.551107</td>\n",
       "      <td>1709.763939</td>\n",
       "      <td>-0.042585</td>\n",
       "      <td>1148.568022</td>\n",
       "      <td>1496.684943</td>\n",
       "      <td>-0.062479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052158</td>\n",
       "      <td>1657.078684</td>\n",
       "      <td>1124.223124</td>\n",
       "      <td>-0.091841</td>\n",
       "      <td>1640.837864</td>\n",
       "      <td>1317.483665</td>\n",
       "      <td>-0.057763</td>\n",
       "      <td>1681.879869</td>\n",
       "      <td>1367.191519</td>\n",
       "      <td>-0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>721.901596</td>\n",
       "      <td>1655.649900</td>\n",
       "      <td>-1.420789e-06</td>\n",
       "      <td>530.846596</td>\n",
       "      <td>1427.740693</td>\n",
       "      <td>-0.034975</td>\n",
       "      <td>442.550361</td>\n",
       "      <td>1126.330376</td>\n",
       "      <td>-0.054130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051886</td>\n",
       "      <td>1116.067886</td>\n",
       "      <td>1094.982624</td>\n",
       "      <td>-0.091441</td>\n",
       "      <td>1040.838242</td>\n",
       "      <td>1238.941789</td>\n",
       "      <td>-0.066161</td>\n",
       "      <td>998.332918</td>\n",
       "      <td>1323.050618</td>\n",
       "      <td>-0.035342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>749.511778</td>\n",
       "      <td>1684.732795</td>\n",
       "      <td>-1.402203e-06</td>\n",
       "      <td>563.877583</td>\n",
       "      <td>1453.747034</td>\n",
       "      <td>-0.036049</td>\n",
       "      <td>486.337095</td>\n",
       "      <td>1149.076819</td>\n",
       "      <td>-0.054778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049362</td>\n",
       "      <td>1145.679951</td>\n",
       "      <td>1128.144860</td>\n",
       "      <td>-0.089420</td>\n",
       "      <td>1065.986991</td>\n",
       "      <td>1271.077037</td>\n",
       "      <td>-0.065419</td>\n",
       "      <td>1017.943382</td>\n",
       "      <td>1357.578039</td>\n",
       "      <td>-0.034874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>626.763403</td>\n",
       "      <td>1036.431742</td>\n",
       "      <td>-1.010617e-06</td>\n",
       "      <td>557.352796</td>\n",
       "      <td>1013.014567</td>\n",
       "      <td>-0.019210</td>\n",
       "      <td>502.974451</td>\n",
       "      <td>940.376868</td>\n",
       "      <td>-0.019415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>646.708667</td>\n",
       "      <td>850.894585</td>\n",
       "      <td>-0.045292</td>\n",
       "      <td>638.892889</td>\n",
       "      <td>895.978625</td>\n",
       "      <td>-0.022630</td>\n",
       "      <td>649.126634</td>\n",
       "      <td>910.227203</td>\n",
       "      <td>0.002203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>303.739804</td>\n",
       "      <td>3094.464386</td>\n",
       "      <td>2.659254e-07</td>\n",
       "      <td>373.745911</td>\n",
       "      <td>3178.373840</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>384.860658</td>\n",
       "      <td>3259.081604</td>\n",
       "      <td>-0.005684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074704</td>\n",
       "      <td>270.859761</td>\n",
       "      <td>3269.004730</td>\n",
       "      <td>0.101987</td>\n",
       "      <td>289.082896</td>\n",
       "      <td>3289.683472</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>303.885542</td>\n",
       "      <td>3296.251648</td>\n",
       "      <td>0.130978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Z</td>\n",
       "      <td>225.790128</td>\n",
       "      <td>1833.904896</td>\n",
       "      <td>-2.789371e-08</td>\n",
       "      <td>222.669122</td>\n",
       "      <td>1967.997986</td>\n",
       "      <td>-0.246023</td>\n",
       "      <td>430.404808</td>\n",
       "      <td>2049.844986</td>\n",
       "      <td>-0.340677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046414</td>\n",
       "      <td>1051.457326</td>\n",
       "      <td>1762.525051</td>\n",
       "      <td>-0.136801</td>\n",
       "      <td>910.432521</td>\n",
       "      <td>1864.261826</td>\n",
       "      <td>-0.133323</td>\n",
       "      <td>791.947807</td>\n",
       "      <td>1792.989178</td>\n",
       "      <td>-0.108027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Z</td>\n",
       "      <td>2572.288327</td>\n",
       "      <td>2967.599804</td>\n",
       "      <td>-1.137866e-07</td>\n",
       "      <td>2394.056368</td>\n",
       "      <td>2864.046263</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>2233.504243</td>\n",
       "      <td>2798.140757</td>\n",
       "      <td>-0.023914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079279</td>\n",
       "      <td>2248.028226</td>\n",
       "      <td>2977.749338</td>\n",
       "      <td>-0.090497</td>\n",
       "      <td>2285.416975</td>\n",
       "      <td>3009.534792</td>\n",
       "      <td>-0.083654</td>\n",
       "      <td>2350.825109</td>\n",
       "      <td>2980.246215</td>\n",
       "      <td>-0.078822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Z</td>\n",
       "      <td>1742.245886</td>\n",
       "      <td>2565.417858</td>\n",
       "      <td>2.051919e-07</td>\n",
       "      <td>1691.130518</td>\n",
       "      <td>2520.568737</td>\n",
       "      <td>-0.055253</td>\n",
       "      <td>1672.879678</td>\n",
       "      <td>2425.837498</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039117</td>\n",
       "      <td>1978.539119</td>\n",
       "      <td>2383.158772</td>\n",
       "      <td>-0.081821</td>\n",
       "      <td>1975.439507</td>\n",
       "      <td>2466.163536</td>\n",
       "      <td>-0.087077</td>\n",
       "      <td>1954.306755</td>\n",
       "      <td>2521.870468</td>\n",
       "      <td>-0.081235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Z</td>\n",
       "      <td>607.798517</td>\n",
       "      <td>1321.071744</td>\n",
       "      <td>7.056732e-07</td>\n",
       "      <td>662.470937</td>\n",
       "      <td>1287.229657</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>824.859679</td>\n",
       "      <td>1257.505655</td>\n",
       "      <td>-0.242309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023399</td>\n",
       "      <td>1208.418965</td>\n",
       "      <td>1214.795113</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>1095.283389</td>\n",
       "      <td>1289.289713</td>\n",
       "      <td>-0.056086</td>\n",
       "      <td>1015.160561</td>\n",
       "      <td>1262.510061</td>\n",
       "      <td>-0.042613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Z</td>\n",
       "      <td>687.228024</td>\n",
       "      <td>1365.019202</td>\n",
       "      <td>6.465051e-07</td>\n",
       "      <td>734.231889</td>\n",
       "      <td>1342.226267</td>\n",
       "      <td>-0.180457</td>\n",
       "      <td>910.643280</td>\n",
       "      <td>1326.208472</td>\n",
       "      <td>-0.231182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019234</td>\n",
       "      <td>1308.938861</td>\n",
       "      <td>1289.016008</td>\n",
       "      <td>-0.054561</td>\n",
       "      <td>1201.678514</td>\n",
       "      <td>1363.672256</td>\n",
       "      <td>-0.063278</td>\n",
       "      <td>1112.614751</td>\n",
       "      <td>1330.185056</td>\n",
       "      <td>-0.049138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_type       wristX       wristY        wristZ   thumb_CmcX  \\\n",
       "0            A  1610.007216  1784.244035 -1.749235e-06  1369.551107   \n",
       "1            A   721.901596  1655.649900 -1.420789e-06   530.846596   \n",
       "2            A   749.511778  1684.732795 -1.402203e-06   563.877583   \n",
       "3            A   626.763403  1036.431742 -1.010617e-06   557.352796   \n",
       "4            A   303.739804  3094.464386  2.659254e-07   373.745911   \n",
       "..         ...          ...          ...           ...          ...   \n",
       "207          Z   225.790128  1833.904896 -2.789371e-08   222.669122   \n",
       "208          Z  2572.288327  2967.599804 -1.137866e-07  2394.056368   \n",
       "209          Z  1742.245886  2565.417858  2.051919e-07  1691.130518   \n",
       "210          Z   607.798517  1321.071744  7.056732e-07   662.470937   \n",
       "211          Z   687.228024  1365.019202  6.465051e-07   734.231889   \n",
       "\n",
       "      thumb_CmcY  thumb_CmcZ   thumb_McpX   thumb_McpY  thumb_McpZ  ...  \\\n",
       "0    1709.763939   -0.042585  1148.568022  1496.684943   -0.062479  ...   \n",
       "1    1427.740693   -0.034975   442.550361  1126.330376   -0.054130  ...   \n",
       "2    1453.747034   -0.036049   486.337095  1149.076819   -0.054778  ...   \n",
       "3    1013.014567   -0.019210   502.974451   940.376868   -0.019415  ...   \n",
       "4    3178.373840   -0.009913   384.860658  3259.081604   -0.005684  ...   \n",
       "..           ...         ...          ...          ...         ...  ...   \n",
       "207  1967.997986   -0.246023   430.404808  2049.844986   -0.340677  ...   \n",
       "208  2864.046263   -0.004316  2233.504243  2798.140757   -0.023914  ...   \n",
       "209  2520.568737   -0.055253  1672.879678  2425.837498   -0.085446  ...   \n",
       "210  1287.229657   -0.190705   824.859679  1257.505655   -0.242309  ...   \n",
       "211  1342.226267   -0.180457   910.643280  1326.208472   -0.231182  ...   \n",
       "\n",
       "     pinky_McpZ   pinky_PipX   pinky_PipY  pinky_PipZ   pinky_DipX  \\\n",
       "0     -0.052158  1657.078684  1124.223124   -0.091841  1640.837864   \n",
       "1     -0.051886  1116.067886  1094.982624   -0.091441  1040.838242   \n",
       "2     -0.049362  1145.679951  1128.144860   -0.089420  1065.986991   \n",
       "3     -0.026157   646.708667   850.894585   -0.045292   638.892889   \n",
       "4      0.074704   270.859761  3269.004730    0.101987   289.082896   \n",
       "..          ...          ...          ...         ...          ...   \n",
       "207   -0.046414  1051.457326  1762.525051   -0.136801   910.432521   \n",
       "208   -0.079279  2248.028226  2977.749338   -0.090497  2285.416975   \n",
       "209   -0.039117  1978.539119  2383.158772   -0.081821  1975.439507   \n",
       "210    0.023399  1208.418965  1214.795113   -0.046823  1095.283389   \n",
       "211    0.019234  1308.938861  1289.016008   -0.054561  1201.678514   \n",
       "\n",
       "      pinky_DipY  pinky_DipZ   pinky_TipX   pinky_TipY  pinky_TipZ  \n",
       "0    1317.483665   -0.057763  1681.879869  1367.191519   -0.020736  \n",
       "1    1238.941789   -0.066161   998.332918  1323.050618   -0.035342  \n",
       "2    1271.077037   -0.065419  1017.943382  1357.578039   -0.034874  \n",
       "3     895.978625   -0.022630   649.126634   910.227203    0.002203  \n",
       "4    3289.683472    0.118750   303.885542  3296.251648    0.130978  \n",
       "..           ...         ...          ...          ...         ...  \n",
       "207  1864.261826   -0.133323   791.947807  1792.989178   -0.108027  \n",
       "208  3009.534792   -0.083654  2350.825109  2980.246215   -0.078822  \n",
       "209  2466.163536   -0.087077  1954.306755  2521.870468   -0.081235  \n",
       "210  1289.289713   -0.056086  1015.160561  1262.510061   -0.042613  \n",
       "211  1363.672256   -0.063278  1112.614751  1330.185056   -0.049138  \n",
       "\n",
       "[212 rows x 64 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Validation or Testing the Model using Pandas\n",
    "df_test = pd.read_csv(\"hands_SIBI_validation.csv\", header=0)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "confirmed-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Categorical using Pandas\n",
    "df_train[\"class_type\"] = pd.Categorical(df_train[\"class_type\"])\n",
    "df_train[\"class_type\"] = df_train.class_type.cat.codes\n",
    "\n",
    "df_test[\"class_type\"] = pd.Categorical(df_test[\"class_type\"])\n",
    "df_test[\"class_type\"] = df_test.class_type.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "current-netscape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Label and Feature for training\n",
    "y_train = df_train.pop(\"class_type\")\n",
    "x_train = df_train.copy()\n",
    "\n",
    "y_test = df_test.pop(\"class_type\")\n",
    "x_test = df_test.copy()\n",
    "\n",
    "# Copied Features turn to Array by using NumPy\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Normalize the Datasets\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "imported-candle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1056, 63)\n",
      "(212, 63)\n",
      "(1056, 63, 1)\n",
      "(212, 63, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check Array Shape before transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Since the array shape is 1x1, we must turn it into 1x10x1 so we can feed it into the model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Check Array Shape after transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "sustainable-frontier",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.483]\n",
      " [ 4.046]\n",
      " [-0.   ]\n",
      " [ 2.168]\n",
      " [ 3.979]\n",
      " [-0.   ]\n",
      " [ 1.917]\n",
      " [ 3.726]\n",
      " [-0.   ]\n",
      " [ 1.785]\n",
      " [ 3.51 ]\n",
      " [-0.   ]\n",
      " [ 1.706]\n",
      " [ 3.333]\n",
      " [-0.   ]\n",
      " [ 2.006]\n",
      " [ 3.451]\n",
      " [ 0.   ]\n",
      " [ 1.888]\n",
      " [ 3.332]\n",
      " [-0.   ]\n",
      " [ 1.971]\n",
      " [ 3.553]\n",
      " [-0.   ]\n",
      " [ 2.048]\n",
      " [ 3.638]\n",
      " [-0.   ]\n",
      " [ 2.176]\n",
      " [ 3.398]\n",
      " [ 0.   ]\n",
      " [ 2.051]\n",
      " [ 3.301]\n",
      " [-0.   ]\n",
      " [ 2.123]\n",
      " [ 3.565]\n",
      " [-0.   ]\n",
      " [ 2.194]\n",
      " [ 3.584]\n",
      " [-0.   ]\n",
      " [ 2.364]\n",
      " [ 3.379]\n",
      " [ 0.   ]\n",
      " [ 2.242]\n",
      " [ 3.276]\n",
      " [-0.   ]\n",
      " [ 2.281]\n",
      " [ 3.527]\n",
      " [-0.   ]\n",
      " [ 2.352]\n",
      " [ 3.554]\n",
      " [ 0.   ]\n",
      " [ 2.559]\n",
      " [ 3.385]\n",
      " [-0.   ]\n",
      " [ 2.453]\n",
      " [ 3.302]\n",
      " [-0.   ]\n",
      " [ 2.439]\n",
      " [ 3.472]\n",
      " [-0.   ]\n",
      " [ 2.489]\n",
      " [ 3.503]\n",
      " [ 0.   ]]\n",
      "[[11.362]\n",
      " [ 8.714]\n",
      " [-0.   ]\n",
      " [10.547]\n",
      " [ 7.91 ]\n",
      " [-0.   ]\n",
      " [10.144]\n",
      " [ 7.111]\n",
      " [ 0.   ]\n",
      " [10.096]\n",
      " [ 6.262]\n",
      " [ 0.   ]\n",
      " [10.108]\n",
      " [ 5.641]\n",
      " [ 0.   ]\n",
      " [10.686]\n",
      " [ 6.474]\n",
      " [ 0.   ]\n",
      " [10.615]\n",
      " [ 5.83 ]\n",
      " [ 0.   ]\n",
      " [10.546]\n",
      " [ 6.443]\n",
      " [ 0.   ]\n",
      " [10.569]\n",
      " [ 6.712]\n",
      " [ 0.   ]\n",
      " [11.125]\n",
      " [ 6.564]\n",
      " [ 0.   ]\n",
      " [11.013]\n",
      " [ 5.979]\n",
      " [ 0.   ]\n",
      " [10.872]\n",
      " [ 6.715]\n",
      " [ 0.   ]\n",
      " [10.898]\n",
      " [ 6.891]\n",
      " [ 0.   ]\n",
      " [11.596]\n",
      " [ 6.668]\n",
      " [ 0.   ]\n",
      " [11.497]\n",
      " [ 6.144]\n",
      " [ 0.   ]\n",
      " [11.266]\n",
      " [ 6.858]\n",
      " [ 0.   ]\n",
      " [11.269]\n",
      " [ 7.077]\n",
      " [ 0.   ]\n",
      " [12.082]\n",
      " [ 6.828]\n",
      " [ 0.   ]\n",
      " [11.994]\n",
      " [ 6.333]\n",
      " [ 0.   ]\n",
      " [11.708]\n",
      " [ 6.801]\n",
      " [ 0.   ]\n",
      " [11.649]\n",
      " [ 7.046]\n",
      " [ 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Check sample train and test features\n",
    "print(x_train[0])\n",
    "print(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "revolutionary-frontier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes according standard Alphabets\n",
    "num_classes = 26\n",
    "\n",
    "# Using the Keras.Utils to put the label categorically \n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "august-concentrate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"SIBI_ML_Model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_64 (Conv1D)           (None, 63, 32)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_65 (Conv1D)           (None, 63, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_32 (MaxPooling (None, 31, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_66 (Conv1D)           (None, 31, 64)            10304     \n",
      "_________________________________________________________________\n",
      "conv1d_67 (Conv1D)           (None, 31, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_33 (MaxPooling (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_68 (Conv1D)           (None, 15, 128)           41088     \n",
      "_________________________________________________________________\n",
      "conv1d_69 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_34 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_70 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "conv1d_71 (Conv1D)           (None, 7, 256)            327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_35 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 1,058,426\n",
      "Trainable params: 1,058,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One Dimensional Convolutional Neural Network model, Train will be feed to 1 Dimension Convolutional Neural Network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "\n",
    "model._name=\"SIBI_ML_Model\"\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "leading-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('val_accuracy')>0.97) and (logs.get('accuracy')>0.99):\n",
    "      print(\"\\nReached 97% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "boolean-habitat",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 2/33 [>.............................] - ETA: 1s - loss: 3.2580 - accuracy: 0.0000e+00WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0780s). Check your callbacks.\n",
      "33/33 [==============================] - 3s 76ms/step - loss: 3.2618 - accuracy: 0.0275 - val_loss: 3.2579 - val_accuracy: 0.0425\n",
      "Epoch 2/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 3.2587 - accuracy: 0.0379 - val_loss: 3.2580 - val_accuracy: 0.0377\n",
      "Epoch 3/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 3.2526 - accuracy: 0.0379 - val_loss: 3.2384 - val_accuracy: 0.0566\n",
      "Epoch 4/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 3.2367 - accuracy: 0.0625 - val_loss: 3.2087 - val_accuracy: 0.0708\n",
      "Epoch 5/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 3.1117 - accuracy: 0.0729 - val_loss: 3.0512 - val_accuracy: 0.0660\n",
      "Epoch 6/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 2.8641 - accuracy: 0.0729 - val_loss: 2.8479 - val_accuracy: 0.1179\n",
      "Epoch 7/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 2.6593 - accuracy: 0.1174 - val_loss: 2.5190 - val_accuracy: 0.1792\n",
      "Epoch 8/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 2.4528 - accuracy: 0.1695 - val_loss: 2.3351 - val_accuracy: 0.2217\n",
      "Epoch 9/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 2.1206 - accuracy: 0.2462 - val_loss: 2.1205 - val_accuracy: 0.2689\n",
      "Epoch 10/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 1.7749 - accuracy: 0.3873 - val_loss: 1.5695 - val_accuracy: 0.4717\n",
      "Epoch 11/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 1.4242 - accuracy: 0.5057 - val_loss: 1.3433 - val_accuracy: 0.5283\n",
      "Epoch 12/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 1.1601 - accuracy: 0.5852 - val_loss: 1.0964 - val_accuracy: 0.6274\n",
      "Epoch 13/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.9198 - accuracy: 0.6506 - val_loss: 0.7411 - val_accuracy: 0.7453\n",
      "Epoch 14/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.7704 - accuracy: 0.7121 - val_loss: 0.8534 - val_accuracy: 0.7075\n",
      "Epoch 15/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.7503 - accuracy: 0.7377 - val_loss: 0.8341 - val_accuracy: 0.7028\n",
      "Epoch 16/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.6107 - accuracy: 0.7898 - val_loss: 0.5302 - val_accuracy: 0.8208\n",
      "Epoch 17/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.4514 - accuracy: 0.8295 - val_loss: 0.4858 - val_accuracy: 0.8396\n",
      "Epoch 18/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.3727 - accuracy: 0.8475 - val_loss: 0.4295 - val_accuracy: 0.8585\n",
      "Epoch 19/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.3790 - accuracy: 0.8598 - val_loss: 0.4463 - val_accuracy: 0.8585\n",
      "Epoch 20/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.4316 - accuracy: 0.8333 - val_loss: 0.4494 - val_accuracy: 0.8491\n",
      "Epoch 21/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.4302 - accuracy: 0.8485 - val_loss: 0.5221 - val_accuracy: 0.8160\n",
      "Epoch 22/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.3568 - accuracy: 0.8693 - val_loss: 0.4089 - val_accuracy: 0.8726\n",
      "Epoch 23/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.3252 - accuracy: 0.8816 - val_loss: 0.3616 - val_accuracy: 0.8774\n",
      "Epoch 24/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.3010 - accuracy: 0.8845 - val_loss: 0.3182 - val_accuracy: 0.8962\n",
      "Epoch 25/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.2753 - accuracy: 0.9006 - val_loss: 0.3666 - val_accuracy: 0.8726\n",
      "Epoch 26/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.2608 - accuracy: 0.8977 - val_loss: 0.2953 - val_accuracy: 0.9104\n",
      "Epoch 27/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.2618 - accuracy: 0.9044 - val_loss: 0.4165 - val_accuracy: 0.8679\n",
      "Epoch 28/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2832 - accuracy: 0.8939 - val_loss: 0.4298 - val_accuracy: 0.8868\n",
      "Epoch 29/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2364 - accuracy: 0.9252 - val_loss: 0.3470 - val_accuracy: 0.8915\n",
      "Epoch 30/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2720 - accuracy: 0.8968 - val_loss: 0.3066 - val_accuracy: 0.8726\n",
      "Epoch 31/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.1757 - accuracy: 0.9375 - val_loss: 0.2420 - val_accuracy: 0.9245\n",
      "Epoch 32/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1573 - accuracy: 0.9422 - val_loss: 0.3078 - val_accuracy: 0.9104\n",
      "Epoch 33/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.1726 - accuracy: 0.9366 - val_loss: 0.2573 - val_accuracy: 0.9151\n",
      "Epoch 34/1000\n",
      "33/33 [==============================] - 3s 79ms/step - loss: 0.1996 - accuracy: 0.9290 - val_loss: 0.5989 - val_accuracy: 0.8443\n",
      "Epoch 35/1000\n",
      "33/33 [==============================] - 3s 84ms/step - loss: 0.2918 - accuracy: 0.8977 - val_loss: 0.3180 - val_accuracy: 0.8915\n",
      "Epoch 36/1000\n",
      "33/33 [==============================] - 3s 81ms/step - loss: 0.2496 - accuracy: 0.8996 - val_loss: 0.4322 - val_accuracy: 0.8915\n",
      "Epoch 37/1000\n",
      "33/33 [==============================] - 2s 69ms/step - loss: 0.2282 - accuracy: 0.9176 - val_loss: 0.3166 - val_accuracy: 0.9104\n",
      "Epoch 38/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.2232 - accuracy: 0.9205 - val_loss: 0.3519 - val_accuracy: 0.8821\n",
      "Epoch 39/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.2618 - accuracy: 0.9044 - val_loss: 0.3631 - val_accuracy: 0.8774\n",
      "Epoch 40/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1728 - accuracy: 0.9299 - val_loss: 0.3001 - val_accuracy: 0.9057\n",
      "Epoch 41/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1324 - accuracy: 0.9470 - val_loss: 0.4470 - val_accuracy: 0.8774\n",
      "Epoch 42/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1258 - accuracy: 0.9508 - val_loss: 0.2573 - val_accuracy: 0.9292\n",
      "Epoch 43/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1308 - accuracy: 0.9545 - val_loss: 0.2260 - val_accuracy: 0.9387\n",
      "Epoch 44/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.1502 - accuracy: 0.9441 - val_loss: 0.2381 - val_accuracy: 0.9198\n",
      "Epoch 45/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.1216 - accuracy: 0.9564 - val_loss: 0.3251 - val_accuracy: 0.8962\n",
      "Epoch 46/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1804 - accuracy: 0.9347 - val_loss: 0.3344 - val_accuracy: 0.8962\n",
      "Epoch 47/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1317 - accuracy: 0.9555 - val_loss: 0.2837 - val_accuracy: 0.9009\n",
      "Epoch 48/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.3049 - accuracy: 0.9110 - val_loss: 0.4135 - val_accuracy: 0.8632\n",
      "Epoch 49/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.1472 - accuracy: 0.9498 - val_loss: 0.2554 - val_accuracy: 0.9198\n",
      "Epoch 50/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.1365 - accuracy: 0.9498 - val_loss: 0.2842 - val_accuracy: 0.9340\n",
      "Epoch 51/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2027 - accuracy: 0.9223 - val_loss: 0.2964 - val_accuracy: 0.9104\n",
      "Epoch 52/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0965 - accuracy: 0.9678 - val_loss: 0.2099 - val_accuracy: 0.9387\n",
      "Epoch 53/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0712 - accuracy: 0.9744 - val_loss: 0.2217 - val_accuracy: 0.9481\n",
      "Epoch 54/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0697 - accuracy: 0.9763 - val_loss: 0.2869 - val_accuracy: 0.9481\n",
      "Epoch 55/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.1115 - accuracy: 0.9555 - val_loss: 0.2593 - val_accuracy: 0.9198\n",
      "Epoch 56/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.2368 - accuracy: 0.9309 - val_loss: 0.2929 - val_accuracy: 0.8962\n",
      "Epoch 57/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0985 - accuracy: 0.9669 - val_loss: 0.2418 - val_accuracy: 0.9245\n",
      "Epoch 58/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1375 - accuracy: 0.9564 - val_loss: 0.2944 - val_accuracy: 0.9292\n",
      "Epoch 59/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1858 - accuracy: 0.9328 - val_loss: 0.5174 - val_accuracy: 0.8632\n",
      "Epoch 60/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1481 - accuracy: 0.9470 - val_loss: 0.2153 - val_accuracy: 0.9292\n",
      "Epoch 61/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.1042 - accuracy: 0.9583 - val_loss: 0.2624 - val_accuracy: 0.9245\n",
      "Epoch 62/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0649 - accuracy: 0.9725 - val_loss: 0.2230 - val_accuracy: 0.9387\n",
      "Epoch 63/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0684 - accuracy: 0.9763 - val_loss: 0.1628 - val_accuracy: 0.9575\n",
      "Epoch 64/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.0540 - accuracy: 0.9811 - val_loss: 0.2541 - val_accuracy: 0.9340\n",
      "Epoch 65/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0858 - accuracy: 0.9659 - val_loss: 0.2254 - val_accuracy: 0.9387\n",
      "Epoch 66/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0829 - accuracy: 0.9678 - val_loss: 0.2493 - val_accuracy: 0.9245\n",
      "Epoch 67/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0665 - accuracy: 0.9716 - val_loss: 0.2802 - val_accuracy: 0.9340\n",
      "Epoch 68/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1066 - accuracy: 0.9621 - val_loss: 0.2448 - val_accuracy: 0.9198\n",
      "Epoch 69/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1618 - accuracy: 0.9413 - val_loss: 0.2703 - val_accuracy: 0.9151\n",
      "Epoch 70/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.1371 - accuracy: 0.9508 - val_loss: 0.3885 - val_accuracy: 0.9104\n",
      "Epoch 71/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0786 - accuracy: 0.9716 - val_loss: 0.2154 - val_accuracy: 0.9292\n",
      "Epoch 72/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0924 - accuracy: 0.9659 - val_loss: 0.2587 - val_accuracy: 0.9151\n",
      "Epoch 73/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0748 - accuracy: 0.9716 - val_loss: 0.2432 - val_accuracy: 0.9245\n",
      "Epoch 74/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0725 - accuracy: 0.9725 - val_loss: 0.2682 - val_accuracy: 0.9340\n",
      "Epoch 75/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0870 - accuracy: 0.9650 - val_loss: 0.3131 - val_accuracy: 0.9104\n",
      "Epoch 76/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1074 - accuracy: 0.9659 - val_loss: 0.3045 - val_accuracy: 0.9245\n",
      "Epoch 77/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0709 - accuracy: 0.9744 - val_loss: 0.2688 - val_accuracy: 0.9528\n",
      "Epoch 78/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0861 - accuracy: 0.9716 - val_loss: 0.3278 - val_accuracy: 0.9340\n",
      "Epoch 79/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0796 - accuracy: 0.9697 - val_loss: 0.3221 - val_accuracy: 0.9151\n",
      "Epoch 80/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1315 - accuracy: 0.9545 - val_loss: 0.5903 - val_accuracy: 0.8396\n",
      "Epoch 81/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.2606 - accuracy: 0.9157 - val_loss: 0.4800 - val_accuracy: 0.8349\n",
      "Epoch 82/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1551 - accuracy: 0.9527 - val_loss: 0.2350 - val_accuracy: 0.9434\n",
      "Epoch 83/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0898 - accuracy: 0.9669 - val_loss: 0.3666 - val_accuracy: 0.9245\n",
      "Epoch 84/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0985 - accuracy: 0.9650 - val_loss: 0.4676 - val_accuracy: 0.8632\n",
      "Epoch 85/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1659 - accuracy: 0.9460 - val_loss: 0.2290 - val_accuracy: 0.9340\n",
      "Epoch 86/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.1563 - accuracy: 0.9555 - val_loss: 0.3613 - val_accuracy: 0.9057\n",
      "Epoch 87/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0622 - accuracy: 0.9811 - val_loss: 0.2895 - val_accuracy: 0.9340\n",
      "Epoch 88/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0559 - accuracy: 0.9792 - val_loss: 0.3059 - val_accuracy: 0.9245\n",
      "Epoch 89/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.0988 - accuracy: 0.9640 - val_loss: 0.3455 - val_accuracy: 0.9104\n",
      "Epoch 90/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0575 - accuracy: 0.9773 - val_loss: 0.2709 - val_accuracy: 0.9387\n",
      "Epoch 91/1000\n",
      "33/33 [==============================] - 2s 55ms/step - loss: 0.1820 - accuracy: 0.9517 - val_loss: 0.6833 - val_accuracy: 0.8208\n",
      "Epoch 92/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.2128 - accuracy: 0.9261 - val_loss: 0.3453 - val_accuracy: 0.9151\n",
      "Epoch 93/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.1458 - accuracy: 0.9650 - val_loss: 0.2459 - val_accuracy: 0.9340\n",
      "Epoch 94/1000\n",
      "33/33 [==============================] - 2s 74ms/step - loss: 0.0789 - accuracy: 0.9754 - val_loss: 0.2661 - val_accuracy: 0.9387\n",
      "Epoch 95/1000\n",
      "33/33 [==============================] - 3s 85ms/step - loss: 0.0485 - accuracy: 0.9820 - val_loss: 0.3214 - val_accuracy: 0.9292\n",
      "Epoch 96/1000\n",
      "33/33 [==============================] - 4s 116ms/step - loss: 0.1043 - accuracy: 0.9669 - val_loss: 0.3200 - val_accuracy: 0.9057\n",
      "Epoch 97/1000\n",
      "33/33 [==============================] - 3s 76ms/step - loss: 0.0638 - accuracy: 0.9773 - val_loss: 0.1944 - val_accuracy: 0.9481\n",
      "Epoch 98/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0512 - accuracy: 0.9792 - val_loss: 0.4044 - val_accuracy: 0.9245\n",
      "Epoch 99/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0882 - accuracy: 0.9716 - val_loss: 0.2398 - val_accuracy: 0.9434\n",
      "Epoch 100/1000\n",
      "33/33 [==============================] - 3s 95ms/step - loss: 0.0927 - accuracy: 0.9697 - val_loss: 0.2936 - val_accuracy: 0.9245\n",
      "Epoch 101/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1052 - accuracy: 0.9631 - val_loss: 0.3439 - val_accuracy: 0.9151\n",
      "Epoch 102/1000\n",
      "33/33 [==============================] - 3s 99ms/step - loss: 0.0767 - accuracy: 0.9725 - val_loss: 0.2531 - val_accuracy: 0.9434\n",
      "Epoch 103/1000\n",
      "33/33 [==============================] - 2s 74ms/step - loss: 0.0768 - accuracy: 0.9706 - val_loss: 0.3979 - val_accuracy: 0.9245\n",
      "Epoch 104/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0627 - accuracy: 0.9811 - val_loss: 0.3061 - val_accuracy: 0.9245\n",
      "Epoch 105/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0816 - accuracy: 0.9725 - val_loss: 0.2107 - val_accuracy: 0.9387\n",
      "Epoch 106/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0656 - accuracy: 0.9801 - val_loss: 0.2724 - val_accuracy: 0.9434\n",
      "Epoch 107/1000\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.0588 - accuracy: 0.9792 - val_loss: 0.2360 - val_accuracy: 0.9434\n",
      "Epoch 108/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0474 - accuracy: 0.9811 - val_loss: 0.2826 - val_accuracy: 0.9387\n",
      "Epoch 109/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.0908 - accuracy: 0.9744 - val_loss: 0.2141 - val_accuracy: 0.9245\n",
      "Epoch 110/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0521 - accuracy: 0.9792 - val_loss: 0.2253 - val_accuracy: 0.9387\n",
      "Epoch 111/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0578 - accuracy: 0.9820 - val_loss: 0.3192 - val_accuracy: 0.9104\n",
      "Epoch 112/1000\n",
      "33/33 [==============================] - 3s 84ms/step - loss: 0.1146 - accuracy: 0.9545 - val_loss: 0.4170 - val_accuracy: 0.9009\n",
      "Epoch 113/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.1623 - accuracy: 0.9498 - val_loss: 0.3799 - val_accuracy: 0.9057\n",
      "Epoch 114/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0888 - accuracy: 0.9735 - val_loss: 0.3125 - val_accuracy: 0.9245\n",
      "Epoch 115/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0510 - accuracy: 0.9763 - val_loss: 0.2831 - val_accuracy: 0.9387\n",
      "Epoch 116/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0323 - accuracy: 0.9886 - val_loss: 0.3018 - val_accuracy: 0.9528\n",
      "Epoch 117/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0474 - accuracy: 0.9811 - val_loss: 0.2924 - val_accuracy: 0.9292\n",
      "Epoch 118/1000\n",
      "33/33 [==============================] - 2s 67ms/step - loss: 0.1164 - accuracy: 0.9640 - val_loss: 0.3615 - val_accuracy: 0.9198\n",
      "Epoch 119/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.1810 - accuracy: 0.9394 - val_loss: 0.2826 - val_accuracy: 0.9151\n",
      "Epoch 120/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0697 - accuracy: 0.9782 - val_loss: 0.3544 - val_accuracy: 0.9292\n",
      "Epoch 121/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0626 - accuracy: 0.9801 - val_loss: 0.2073 - val_accuracy: 0.9292\n",
      "Epoch 122/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0545 - accuracy: 0.9820 - val_loss: 0.2815 - val_accuracy: 0.9340\n",
      "Epoch 123/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0474 - accuracy: 0.9811 - val_loss: 0.2545 - val_accuracy: 0.9670\n",
      "Epoch 124/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0546 - accuracy: 0.9811 - val_loss: 0.2881 - val_accuracy: 0.9292\n",
      "Epoch 125/1000\n",
      "33/33 [==============================] - 2s 67ms/step - loss: 0.0647 - accuracy: 0.9773 - val_loss: 0.2559 - val_accuracy: 0.9528\n",
      "Epoch 126/1000\n",
      "33/33 [==============================] - 2s 56ms/step - loss: 0.0688 - accuracy: 0.9716 - val_loss: 0.2497 - val_accuracy: 0.9387\n",
      "Epoch 127/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.0978 - accuracy: 0.9688 - val_loss: 0.2746 - val_accuracy: 0.9292\n",
      "Epoch 128/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0603 - accuracy: 0.9773 - val_loss: 0.2518 - val_accuracy: 0.9340\n",
      "Epoch 129/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0381 - accuracy: 0.9858 - val_loss: 0.2761 - val_accuracy: 0.9292\n",
      "Epoch 130/1000\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.1330 - accuracy: 0.96 - 2s 71ms/step - loss: 0.1330 - accuracy: 0.9602 - val_loss: 0.3892 - val_accuracy: 0.9151\n",
      "Epoch 131/1000\n",
      "33/33 [==============================] - 3s 76ms/step - loss: 0.0798 - accuracy: 0.9716 - val_loss: 0.2234 - val_accuracy: 0.9481\n",
      "Epoch 132/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0606 - accuracy: 0.9792 - val_loss: 0.2775 - val_accuracy: 0.9387\n",
      "Epoch 133/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1290 - accuracy: 0.9688 - val_loss: 0.2004 - val_accuracy: 0.9434\n",
      "Epoch 134/1000\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.1095 - accuracy: 0.9631 - val_loss: 0.4160 - val_accuracy: 0.9198\n",
      "Epoch 135/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1053 - accuracy: 0.9650 - val_loss: 0.2467 - val_accuracy: 0.9151\n",
      "Epoch 136/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0476 - accuracy: 0.9839 - val_loss: 0.2664 - val_accuracy: 0.9481\n",
      "Epoch 137/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0795 - accuracy: 0.9792 - val_loss: 0.3186 - val_accuracy: 0.9151\n",
      "Epoch 138/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0588 - accuracy: 0.9801 - val_loss: 0.2947 - val_accuracy: 0.9387\n",
      "Epoch 139/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0465 - accuracy: 0.9830 - val_loss: 0.2566 - val_accuracy: 0.9340\n",
      "Epoch 140/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0629 - accuracy: 0.9706 - val_loss: 0.2785 - val_accuracy: 0.9245\n",
      "Epoch 141/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0604 - accuracy: 0.9782 - val_loss: 0.2701 - val_accuracy: 0.9245\n",
      "Epoch 142/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0345 - accuracy: 0.9839 - val_loss: 0.2492 - val_accuracy: 0.9387\n",
      "Epoch 143/1000\n",
      "33/33 [==============================] - 2s 72ms/step - loss: 0.0346 - accuracy: 0.9839 - val_loss: 0.2912 - val_accuracy: 0.9340\n",
      "Epoch 144/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0344 - accuracy: 0.9848 - val_loss: 0.2200 - val_accuracy: 0.9575\n",
      "Epoch 145/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0328 - accuracy: 0.9848 - val_loss: 0.2233 - val_accuracy: 0.9434\n",
      "Epoch 146/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0413 - accuracy: 0.9820 - val_loss: 0.2522 - val_accuracy: 0.9340\n",
      "Epoch 147/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.2868 - val_accuracy: 0.9481\n",
      "Epoch 148/1000\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.0222 - accuracy: 0.9896 - val_loss: 0.3153 - val_accuracy: 0.9434\n",
      "Epoch 149/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0452 - accuracy: 0.9830 - val_loss: 0.2859 - val_accuracy: 0.9434\n",
      "Epoch 150/1000\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.0269 - accuracy: 0.9905 - val_loss: 0.2570 - val_accuracy: 0.9528\n",
      "Epoch 151/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0267 - accuracy: 0.9905 - val_loss: 0.2337 - val_accuracy: 0.9387\n",
      "Epoch 152/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0197 - accuracy: 0.9905 - val_loss: 0.2518 - val_accuracy: 0.9481\n",
      "Epoch 153/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0240 - accuracy: 0.9915 - val_loss: 0.2598 - val_accuracy: 0.9481\n",
      "Epoch 154/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0200 - accuracy: 0.9924 - val_loss: 0.2319 - val_accuracy: 0.9528\n",
      "Epoch 155/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0187 - accuracy: 0.9934 - val_loss: 0.2500 - val_accuracy: 0.9481\n",
      "Epoch 156/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0213 - accuracy: 0.9915 - val_loss: 0.2433 - val_accuracy: 0.9434\n",
      "Epoch 157/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0523 - accuracy: 0.9830 - val_loss: 0.2479 - val_accuracy: 0.9434\n",
      "Epoch 158/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0397 - accuracy: 0.9858 - val_loss: 0.2572 - val_accuracy: 0.9528\n",
      "Epoch 159/1000\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.0210 - accuracy: 0.9915 - val_loss: 0.3006 - val_accuracy: 0.9528\n",
      "Epoch 160/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0299 - accuracy: 0.9905 - val_loss: 0.2830 - val_accuracy: 0.9434\n",
      "Epoch 161/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0221 - accuracy: 0.9896 - val_loss: 0.2435 - val_accuracy: 0.9575\n",
      "Epoch 162/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0224 - accuracy: 0.9924 - val_loss: 0.2624 - val_accuracy: 0.9623\n",
      "Epoch 163/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0199 - accuracy: 0.9934 - val_loss: 0.2943 - val_accuracy: 0.9387\n",
      "Epoch 164/1000\n",
      "33/33 [==============================] - 2s 57ms/step - loss: 0.0256 - accuracy: 0.9877 - val_loss: 0.2906 - val_accuracy: 0.9434\n",
      "Epoch 165/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0209 - accuracy: 0.9896 - val_loss: 0.2776 - val_accuracy: 0.9528\n",
      "Epoch 166/1000\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.0215 - accuracy: 0.9934 - val_loss: 0.3026 - val_accuracy: 0.9575\n",
      "Epoch 167/1000\n",
      "33/33 [==============================] - 2s 65ms/step - loss: 0.0619 - accuracy: 0.9839 - val_loss: 0.3403 - val_accuracy: 0.9198\n",
      "Epoch 168/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.1220 - accuracy: 0.9612 - val_loss: 0.3839 - val_accuracy: 0.9009\n",
      "Epoch 169/1000\n",
      "33/33 [==============================] - 3s 80ms/step - loss: 0.2040 - accuracy: 0.9432 - val_loss: 0.2744 - val_accuracy: 0.9104\n",
      "Epoch 170/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.1113 - accuracy: 0.9602 - val_loss: 0.2974 - val_accuracy: 0.9434\n",
      "Epoch 171/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1413 - accuracy: 0.9612 - val_loss: 0.3202 - val_accuracy: 0.9292\n",
      "Epoch 172/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.2722 - accuracy: 0.9318 - val_loss: 0.3974 - val_accuracy: 0.9104\n",
      "Epoch 173/1000\n",
      "33/33 [==============================] - 2s 72ms/step - loss: 0.1930 - accuracy: 0.9337 - val_loss: 0.2434 - val_accuracy: 0.9434\n",
      "Epoch 174/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0951 - accuracy: 0.9669 - val_loss: 0.2811 - val_accuracy: 0.9434\n",
      "Epoch 175/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0676 - accuracy: 0.9744 - val_loss: 0.2494 - val_accuracy: 0.9340\n",
      "Epoch 176/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0453 - accuracy: 0.9792 - val_loss: 0.2921 - val_accuracy: 0.9481\n",
      "Epoch 177/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0621 - accuracy: 0.9763 - val_loss: 0.3431 - val_accuracy: 0.9104\n",
      "Epoch 178/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0942 - accuracy: 0.9688 - val_loss: 0.4358 - val_accuracy: 0.9057\n",
      "Epoch 179/1000\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.1772 - accuracy: 0.9489 - val_loss: 0.4721 - val_accuracy: 0.9057\n",
      "Epoch 180/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1124 - accuracy: 0.9612 - val_loss: 0.3966 - val_accuracy: 0.8962\n",
      "Epoch 181/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.2658 - accuracy: 0.9242 - val_loss: 0.2715 - val_accuracy: 0.9292\n",
      "Epoch 182/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.1010 - accuracy: 0.9678 - val_loss: 0.2465 - val_accuracy: 0.9245\n",
      "Epoch 183/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0843 - accuracy: 0.9763 - val_loss: 0.1850 - val_accuracy: 0.9575\n",
      "Epoch 184/1000\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0410 - accuracy: 0.9867 - val_loss: 0.2502 - val_accuracy: 0.9528\n",
      "Epoch 185/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0720 - accuracy: 0.9706 - val_loss: 0.2107 - val_accuracy: 0.9623\n",
      "Epoch 186/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0926 - accuracy: 0.9688 - val_loss: 0.2402 - val_accuracy: 0.9387\n",
      "Epoch 187/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0498 - accuracy: 0.9848 - val_loss: 0.2640 - val_accuracy: 0.9434\n",
      "Epoch 188/1000\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0742 - accuracy: 0.9773 - val_loss: 0.2592 - val_accuracy: 0.9481\n",
      "Epoch 189/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0778 - accuracy: 0.9744 - val_loss: 0.3035 - val_accuracy: 0.9528\n",
      "Epoch 190/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0554 - accuracy: 0.9754 - val_loss: 0.2207 - val_accuracy: 0.9434\n",
      "Epoch 191/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0485 - accuracy: 0.9848 - val_loss: 0.2860 - val_accuracy: 0.9528\n",
      "Epoch 192/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0804 - accuracy: 0.9697 - val_loss: 0.2644 - val_accuracy: 0.9528\n",
      "Epoch 193/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0537 - accuracy: 0.9782 - val_loss: 0.3535 - val_accuracy: 0.9340\n",
      "Epoch 194/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0594 - accuracy: 0.9792 - val_loss: 0.2915 - val_accuracy: 0.9387\n",
      "Epoch 195/1000\n",
      "33/33 [==============================] - 2s 58ms/step - loss: 0.0884 - accuracy: 0.9688 - val_loss: 0.2774 - val_accuracy: 0.9387\n",
      "Epoch 196/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0838 - accuracy: 0.9688 - val_loss: 0.2958 - val_accuracy: 0.9434\n",
      "Epoch 197/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0502 - accuracy: 0.9820 - val_loss: 0.3978 - val_accuracy: 0.9292\n",
      "Epoch 198/1000\n",
      "33/33 [==============================] - 3s 78ms/step - loss: 0.0874 - accuracy: 0.9688 - val_loss: 0.2680 - val_accuracy: 0.9387\n",
      "Epoch 199/1000\n",
      "33/33 [==============================] - 2s 73ms/step - loss: 0.0520 - accuracy: 0.9867 - val_loss: 0.3485 - val_accuracy: 0.9434\n",
      "Epoch 200/1000\n",
      "33/33 [==============================] - 4s 128ms/step - loss: 0.0681 - accuracy: 0.9792 - val_loss: 0.2668 - val_accuracy: 0.9340\n",
      "Epoch 201/1000\n",
      "33/33 [==============================] - 4s 120ms/step - loss: 0.0470 - accuracy: 0.9820 - val_loss: 0.3024 - val_accuracy: 0.9528\n",
      "Epoch 202/1000\n",
      "33/33 [==============================] - 3s 98ms/step - loss: 0.0354 - accuracy: 0.9877 - val_loss: 0.2972 - val_accuracy: 0.9575\n",
      "Epoch 203/1000\n",
      "33/33 [==============================] - 3s 91ms/step - loss: 0.0223 - accuracy: 0.9886 - val_loss: 0.3434 - val_accuracy: 0.9434\n",
      "Epoch 204/1000\n",
      "33/33 [==============================] - 3s 76ms/step - loss: 0.0426 - accuracy: 0.9839 - val_loss: 0.2974 - val_accuracy: 0.9292\n",
      "Epoch 205/1000\n",
      "33/33 [==============================] - 3s 80ms/step - loss: 0.0689 - accuracy: 0.9763 - val_loss: 0.3035 - val_accuracy: 0.9575\n",
      "Epoch 206/1000\n",
      "33/33 [==============================] - 2s 68ms/step - loss: 0.0375 - accuracy: 0.9848 - val_loss: 0.2578 - val_accuracy: 0.9434\n",
      "Epoch 207/1000\n",
      "33/33 [==============================] - 2s 71ms/step - loss: 0.0387 - accuracy: 0.9858 - val_loss: 0.2185 - val_accuracy: 0.9623\n",
      "Epoch 208/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.0304 - accuracy: 0.9905 - val_loss: 0.2294 - val_accuracy: 0.9528\n",
      "Epoch 209/1000\n",
      "33/33 [==============================] - 3s 79ms/step - loss: 0.0484 - accuracy: 0.9830 - val_loss: 0.3394 - val_accuracy: 0.9292\n",
      "Epoch 210/1000\n",
      "33/33 [==============================] - 3s 79ms/step - loss: 0.1113 - accuracy: 0.9697 - val_loss: 0.4068 - val_accuracy: 0.9387\n",
      "Epoch 211/1000\n",
      "33/33 [==============================] - 3s 91ms/step - loss: 0.0711 - accuracy: 0.9735 - val_loss: 0.2405 - val_accuracy: 0.9434\n",
      "Epoch 212/1000\n",
      "33/33 [==============================] - 3s 79ms/step - loss: 0.0667 - accuracy: 0.9763 - val_loss: 0.3005 - val_accuracy: 0.9387\n",
      "Epoch 213/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.1509 - accuracy: 0.9517 - val_loss: 0.2940 - val_accuracy: 0.9387\n",
      "Epoch 214/1000\n",
      "33/33 [==============================] - 3s 80ms/step - loss: 0.0669 - accuracy: 0.9754 - val_loss: 0.2799 - val_accuracy: 0.9340\n",
      "Epoch 215/1000\n",
      "33/33 [==============================] - 3s 88ms/step - loss: 0.0481 - accuracy: 0.9811 - val_loss: 0.2918 - val_accuracy: 0.9528\n",
      "Epoch 216/1000\n",
      "33/33 [==============================] - 3s 81ms/step - loss: 0.0259 - accuracy: 0.9886 - val_loss: 0.2999 - val_accuracy: 0.9528\n",
      "Epoch 217/1000\n",
      "33/33 [==============================] - 3s 79ms/step - loss: 0.0470 - accuracy: 0.9811 - val_loss: 0.2578 - val_accuracy: 0.9481\n",
      "Epoch 218/1000\n",
      "33/33 [==============================] - 2s 75ms/step - loss: 0.0813 - accuracy: 0.9801 - val_loss: 0.2027 - val_accuracy: 0.9387\n",
      "Epoch 219/1000\n",
      "33/33 [==============================] - 3s 97ms/step - loss: 0.0778 - accuracy: 0.9697 - val_loss: 0.2862 - val_accuracy: 0.9198\n",
      "Epoch 220/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0643 - accuracy: 0.9735 - val_loss: 0.2441 - val_accuracy: 0.9481\n",
      "Epoch 221/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0544 - accuracy: 0.9820 - val_loss: 0.2341 - val_accuracy: 0.9481\n",
      "Epoch 222/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0330 - accuracy: 0.9858 - val_loss: 0.2484 - val_accuracy: 0.9481\n",
      "Epoch 223/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0321 - accuracy: 0.9877 - val_loss: 0.2401 - val_accuracy: 0.9623\n",
      "Epoch 224/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0322 - accuracy: 0.9867 - val_loss: 0.2263 - val_accuracy: 0.9623\n",
      "Epoch 225/1000\n",
      "33/33 [==============================] - 2s 64ms/step - loss: 0.0272 - accuracy: 0.9915 - val_loss: 0.1704 - val_accuracy: 0.9575\n",
      "Epoch 226/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0213 - accuracy: 0.9905 - val_loss: 0.2059 - val_accuracy: 0.9623\n",
      "Epoch 227/1000\n",
      "33/33 [==============================] - 2s 62ms/step - loss: 0.0238 - accuracy: 0.9867 - val_loss: 0.2003 - val_accuracy: 0.9670\n",
      "Epoch 228/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0183 - accuracy: 0.9924 - val_loss: 0.2284 - val_accuracy: 0.9623\n",
      "Epoch 229/1000\n",
      "33/33 [==============================] - 2s 63ms/step - loss: 0.0189 - accuracy: 0.9953 - val_loss: 0.2170 - val_accuracy: 0.9623\n",
      "Epoch 230/1000\n",
      "33/33 [==============================] - 2s 61ms/step - loss: 0.0123 - accuracy: 0.9953 - val_loss: 0.2280 - val_accuracy: 0.9670\n",
      "Epoch 231/1000\n",
      "33/33 [==============================] - 2s 76ms/step - loss: 0.0134 - accuracy: 0.9953 - val_loss: 0.2465 - val_accuracy: 0.9623\n",
      "Epoch 232/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0163 - accuracy: 0.9943 - val_loss: 0.2492 - val_accuracy: 0.9670\n",
      "Epoch 233/1000\n",
      "33/33 [==============================] - 2s 66ms/step - loss: 0.0323 - accuracy: 0.9896 - val_loss: 0.1623 - val_accuracy: 0.9623\n",
      "Epoch 234/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0181 - accuracy: 0.9934 - val_loss: 0.2327 - val_accuracy: 0.9623\n",
      "Epoch 235/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0195 - accuracy: 0.9915 - val_loss: 0.2624 - val_accuracy: 0.9623\n",
      "Epoch 236/1000\n",
      "33/33 [==============================] - 2s 60ms/step - loss: 0.0399 - accuracy: 0.9905 - val_loss: 0.1690 - val_accuracy: 0.9670\n",
      "Epoch 237/1000\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0184 - accuracy: 0.9915 - val_loss: 0.2093 - val_accuracy: 0.9670\n",
      "Epoch 238/1000\n",
      "33/33 [==============================] - ETA: 0s - loss: 0.0162 - accuracy: 0.9943\n",
      "Reached 97% accuracy so cancelling training!\n",
      "33/33 [==============================] - 2s 59ms/step - loss: 0.0162 - accuracy: 0.9943 - val_loss: 0.2145 - val_accuracy: 0.9764\n"
     ]
    }
   ],
   "source": [
    "#Train the Model\n",
    "history = model.fit(x_train, y_train, epochs=1000, batch_size=32, validation_data=(x_test, y_test), callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "adapted-essex",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training and validation loss')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "acc      = history.history[     'accuracy' ]\n",
    "val_acc  = history.history[ 'val_accuracy' ]\n",
    "loss     = history.history[    'loss' ]\n",
    "val_loss = history.history['val_loss' ]\n",
    "\n",
    "epochs   = range(len(acc)) # Get number of epochs\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation accuracy per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot  ( epochs,     acc )\n",
    "plt.plot  ( epochs, val_acc )\n",
    "plt.title ('Training and validation accuracy')\n",
    "plt.figure()\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot  ( epochs,     loss )\n",
    "plt.plot  ( epochs, val_loss )\n",
    "plt.title ('Training and validation loss'   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "coated-lesson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved into model_SIBI.h5\n"
     ]
    }
   ],
   "source": [
    "#Saving the model into H5 system file\n",
    "save_model = \"model_SIBI.h5\"\n",
    "model.save(save_model)\n",
    "print(\"Model Saved into\", save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "variable-violin",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Already saved a model, cleaning up\n",
      "\n",
      "INFO:tensorflow:Assets written to: server_model_SIBI\\1\\assets\n",
      "\n",
      "export_path = server_model_SIBI\\1\n",
      "total 340\n",
      "drwxr-xr-x 1 afkaa afkaa      0 Dec 21 14:29 assets\n",
      "-rw-r--r-- 1 afkaa afkaa 342183 Dec 21 14:29 saved_model.pb\n",
      "drwxr-xr-x 1 afkaa afkaa      0 Dec 21 14:29 variables\n"
     ]
    }
   ],
   "source": [
    "#Saving the model for TF-Serving Type\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"server_model_SIBI\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    print('\\nAlready saved a model, cleaning up\\n')\n",
    "    !rm -r {export_path}\n",
    "\n",
    "model.save(export_path, save_format=\"tf\")\n",
    "\n",
    "print('\\nexport_path = {}'.format(export_path))\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "acoustic-salon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 63, 1)\n",
      "[[[ 2.991]\n",
      "  [ 6.479]\n",
      "  [ 0.   ]\n",
      "  [ 2.221]\n",
      "  [ 5.938]\n",
      "  [-0.   ]\n",
      "  [ 1.711]\n",
      "  [ 4.989]\n",
      "  [-0.   ]\n",
      "  [ 1.977]\n",
      "  [ 4.178]\n",
      "  [-0.001]\n",
      "  [ 2.819]\n",
      "  [ 3.851]\n",
      "  [-0.001]\n",
      "  [ 2.324]\n",
      "  [ 3.676]\n",
      "  [-0.   ]\n",
      "  [ 2.196]\n",
      "  [ 2.853]\n",
      "  [-0.   ]\n",
      "  [ 2.278]\n",
      "  [ 3.302]\n",
      "  [-0.001]\n",
      "  [ 2.429]\n",
      "  [ 3.784]\n",
      "  [-0.001]\n",
      "  [ 2.97 ]\n",
      "  [ 3.499]\n",
      "  [-0.   ]\n",
      "  [ 3.102]\n",
      "  [ 2.235]\n",
      "  [-0.   ]\n",
      "  [ 3.286]\n",
      "  [ 1.507]\n",
      "  [-0.   ]\n",
      "  [ 3.471]\n",
      "  [ 0.808]\n",
      "  [-0.001]\n",
      "  [ 3.516]\n",
      "  [ 3.659]\n",
      "  [-0.   ]\n",
      "  [ 3.673]\n",
      "  [ 2.502]\n",
      "  [-0.   ]\n",
      "  [ 3.765]\n",
      "  [ 1.799]\n",
      "  [-0.   ]\n",
      "  [ 3.843]\n",
      "  [ 1.136]\n",
      "  [-0.001]\n",
      "  [ 4.08 ]\n",
      "  [ 4.077]\n",
      "  [-0.   ]\n",
      "  [ 4.168]\n",
      "  [ 3.108]\n",
      "  [-0.   ]\n",
      "  [ 4.199]\n",
      "  [ 2.529]\n",
      "  [-0.   ]\n",
      "  [ 4.226]\n",
      "  [ 1.994]\n",
      "  [-0.   ]]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "input_test = [[[762.6636624336243], [1652.198314666748], [1.0169689090844258e-07],\n",
    "               [566.4011240005493], [1514.1772031784058], [-0.06466735899448395],\n",
    "               [436.24624609947205], [1272.1819877624512], [-0.10941988229751587],\n",
    "               [504.03743982315063], [1065.2660131454468], [-0.1551685929298401],\n",
    "               [718.9680337905884], [981.9985628128052], [-0.19297298789024353],\n",
    "               [592.5631523132324], [937.2789859771729], [-0.024738160893321037],\n",
    "               [559.9795579910278], [727.5906205177307], [-0.1174304261803627],\n",
    "               [581.0156464576721], [841.9082164764404], [-0.19315676391124725],\n",
    "               [619.5127964019775], [965.0120735168457], [-0.23132655024528503],\n",
    "               [757.310152053833], [892.2686576843262], [-0.030217956751585007],\n",
    "               [790.9350991249084], [569.8509216308594], [-0.07613588869571686],\n",
    "               [838.0098938941956], [384.23940539360046], [-0.11438910663127899],\n",
    "               [885.1863145828247], [206.1316967010498], [-0.14229716360569], \n",
    "               [896.6857194900513], [933.0331087112427], [-0.053754180669784546],\n",
    "               [936.7329478263855], [638.064980506897], [-0.09413756430149078], \n",
    "               [960.0444436073303], [458.6908519268036], [-0.11900665611028671],\n",
    "               [980.0054430961609], [289.8055911064148], [-0.1342657208442688], \n",
    "               [1040.4841899871826], [1039.6770238876343], [-0.08633966743946075],\n",
    "               [1062.9222393035889], [792.4231886863708], [-0.10331644117832184],\n",
    "               [1070.7426071166992], [644.8165774345398], [-0.10464464873075485],\n",
    "               [1077.575922012329], [508.50069522857666], [-0.10636621713638306]]]\n",
    "input_test = np.array(input_test)\n",
    "input_test = input_test/255.0\n",
    "input = np.reshape(input_test, (input_test.shape[0], input_test.shape[1], 1))\n",
    "print(input_test.shape)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "front-survey",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "#Print the Prediction\n",
    "print(model.predict(input_test))\n",
    "print(model.predict_classes(input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "confirmed-floating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n"
     ]
    }
   ],
   "source": [
    "classes = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6,\n",
    "    'H': 7,\n",
    "    'I': 8,\n",
    "    'J': 9,\n",
    "    'K': 10,\n",
    "    'L': 11,\n",
    "    'M': 12,\n",
    "    'N': 13,\n",
    "    'O': 14,\n",
    "    'P': 15,\n",
    "    'Q': 16,\n",
    "    'R': 17,\n",
    "    'S': 18,\n",
    "    'T': 19,\n",
    "    'U': 20,\n",
    "    'V': 21,\n",
    "    'W': 22,\n",
    "    'X': 23,\n",
    "    'Y': 24,\n",
    "    'Z': 25\n",
    "}\n",
    "\n",
    "predictions = model.predict_classes(input_test)\n",
    "for alphabets, values in classes.items():\n",
    "    if values == predictions[0] :\n",
    "        print(alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "announced-leeds",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
