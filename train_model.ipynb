{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "unauthorized-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the much needed stuff for training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "sorted-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_IpX</th>\n",
       "      <th>thumb_IpY</th>\n",
       "      <th>thumb_TipX</th>\n",
       "      <th>...</th>\n",
       "      <th>ring_TipX</th>\n",
       "      <th>ring_TipY</th>\n",
       "      <th>pinky_McpX</th>\n",
       "      <th>pinky_McpY</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>614.663780</td>\n",
       "      <td>905.806620</td>\n",
       "      <td>549.961701</td>\n",
       "      <td>910.559009</td>\n",
       "      <td>484.506458</td>\n",
       "      <td>895.301416</td>\n",
       "      <td>446.415693</td>\n",
       "      <td>872.791400</td>\n",
       "      <td>435.599104</td>\n",
       "      <td>...</td>\n",
       "      <td>597.933635</td>\n",
       "      <td>906.572645</td>\n",
       "      <td>621.966362</td>\n",
       "      <td>825.802557</td>\n",
       "      <td>617.009506</td>\n",
       "      <td>835.676272</td>\n",
       "      <td>628.267184</td>\n",
       "      <td>863.719888</td>\n",
       "      <td>636.857986</td>\n",
       "      <td>880.355752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>1586.340550</td>\n",
       "      <td>1973.989626</td>\n",
       "      <td>1321.859984</td>\n",
       "      <td>1894.983892</td>\n",
       "      <td>1098.946306</td>\n",
       "      <td>1643.605182</td>\n",
       "      <td>1026.106686</td>\n",
       "      <td>1423.268890</td>\n",
       "      <td>1142.311011</td>\n",
       "      <td>...</td>\n",
       "      <td>1599.906293</td>\n",
       "      <td>1636.526762</td>\n",
       "      <td>1844.856026</td>\n",
       "      <td>1433.191668</td>\n",
       "      <td>1677.424247</td>\n",
       "      <td>1448.975126</td>\n",
       "      <td>1652.730848</td>\n",
       "      <td>1627.551797</td>\n",
       "      <td>1734.164803</td>\n",
       "      <td>1648.017813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1586.658151</td>\n",
       "      <td>1899.797421</td>\n",
       "      <td>1347.672125</td>\n",
       "      <td>1834.859933</td>\n",
       "      <td>1150.592813</td>\n",
       "      <td>1586.523955</td>\n",
       "      <td>1096.266506</td>\n",
       "      <td>1351.857016</td>\n",
       "      <td>1162.683060</td>\n",
       "      <td>...</td>\n",
       "      <td>1619.031085</td>\n",
       "      <td>1549.667838</td>\n",
       "      <td>1807.934423</td>\n",
       "      <td>1360.078092</td>\n",
       "      <td>1668.181927</td>\n",
       "      <td>1352.136308</td>\n",
       "      <td>1661.550659</td>\n",
       "      <td>1533.032684</td>\n",
       "      <td>1746.261959</td>\n",
       "      <td>1546.756997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>1609.015491</td>\n",
       "      <td>1907.042126</td>\n",
       "      <td>1332.827688</td>\n",
       "      <td>1813.612083</td>\n",
       "      <td>1129.451521</td>\n",
       "      <td>1560.214301</td>\n",
       "      <td>1071.799619</td>\n",
       "      <td>1348.270438</td>\n",
       "      <td>1158.308507</td>\n",
       "      <td>...</td>\n",
       "      <td>1592.248868</td>\n",
       "      <td>1547.547122</td>\n",
       "      <td>1834.185079</td>\n",
       "      <td>1350.679505</td>\n",
       "      <td>1657.451330</td>\n",
       "      <td>1350.081768</td>\n",
       "      <td>1638.767816</td>\n",
       "      <td>1529.082674</td>\n",
       "      <td>1727.664467</td>\n",
       "      <td>1547.885221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>1650.991414</td>\n",
       "      <td>1806.711465</td>\n",
       "      <td>1384.922959</td>\n",
       "      <td>1740.992451</td>\n",
       "      <td>1179.722488</td>\n",
       "      <td>1506.995106</td>\n",
       "      <td>1119.851872</td>\n",
       "      <td>1279.860915</td>\n",
       "      <td>1177.426620</td>\n",
       "      <td>...</td>\n",
       "      <td>1628.596647</td>\n",
       "      <td>1483.385787</td>\n",
       "      <td>1807.114396</td>\n",
       "      <td>1288.052511</td>\n",
       "      <td>1687.552578</td>\n",
       "      <td>1268.021172</td>\n",
       "      <td>1683.299860</td>\n",
       "      <td>1451.865086</td>\n",
       "      <td>1756.480227</td>\n",
       "      <td>1475.592785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1028</th>\n",
       "      <td>Z</td>\n",
       "      <td>632.342100</td>\n",
       "      <td>1417.761445</td>\n",
       "      <td>511.266172</td>\n",
       "      <td>1452.934027</td>\n",
       "      <td>654.702663</td>\n",
       "      <td>1413.266897</td>\n",
       "      <td>992.956877</td>\n",
       "      <td>1352.535844</td>\n",
       "      <td>1304.301739</td>\n",
       "      <td>...</td>\n",
       "      <td>965.736210</td>\n",
       "      <td>1345.040798</td>\n",
       "      <td>1115.571737</td>\n",
       "      <td>1094.783783</td>\n",
       "      <td>1257.244349</td>\n",
       "      <td>1213.543057</td>\n",
       "      <td>1127.177358</td>\n",
       "      <td>1351.791143</td>\n",
       "      <td>998.408437</td>\n",
       "      <td>1384.162068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1029</th>\n",
       "      <td>Z</td>\n",
       "      <td>707.776546</td>\n",
       "      <td>1417.585611</td>\n",
       "      <td>533.494473</td>\n",
       "      <td>1449.532986</td>\n",
       "      <td>626.750469</td>\n",
       "      <td>1442.848206</td>\n",
       "      <td>935.930610</td>\n",
       "      <td>1423.299432</td>\n",
       "      <td>1238.717318</td>\n",
       "      <td>...</td>\n",
       "      <td>934.907556</td>\n",
       "      <td>1375.932932</td>\n",
       "      <td>1138.646483</td>\n",
       "      <td>1136.404514</td>\n",
       "      <td>1227.160931</td>\n",
       "      <td>1300.656319</td>\n",
       "      <td>1077.356815</td>\n",
       "      <td>1399.181604</td>\n",
       "      <td>964.920759</td>\n",
       "      <td>1406.220436</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1030</th>\n",
       "      <td>Z</td>\n",
       "      <td>484.730750</td>\n",
       "      <td>1203.294873</td>\n",
       "      <td>609.190941</td>\n",
       "      <td>1234.577775</td>\n",
       "      <td>845.985651</td>\n",
       "      <td>1188.504934</td>\n",
       "      <td>1064.718962</td>\n",
       "      <td>1230.792642</td>\n",
       "      <td>1245.759130</td>\n",
       "      <td>...</td>\n",
       "      <td>1007.254124</td>\n",
       "      <td>1246.820688</td>\n",
       "      <td>937.975585</td>\n",
       "      <td>1005.628824</td>\n",
       "      <td>1131.682754</td>\n",
       "      <td>1035.978436</td>\n",
       "      <td>1161.693215</td>\n",
       "      <td>1084.606409</td>\n",
       "      <td>1170.148134</td>\n",
       "      <td>1053.281426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1031</th>\n",
       "      <td>Z</td>\n",
       "      <td>445.235610</td>\n",
       "      <td>1298.323512</td>\n",
       "      <td>468.829751</td>\n",
       "      <td>1329.984903</td>\n",
       "      <td>677.648187</td>\n",
       "      <td>1299.878120</td>\n",
       "      <td>947.729409</td>\n",
       "      <td>1341.548443</td>\n",
       "      <td>1169.836998</td>\n",
       "      <td>...</td>\n",
       "      <td>903.057277</td>\n",
       "      <td>1266.618848</td>\n",
       "      <td>957.399011</td>\n",
       "      <td>1050.945640</td>\n",
       "      <td>1096.230984</td>\n",
       "      <td>1104.877949</td>\n",
       "      <td>1034.316659</td>\n",
       "      <td>1175.140142</td>\n",
       "      <td>995.093167</td>\n",
       "      <td>1144.306540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1032</th>\n",
       "      <td>Z</td>\n",
       "      <td>550.345659</td>\n",
       "      <td>1311.316252</td>\n",
       "      <td>553.188920</td>\n",
       "      <td>1309.291124</td>\n",
       "      <td>761.222124</td>\n",
       "      <td>1277.625799</td>\n",
       "      <td>1033.439517</td>\n",
       "      <td>1299.137354</td>\n",
       "      <td>1267.425895</td>\n",
       "      <td>...</td>\n",
       "      <td>960.366845</td>\n",
       "      <td>1254.362702</td>\n",
       "      <td>1017.805338</td>\n",
       "      <td>1062.273979</td>\n",
       "      <td>1146.723390</td>\n",
       "      <td>1102.788806</td>\n",
       "      <td>1068.720341</td>\n",
       "      <td>1209.428310</td>\n",
       "      <td>998.680770</td>\n",
       "      <td>1223.415375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1033 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_type       wristX       wristY   thumb_CmcX   thumb_CmcY  \\\n",
       "0             A   614.663780   905.806620   549.961701   910.559009   \n",
       "1             A  1586.340550  1973.989626  1321.859984  1894.983892   \n",
       "2             A  1586.658151  1899.797421  1347.672125  1834.859933   \n",
       "3             A  1609.015491  1907.042126  1332.827688  1813.612083   \n",
       "4             A  1650.991414  1806.711465  1384.922959  1740.992451   \n",
       "...         ...          ...          ...          ...          ...   \n",
       "1028          Z   632.342100  1417.761445   511.266172  1452.934027   \n",
       "1029          Z   707.776546  1417.585611   533.494473  1449.532986   \n",
       "1030          Z   484.730750  1203.294873   609.190941  1234.577775   \n",
       "1031          Z   445.235610  1298.323512   468.829751  1329.984903   \n",
       "1032          Z   550.345659  1311.316252   553.188920  1309.291124   \n",
       "\n",
       "       thumb_McpX   thumb_McpY    thumb_IpX    thumb_IpY   thumb_TipX  ...  \\\n",
       "0      484.506458   895.301416   446.415693   872.791400   435.599104  ...   \n",
       "1     1098.946306  1643.605182  1026.106686  1423.268890  1142.311011  ...   \n",
       "2     1150.592813  1586.523955  1096.266506  1351.857016  1162.683060  ...   \n",
       "3     1129.451521  1560.214301  1071.799619  1348.270438  1158.308507  ...   \n",
       "4     1179.722488  1506.995106  1119.851872  1279.860915  1177.426620  ...   \n",
       "...           ...          ...          ...          ...          ...  ...   \n",
       "1028   654.702663  1413.266897   992.956877  1352.535844  1304.301739  ...   \n",
       "1029   626.750469  1442.848206   935.930610  1423.299432  1238.717318  ...   \n",
       "1030   845.985651  1188.504934  1064.718962  1230.792642  1245.759130  ...   \n",
       "1031   677.648187  1299.878120   947.729409  1341.548443  1169.836998  ...   \n",
       "1032   761.222124  1277.625799  1033.439517  1299.137354  1267.425895  ...   \n",
       "\n",
       "        ring_TipX    ring_TipY   pinky_McpX   pinky_McpY   pinky_PipX  \\\n",
       "0      597.933635   906.572645   621.966362   825.802557   617.009506   \n",
       "1     1599.906293  1636.526762  1844.856026  1433.191668  1677.424247   \n",
       "2     1619.031085  1549.667838  1807.934423  1360.078092  1668.181927   \n",
       "3     1592.248868  1547.547122  1834.185079  1350.679505  1657.451330   \n",
       "4     1628.596647  1483.385787  1807.114396  1288.052511  1687.552578   \n",
       "...           ...          ...          ...          ...          ...   \n",
       "1028   965.736210  1345.040798  1115.571737  1094.783783  1257.244349   \n",
       "1029   934.907556  1375.932932  1138.646483  1136.404514  1227.160931   \n",
       "1030  1007.254124  1246.820688   937.975585  1005.628824  1131.682754   \n",
       "1031   903.057277  1266.618848   957.399011  1050.945640  1096.230984   \n",
       "1032   960.366845  1254.362702  1017.805338  1062.273979  1146.723390   \n",
       "\n",
       "       pinky_PipY   pinky_DipX   pinky_DipY   pinky_TipX   pinky_TipY  \n",
       "0      835.676272   628.267184   863.719888   636.857986   880.355752  \n",
       "1     1448.975126  1652.730848  1627.551797  1734.164803  1648.017813  \n",
       "2     1352.136308  1661.550659  1533.032684  1746.261959  1546.756997  \n",
       "3     1350.081768  1638.767816  1529.082674  1727.664467  1547.885221  \n",
       "4     1268.021172  1683.299860  1451.865086  1756.480227  1475.592785  \n",
       "...           ...          ...          ...          ...          ...  \n",
       "1028  1213.543057  1127.177358  1351.791143   998.408437  1384.162068  \n",
       "1029  1300.656319  1077.356815  1399.181604   964.920759  1406.220436  \n",
       "1030  1035.978436  1161.693215  1084.606409  1170.148134  1053.281426  \n",
       "1031  1104.877949  1034.316659  1175.140142   995.093167  1144.306540  \n",
       "1032  1102.788806  1068.720341  1209.428310   998.680770  1223.415375  \n",
       "\n",
       "[1033 rows x 43 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Training the model using Pandas\n",
    "df_train = pd.read_csv(\"hands_SIBI_training.csv\", header=0)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ruled-cologne",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_IpX</th>\n",
       "      <th>thumb_IpY</th>\n",
       "      <th>thumb_TipX</th>\n",
       "      <th>...</th>\n",
       "      <th>ring_TipX</th>\n",
       "      <th>ring_TipY</th>\n",
       "      <th>pinky_McpX</th>\n",
       "      <th>pinky_McpY</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1595.475198</td>\n",
       "      <td>1713.882845</td>\n",
       "      <td>1371.011287</td>\n",
       "      <td>1654.651115</td>\n",
       "      <td>1140.459070</td>\n",
       "      <td>1461.532063</td>\n",
       "      <td>1041.989141</td>\n",
       "      <td>1233.009396</td>\n",
       "      <td>1109.759125</td>\n",
       "      <td>...</td>\n",
       "      <td>1564.787345</td>\n",
       "      <td>1370.234713</td>\n",
       "      <td>1741.941414</td>\n",
       "      <td>1206.933278</td>\n",
       "      <td>1664.283545</td>\n",
       "      <td>1119.481146</td>\n",
       "      <td>1645.726819</td>\n",
       "      <td>1310.211016</td>\n",
       "      <td>1689.494687</td>\n",
       "      <td>1349.412327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>719.049096</td>\n",
       "      <td>1577.525616</td>\n",
       "      <td>508.609653</td>\n",
       "      <td>1392.832041</td>\n",
       "      <td>435.399145</td>\n",
       "      <td>1078.577757</td>\n",
       "      <td>453.572810</td>\n",
       "      <td>842.452109</td>\n",
       "      <td>486.912608</td>\n",
       "      <td>...</td>\n",
       "      <td>843.840003</td>\n",
       "      <td>1323.482871</td>\n",
       "      <td>1090.264320</td>\n",
       "      <td>1240.741253</td>\n",
       "      <td>1128.179789</td>\n",
       "      <td>1106.518865</td>\n",
       "      <td>1024.695754</td>\n",
       "      <td>1242.374778</td>\n",
       "      <td>977.687418</td>\n",
       "      <td>1327.082992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>757.780492</td>\n",
       "      <td>1601.105571</td>\n",
       "      <td>552.570701</td>\n",
       "      <td>1429.801345</td>\n",
       "      <td>481.703639</td>\n",
       "      <td>1114.173293</td>\n",
       "      <td>499.344498</td>\n",
       "      <td>878.003299</td>\n",
       "      <td>518.134713</td>\n",
       "      <td>...</td>\n",
       "      <td>877.598047</td>\n",
       "      <td>1347.837567</td>\n",
       "      <td>1113.693476</td>\n",
       "      <td>1258.446693</td>\n",
       "      <td>1158.711910</td>\n",
       "      <td>1131.860495</td>\n",
       "      <td>1060.676217</td>\n",
       "      <td>1270.025492</td>\n",
       "      <td>1010.870814</td>\n",
       "      <td>1352.582455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>615.760669</td>\n",
       "      <td>1031.698437</td>\n",
       "      <td>546.501115</td>\n",
       "      <td>1009.297512</td>\n",
       "      <td>496.653780</td>\n",
       "      <td>939.493802</td>\n",
       "      <td>477.223635</td>\n",
       "      <td>880.857238</td>\n",
       "      <td>457.223967</td>\n",
       "      <td>...</td>\n",
       "      <td>606.932983</td>\n",
       "      <td>927.758379</td>\n",
       "      <td>666.484147</td>\n",
       "      <td>879.113685</td>\n",
       "      <td>633.975327</td>\n",
       "      <td>843.605268</td>\n",
       "      <td>628.140047</td>\n",
       "      <td>895.670514</td>\n",
       "      <td>643.053487</td>\n",
       "      <td>913.554010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>322.910583</td>\n",
       "      <td>3155.554688</td>\n",
       "      <td>387.274112</td>\n",
       "      <td>3209.536560</td>\n",
       "      <td>392.819341</td>\n",
       "      <td>3277.465576</td>\n",
       "      <td>393.364716</td>\n",
       "      <td>3335.383026</td>\n",
       "      <td>401.043561</td>\n",
       "      <td>...</td>\n",
       "      <td>311.369056</td>\n",
       "      <td>3373.221863</td>\n",
       "      <td>272.978125</td>\n",
       "      <td>3264.753571</td>\n",
       "      <td>278.960250</td>\n",
       "      <td>3301.982941</td>\n",
       "      <td>294.675690</td>\n",
       "      <td>3319.776947</td>\n",
       "      <td>307.635242</td>\n",
       "      <td>3325.865021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>Z</td>\n",
       "      <td>758.030560</td>\n",
       "      <td>2493.725372</td>\n",
       "      <td>555.423833</td>\n",
       "      <td>2410.149605</td>\n",
       "      <td>554.325989</td>\n",
       "      <td>2121.368992</td>\n",
       "      <td>785.251643</td>\n",
       "      <td>1939.650558</td>\n",
       "      <td>1009.190364</td>\n",
       "      <td>...</td>\n",
       "      <td>836.804911</td>\n",
       "      <td>1998.270870</td>\n",
       "      <td>878.059498</td>\n",
       "      <td>1776.177761</td>\n",
       "      <td>1103.563620</td>\n",
       "      <td>1768.856277</td>\n",
       "      <td>1011.302115</td>\n",
       "      <td>1940.822914</td>\n",
       "      <td>882.944364</td>\n",
       "      <td>1985.800049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>Z</td>\n",
       "      <td>2576.749849</td>\n",
       "      <td>2868.969537</td>\n",
       "      <td>2432.240124</td>\n",
       "      <td>2800.074300</td>\n",
       "      <td>2270.718741</td>\n",
       "      <td>2749.498940</td>\n",
       "      <td>2145.062299</td>\n",
       "      <td>2784.001738</td>\n",
       "      <td>2058.201814</td>\n",
       "      <td>...</td>\n",
       "      <td>2296.587610</td>\n",
       "      <td>2895.431664</td>\n",
       "      <td>2441.918750</td>\n",
       "      <td>2826.890079</td>\n",
       "      <td>2278.390603</td>\n",
       "      <td>2947.817417</td>\n",
       "      <td>2309.023933</td>\n",
       "      <td>2981.885049</td>\n",
       "      <td>2355.511651</td>\n",
       "      <td>2958.717896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>Z</td>\n",
       "      <td>1902.547185</td>\n",
       "      <td>2725.740898</td>\n",
       "      <td>1779.595316</td>\n",
       "      <td>2662.946720</td>\n",
       "      <td>1690.045661</td>\n",
       "      <td>2521.979805</td>\n",
       "      <td>1711.686770</td>\n",
       "      <td>2370.669079</td>\n",
       "      <td>1794.469457</td>\n",
       "      <td>...</td>\n",
       "      <td>1875.205433</td>\n",
       "      <td>2507.626522</td>\n",
       "      <td>2008.996679</td>\n",
       "      <td>2444.163872</td>\n",
       "      <td>1994.378469</td>\n",
       "      <td>2405.037231</td>\n",
       "      <td>1955.960378</td>\n",
       "      <td>2498.758759</td>\n",
       "      <td>1947.145678</td>\n",
       "      <td>2534.302185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>Z</td>\n",
       "      <td>646.825790</td>\n",
       "      <td>1212.169051</td>\n",
       "      <td>674.274445</td>\n",
       "      <td>1242.027044</td>\n",
       "      <td>843.870282</td>\n",
       "      <td>1261.058211</td>\n",
       "      <td>1057.972789</td>\n",
       "      <td>1293.666124</td>\n",
       "      <td>1220.292091</td>\n",
       "      <td>...</td>\n",
       "      <td>1005.024076</td>\n",
       "      <td>1216.723204</td>\n",
       "      <td>1082.362056</td>\n",
       "      <td>1026.123166</td>\n",
       "      <td>1191.763639</td>\n",
       "      <td>1050.326347</td>\n",
       "      <td>1154.398918</td>\n",
       "      <td>1090.296507</td>\n",
       "      <td>1120.047092</td>\n",
       "      <td>1066.013694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203</th>\n",
       "      <td>Z</td>\n",
       "      <td>742.793560</td>\n",
       "      <td>1290.448785</td>\n",
       "      <td>756.768525</td>\n",
       "      <td>1313.876748</td>\n",
       "      <td>911.147416</td>\n",
       "      <td>1330.813885</td>\n",
       "      <td>1126.545787</td>\n",
       "      <td>1361.523628</td>\n",
       "      <td>1301.351786</td>\n",
       "      <td>...</td>\n",
       "      <td>1108.314395</td>\n",
       "      <td>1289.618134</td>\n",
       "      <td>1201.952457</td>\n",
       "      <td>1120.719910</td>\n",
       "      <td>1311.442971</td>\n",
       "      <td>1128.456712</td>\n",
       "      <td>1273.124337</td>\n",
       "      <td>1169.704914</td>\n",
       "      <td>1245.737553</td>\n",
       "      <td>1152.362823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>204 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_type       wristX       wristY   thumb_CmcX   thumb_CmcY  \\\n",
       "0            A  1595.475198  1713.882845  1371.011287  1654.651115   \n",
       "1            A   719.049096  1577.525616   508.609653  1392.832041   \n",
       "2            A   757.780492  1601.105571   552.570701  1429.801345   \n",
       "3            A   615.760669  1031.698437   546.501115  1009.297512   \n",
       "4            A   322.910583  3155.554688   387.274112  3209.536560   \n",
       "..         ...          ...          ...          ...          ...   \n",
       "199          Z   758.030560  2493.725372   555.423833  2410.149605   \n",
       "200          Z  2576.749849  2868.969537  2432.240124  2800.074300   \n",
       "201          Z  1902.547185  2725.740898  1779.595316  2662.946720   \n",
       "202          Z   646.825790  1212.169051   674.274445  1242.027044   \n",
       "203          Z   742.793560  1290.448785   756.768525  1313.876748   \n",
       "\n",
       "      thumb_McpX   thumb_McpY    thumb_IpX    thumb_IpY   thumb_TipX  ...  \\\n",
       "0    1140.459070  1461.532063  1041.989141  1233.009396  1109.759125  ...   \n",
       "1     435.399145  1078.577757   453.572810   842.452109   486.912608  ...   \n",
       "2     481.703639  1114.173293   499.344498   878.003299   518.134713  ...   \n",
       "3     496.653780   939.493802   477.223635   880.857238   457.223967  ...   \n",
       "4     392.819341  3277.465576   393.364716  3335.383026   401.043561  ...   \n",
       "..           ...          ...          ...          ...          ...  ...   \n",
       "199   554.325989  2121.368992   785.251643  1939.650558  1009.190364  ...   \n",
       "200  2270.718741  2749.498940  2145.062299  2784.001738  2058.201814  ...   \n",
       "201  1690.045661  2521.979805  1711.686770  2370.669079  1794.469457  ...   \n",
       "202   843.870282  1261.058211  1057.972789  1293.666124  1220.292091  ...   \n",
       "203   911.147416  1330.813885  1126.545787  1361.523628  1301.351786  ...   \n",
       "\n",
       "       ring_TipX    ring_TipY   pinky_McpX   pinky_McpY   pinky_PipX  \\\n",
       "0    1564.787345  1370.234713  1741.941414  1206.933278  1664.283545   \n",
       "1     843.840003  1323.482871  1090.264320  1240.741253  1128.179789   \n",
       "2     877.598047  1347.837567  1113.693476  1258.446693  1158.711910   \n",
       "3     606.932983   927.758379   666.484147   879.113685   633.975327   \n",
       "4     311.369056  3373.221863   272.978125  3264.753571   278.960250   \n",
       "..           ...          ...          ...          ...          ...   \n",
       "199   836.804911  1998.270870   878.059498  1776.177761  1103.563620   \n",
       "200  2296.587610  2895.431664  2441.918750  2826.890079  2278.390603   \n",
       "201  1875.205433  2507.626522  2008.996679  2444.163872  1994.378469   \n",
       "202  1005.024076  1216.723204  1082.362056  1026.123166  1191.763639   \n",
       "203  1108.314395  1289.618134  1201.952457  1120.719910  1311.442971   \n",
       "\n",
       "      pinky_PipY   pinky_DipX   pinky_DipY   pinky_TipX   pinky_TipY  \n",
       "0    1119.481146  1645.726819  1310.211016  1689.494687  1349.412327  \n",
       "1    1106.518865  1024.695754  1242.374778   977.687418  1327.082992  \n",
       "2    1131.860495  1060.676217  1270.025492  1010.870814  1352.582455  \n",
       "3     843.605268   628.140047   895.670514   643.053487   913.554010  \n",
       "4    3301.982941   294.675690  3319.776947   307.635242  3325.865021  \n",
       "..           ...          ...          ...          ...          ...  \n",
       "199  1768.856277  1011.302115  1940.822914   882.944364  1985.800049  \n",
       "200  2947.817417  2309.023933  2981.885049  2355.511651  2958.717896  \n",
       "201  2405.037231  1955.960378  2498.758759  1947.145678  2534.302185  \n",
       "202  1050.326347  1154.398918  1090.296507  1120.047092  1066.013694  \n",
       "203  1128.456712  1273.124337  1169.704914  1245.737553  1152.362823  \n",
       "\n",
       "[204 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Validation or Testing the Model using Pandas\n",
    "df_test = pd.read_csv(\"hands_SIBI_validation.csv\", header=0)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "complex-operation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Categorical using Pandas\n",
    "df_train[\"class_type\"] = pd.Categorical(df_train[\"class_type\"])\n",
    "df_train[\"class_type\"] = df_train.class_type.cat.codes\n",
    "\n",
    "df_test[\"class_type\"] = pd.Categorical(df_test[\"class_type\"])\n",
    "df_test[\"class_type\"] = df_test.class_type.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "animal-entrance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Label and Feature for training\n",
    "y_train = df_train.pop(\"class_type\")\n",
    "x_train = df_train.copy()\n",
    "\n",
    "y_test = df_test.pop(\"class_type\")\n",
    "x_test = df_test.copy()\n",
    "\n",
    "# Copied Features turn to Array by using NumPy\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "blond-washer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1033, 42)\n",
      "(204, 42)\n",
      "(1033, 42, 1)\n",
      "(204, 42, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check Array Shape before transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Since the array shape is 1x1, we must turn it into 1x10x1 so we can feed it into the model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Check Array Shape after transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "invalid-handling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[614.664]\n",
      " [905.807]\n",
      " [549.962]\n",
      " [910.559]\n",
      " [484.506]\n",
      " [895.301]\n",
      " [446.416]\n",
      " [872.791]\n",
      " [435.599]\n",
      " [842.709]\n",
      " [492.176]\n",
      " [845.125]\n",
      " [455.925]\n",
      " [844.86 ]\n",
      " [478.732]\n",
      " [890.223]\n",
      " [501.187]\n",
      " [922.558]\n",
      " [536.641]\n",
      " [826.821]\n",
      " [510.607]\n",
      " [840.276]\n",
      " [530.386]\n",
      " [889.723]\n",
      " [550.536]\n",
      " [916.596]\n",
      " [581.733]\n",
      " [822.1  ]\n",
      " [561.356]\n",
      " [835.389]\n",
      " [580.989]\n",
      " [882.792]\n",
      " [597.934]\n",
      " [906.573]\n",
      " [621.966]\n",
      " [825.803]\n",
      " [617.01 ]\n",
      " [835.676]\n",
      " [628.267]\n",
      " [863.72 ]\n",
      " [636.858]\n",
      " [880.356]]\n",
      "[[2667.218]\n",
      " [1947.497]\n",
      " [2643.249]\n",
      " [1810.535]\n",
      " [2597.547]\n",
      " [1667.531]\n",
      " [2558.04 ]\n",
      " [1543.697]\n",
      " [2516.062]\n",
      " [1443.765]\n",
      " [2626.635]\n",
      " [1594.4  ]\n",
      " [2726.339]\n",
      " [1509.7  ]\n",
      " [2755.825]\n",
      " [1631.71 ]\n",
      " [2742.401]\n",
      " [1712.601]\n",
      " [2634.353]\n",
      " [1601.582]\n",
      " [2737.23 ]\n",
      " [1524.637]\n",
      " [2755.177]\n",
      " [1665.086]\n",
      " [2742.176]\n",
      " [1736.505]\n",
      " [2640.166]\n",
      " [1617.782]\n",
      " [2727.282]\n",
      " [1520.309]\n",
      " [2741.725]\n",
      " [1637.303]\n",
      " [2723.755]\n",
      " [1704.204]\n",
      " [2640.379]\n",
      " [1641.465]\n",
      " [2662.197]\n",
      " [1533.31 ]\n",
      " [2656.062]\n",
      " [1516.998]\n",
      " [2638.775]\n",
      " [1495.61 ]]\n"
     ]
    }
   ],
   "source": [
    "# Check sample train and test features\n",
    "print(x_train[0])\n",
    "print(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driven-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes according standard Alphabets\n",
    "num_classes = 26\n",
    "\n",
    "# Using the Keras.Utils to put the label categorically \n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "juvenile-cricket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_12 (Conv1D)           (None, 42, 32)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_13 (Conv1D)           (None, 42, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling (None, 21, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_14 (Conv1D)           (None, 21, 64)            10304     \n",
      "_________________________________________________________________\n",
      "conv1d_15 (Conv1D)           (None, 21, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling (None, 10, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_16 (Conv1D)           (None, 10, 128)           41088     \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 10, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling (None, 5, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 5, 256)            164096    \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 5, 256)            327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling (None, 2, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 927,354\n",
      "Trainable params: 927,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One Dimensional Convolutional Neural Network model, Train will be feed to 1 Dimension Convolutional Neural Network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "meaning-frequency",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "30/30 [==============================] - 2s 61ms/step - loss: 3.3895 - accuracy: 0.0333 - val_loss: 3.2281 - val_accuracy: 0.0735\n",
      "Epoch 2/500\n",
      "30/30 [==============================] - 2s 67ms/step - loss: 3.0936 - accuracy: 0.0907 - val_loss: 2.9086 - val_accuracy: 0.0931\n",
      "Epoch 3/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 2.7562 - accuracy: 0.1057 - val_loss: 2.8102 - val_accuracy: 0.1324\n",
      "Epoch 4/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 2.5414 - accuracy: 0.1857 - val_loss: 2.4765 - val_accuracy: 0.1912\n",
      "Epoch 5/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 2.3031 - accuracy: 0.2380 - val_loss: 2.3175 - val_accuracy: 0.2598\n",
      "Epoch 6/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1.8721 - accuracy: 0.3671 - val_loss: 1.9867 - val_accuracy: 0.3480\n",
      "Epoch 7/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1.6423 - accuracy: 0.4514 - val_loss: 1.7368 - val_accuracy: 0.4363\n",
      "Epoch 8/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 1.4417 - accuracy: 0.5219 - val_loss: 1.7386 - val_accuracy: 0.4706\n",
      "Epoch 9/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 1.3164 - accuracy: 0.5646 - val_loss: 1.6846 - val_accuracy: 0.4853\n",
      "Epoch 10/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 1.3419 - accuracy: 0.5560 - val_loss: 1.4794 - val_accuracy: 0.5294\n",
      "Epoch 11/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 1.1471 - accuracy: 0.6062 - val_loss: 1.6156 - val_accuracy: 0.4951\n",
      "Epoch 12/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 1.1152 - accuracy: 0.6146 - val_loss: 1.4115 - val_accuracy: 0.5784\n",
      "Epoch 13/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.9869 - accuracy: 0.6446 - val_loss: 1.3879 - val_accuracy: 0.5735\n",
      "Epoch 14/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.9899 - accuracy: 0.6649 - val_loss: 1.3761 - val_accuracy: 0.5784\n",
      "Epoch 15/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.8898 - accuracy: 0.6926 - val_loss: 1.2217 - val_accuracy: 0.5833\n",
      "Epoch 16/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.7605 - accuracy: 0.7289 - val_loss: 1.2029 - val_accuracy: 0.6225\n",
      "Epoch 17/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.7050 - accuracy: 0.7545 - val_loss: 1.1485 - val_accuracy: 0.6520\n",
      "Epoch 18/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.6767 - accuracy: 0.7556 - val_loss: 1.3423 - val_accuracy: 0.6324\n",
      "Epoch 19/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.8327 - accuracy: 0.7385 - val_loss: 1.4087 - val_accuracy: 0.5490\n",
      "Epoch 20/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.7256 - accuracy: 0.7513 - val_loss: 1.2832 - val_accuracy: 0.7010\n",
      "Epoch 21/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6449 - accuracy: 0.7812 - val_loss: 1.1846 - val_accuracy: 0.6569\n",
      "Epoch 22/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.5946 - accuracy: 0.7791 - val_loss: 1.4864 - val_accuracy: 0.5784\n",
      "Epoch 23/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.6124 - accuracy: 0.7812 - val_loss: 1.2368 - val_accuracy: 0.6961\n",
      "Epoch 24/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.4682 - accuracy: 0.8282 - val_loss: 1.2424 - val_accuracy: 0.7206\n",
      "Epoch 25/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.6318 - accuracy: 0.7940 - val_loss: 1.4588 - val_accuracy: 0.5980\n",
      "Epoch 26/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.5677 - accuracy: 0.7994 - val_loss: 1.3736 - val_accuracy: 0.6667\n",
      "Epoch 27/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.5520 - accuracy: 0.8132 - val_loss: 1.4096 - val_accuracy: 0.6667\n",
      "Epoch 28/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.5137 - accuracy: 0.8303 - val_loss: 1.2419 - val_accuracy: 0.7206\n",
      "Epoch 29/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3931 - accuracy: 0.8655 - val_loss: 1.1742 - val_accuracy: 0.7108\n",
      "Epoch 30/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.3360 - accuracy: 0.8773 - val_loss: 1.0866 - val_accuracy: 0.7500\n",
      "Epoch 31/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2928 - accuracy: 0.8986 - val_loss: 1.6432 - val_accuracy: 0.7010\n",
      "Epoch 32/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.3775 - accuracy: 0.8751 - val_loss: 1.3663 - val_accuracy: 0.6912\n",
      "Epoch 33/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3144 - accuracy: 0.8847 - val_loss: 1.3373 - val_accuracy: 0.7255\n",
      "Epoch 34/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.4325 - accuracy: 0.8510 - val_loss: 1.0986 - val_accuracy: 0.7059\n",
      "Epoch 35/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3810 - accuracy: 0.8613 - val_loss: 1.3815 - val_accuracy: 0.7010\n",
      "Epoch 36/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3188 - accuracy: 0.8933 - val_loss: 1.3177 - val_accuracy: 0.7108\n",
      "Epoch 37/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2873 - accuracy: 0.8954 - val_loss: 1.1826 - val_accuracy: 0.7500\n",
      "Epoch 38/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2778 - accuracy: 0.9007 - val_loss: 1.2557 - val_accuracy: 0.7402\n",
      "Epoch 39/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2580 - accuracy: 0.9157 - val_loss: 1.5241 - val_accuracy: 0.7255\n",
      "Epoch 40/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.3735 - accuracy: 0.8879 - val_loss: 1.3112 - val_accuracy: 0.6520\n",
      "Epoch 41/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2717 - accuracy: 0.9125 - val_loss: 1.1894 - val_accuracy: 0.7206\n",
      "Epoch 42/500\n",
      "30/30 [==============================] - 1s 26ms/step - loss: 0.2334 - accuracy: 0.9232 - val_loss: 1.2902 - val_accuracy: 0.7598\n",
      "Epoch 43/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2698 - accuracy: 0.9221 - val_loss: 1.2090 - val_accuracy: 0.7255\n",
      "Epoch 44/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2714 - accuracy: 0.9072 - val_loss: 1.2667 - val_accuracy: 0.7500\n",
      "Epoch 45/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.3528 - accuracy: 0.9000 - val_loss: 1.1126 - val_accuracy: 0.7157\n",
      "Epoch 46/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.3307 - accuracy: 0.8794 - val_loss: 1.5349 - val_accuracy: 0.7108\n",
      "Epoch 47/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2361 - accuracy: 0.9157 - val_loss: 1.5807 - val_accuracy: 0.7402\n",
      "Epoch 48/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1867 - accuracy: 0.9360 - val_loss: 1.3430 - val_accuracy: 0.7549\n",
      "Epoch 49/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.1578 - accuracy: 0.9466 - val_loss: 1.3196 - val_accuracy: 0.7843\n",
      "Epoch 50/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2088 - accuracy: 0.9413 - val_loss: 1.5630 - val_accuracy: 0.7647\n",
      "Epoch 51/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1783 - accuracy: 0.9349 - val_loss: 1.4990 - val_accuracy: 0.7549\n",
      "Epoch 52/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1127 - accuracy: 0.9552 - val_loss: 1.6875 - val_accuracy: 0.7647\n",
      "Epoch 53/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1037 - accuracy: 0.9616 - val_loss: 1.6905 - val_accuracy: 0.7598\n",
      "Epoch 54/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2556 - accuracy: 0.9242 - val_loss: 1.2444 - val_accuracy: 0.7402\n",
      "Epoch 55/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1937 - accuracy: 0.9306 - val_loss: 1.5448 - val_accuracy: 0.7647\n",
      "Epoch 56/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1502 - accuracy: 0.9469 - val_loss: 1.6671 - val_accuracy: 0.7696\n",
      "Epoch 57/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1640 - accuracy: 0.9530 - val_loss: 1.6984 - val_accuracy: 0.7696\n",
      "Epoch 58/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1094 - accuracy: 0.9594 - val_loss: 2.0053 - val_accuracy: 0.7304\n",
      "Epoch 59/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1604 - accuracy: 0.9520 - val_loss: 1.6105 - val_accuracy: 0.7647\n",
      "Epoch 60/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1282 - accuracy: 0.9573 - val_loss: 1.4812 - val_accuracy: 0.7941\n",
      "Epoch 61/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0982 - accuracy: 0.9648 - val_loss: 1.7049 - val_accuracy: 0.7549\n",
      "Epoch 62/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1163 - accuracy: 0.9605 - val_loss: 1.8802 - val_accuracy: 0.7500\n",
      "Epoch 63/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1891 - accuracy: 0.9520 - val_loss: 1.5512 - val_accuracy: 0.7402\n",
      "Epoch 64/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2099 - accuracy: 0.9338 - val_loss: 1.3751 - val_accuracy: 0.7500\n",
      "Epoch 65/500\n",
      "30/30 [==============================] - 1s 27ms/step - loss: 0.2600 - accuracy: 0.9178 - val_loss: 1.5198 - val_accuracy: 0.7304\n",
      "Epoch 66/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2466 - accuracy: 0.9157 - val_loss: 1.5678 - val_accuracy: 0.7647\n",
      "Epoch 67/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1766 - accuracy: 0.9344 - val_loss: 1.3684 - val_accuracy: 0.7745\n",
      "Epoch 68/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2216 - accuracy: 0.9168 - val_loss: 1.3408 - val_accuracy: 0.7500\n",
      "Epoch 69/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.3019 - accuracy: 0.92 - 1s 28ms/step - loss: 0.3019 - accuracy: 0.9200 - val_loss: 1.3682 - val_accuracy: 0.6961\n",
      "Epoch 70/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.4362 - accuracy: 0.8762 - val_loss: 1.2646 - val_accuracy: 0.7598\n",
      "Epoch 71/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.3410 - accuracy: 0.8954 - val_loss: 1.1124 - val_accuracy: 0.7353\n",
      "Epoch 72/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2395 - accuracy: 0.9232 - val_loss: 1.5134 - val_accuracy: 0.7500\n",
      "Epoch 73/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2141 - accuracy: 0.9264 - val_loss: 1.0599 - val_accuracy: 0.7549\n",
      "Epoch 74/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2555 - accuracy: 0.9210 - val_loss: 1.3571 - val_accuracy: 0.7549\n",
      "Epoch 75/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1447 - accuracy: 0.9456 - val_loss: 1.3454 - val_accuracy: 0.7402\n",
      "Epoch 76/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1950 - accuracy: 0.9402 - val_loss: 1.4979 - val_accuracy: 0.7794\n",
      "Epoch 77/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2531 - accuracy: 0.9253 - val_loss: 1.2741 - val_accuracy: 0.7451\n",
      "Epoch 78/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1952 - accuracy: 0.9438 - val_loss: 1.3236 - val_accuracy: 0.7598\n",
      "Epoch 79/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1082 - accuracy: 0.9680 - val_loss: 1.4467 - val_accuracy: 0.7843\n",
      "Epoch 80/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.1382 - accuracy: 0.9605 - val_loss: 1.3548 - val_accuracy: 0.7794\n",
      "Epoch 81/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1444 - accuracy: 0.9530 - val_loss: 1.3611 - val_accuracy: 0.7745\n",
      "Epoch 82/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0909 - accuracy: 0.9658 - val_loss: 1.5047 - val_accuracy: 0.7843\n",
      "Epoch 83/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0987 - accuracy: 0.9658 - val_loss: 1.3746 - val_accuracy: 0.7892\n",
      "Epoch 84/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0828 - accuracy: 0.9723 - val_loss: 1.3714 - val_accuracy: 0.8088\n",
      "Epoch 85/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0724 - accuracy: 0.9733 - val_loss: 1.3343 - val_accuracy: 0.7941\n",
      "Epoch 86/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0799 - accuracy: 0.9733 - val_loss: 1.3137 - val_accuracy: 0.7941\n",
      "Epoch 87/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0712 - accuracy: 0.9733 - val_loss: 1.3216 - val_accuracy: 0.8039\n",
      "Epoch 88/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0601 - accuracy: 0.9733 - val_loss: 1.3914 - val_accuracy: 0.7941\n",
      "Epoch 89/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0603 - accuracy: 0.9760 - val_loss: 1.4200 - val_accuracy: 0.8088\n",
      "Epoch 90/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0645 - accuracy: 0.9765 - val_loss: 1.3801 - val_accuracy: 0.7745\n",
      "Epoch 91/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0589 - accuracy: 0.9765 - val_loss: 1.3826 - val_accuracy: 0.7990\n",
      "Epoch 92/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0611 - accuracy: 0.9733 - val_loss: 1.3989 - val_accuracy: 0.7843\n",
      "Epoch 93/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0646 - accuracy: 0.9733 - val_loss: 1.4272 - val_accuracy: 0.7990\n",
      "Epoch 94/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0573 - accuracy: 0.9765 - val_loss: 1.4675 - val_accuracy: 0.7990\n",
      "Epoch 95/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0489 - accuracy: 0.9829 - val_loss: 1.5629 - val_accuracy: 0.7990\n",
      "Epoch 96/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0505 - accuracy: 0.9797 - val_loss: 1.5493 - val_accuracy: 0.7990\n",
      "Epoch 97/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0432 - accuracy: 0.9819 - val_loss: 1.5869 - val_accuracy: 0.8039\n",
      "Epoch 98/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0548 - accuracy: 0.9787 - val_loss: 1.6331 - val_accuracy: 0.7892\n",
      "Epoch 99/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.0544 - accuracy: 0.9797 - val_loss: 1.5841 - val_accuracy: 0.8039\n",
      "Epoch 100/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0536 - accuracy: 0.9750 - val_loss: 1.5739 - val_accuracy: 0.8088\n",
      "Epoch 101/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0460 - accuracy: 0.9829 - val_loss: 1.5621 - val_accuracy: 0.7990\n",
      "Epoch 102/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0641 - accuracy: 0.9733 - val_loss: 1.4896 - val_accuracy: 0.8039\n",
      "Epoch 103/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0690 - accuracy: 0.9723 - val_loss: 1.5982 - val_accuracy: 0.7843\n",
      "Epoch 104/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0490 - accuracy: 0.9776 - val_loss: 1.5604 - val_accuracy: 0.7990\n",
      "Epoch 105/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0483 - accuracy: 0.9829 - val_loss: 1.6562 - val_accuracy: 0.7941\n",
      "Epoch 106/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0580 - accuracy: 0.9755 - val_loss: 1.6052 - val_accuracy: 0.8186\n",
      "Epoch 107/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0403 - accuracy: 0.9851 - val_loss: 1.6274 - val_accuracy: 0.8088\n",
      "Epoch 108/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0658 - accuracy: 0.9733 - val_loss: 1.5749 - val_accuracy: 0.8039\n",
      "Epoch 109/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0644 - accuracy: 0.9797 - val_loss: 1.7014 - val_accuracy: 0.7794\n",
      "Epoch 110/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0960 - accuracy: 0.9691 - val_loss: 1.5254 - val_accuracy: 0.7990\n",
      "Epoch 111/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0912 - accuracy: 0.9677 - val_loss: 1.5608 - val_accuracy: 0.7892\n",
      "Epoch 112/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2918 - accuracy: 0.9232 - val_loss: 1.6315 - val_accuracy: 0.6765\n",
      "Epoch 113/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.5591 - accuracy: 0.8463 - val_loss: 1.2627 - val_accuracy: 0.7255\n",
      "Epoch 114/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.3369 - accuracy: 0.8879 - val_loss: 1.2894 - val_accuracy: 0.7500\n",
      "Epoch 115/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2883 - accuracy: 0.9018 - val_loss: 1.9750 - val_accuracy: 0.6569\n",
      "Epoch 116/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.4731 - accuracy: 0.8730 - val_loss: 1.2085 - val_accuracy: 0.7304\n",
      "Epoch 117/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.3039 - accuracy: 0.9018 - val_loss: 1.1883 - val_accuracy: 0.7304\n",
      "Epoch 118/500\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.3529 - accuracy: 0.9007 - val_loss: 1.4480 - val_accuracy: 0.7010\n",
      "Epoch 119/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.3423 - accuracy: 0.8901 - val_loss: 1.0916 - val_accuracy: 0.7843\n",
      "Epoch 120/500\n",
      "30/30 [==============================] - 1s 41ms/step - loss: 0.1925 - accuracy: 0.9317 - val_loss: 1.1869 - val_accuracy: 0.7843\n",
      "Epoch 121/500\n",
      "30/30 [==============================] - 1s 45ms/step - loss: 0.1515 - accuracy: 0.9456 - val_loss: 1.1598 - val_accuracy: 0.7892\n",
      "Epoch 122/500\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 0.0938 - accuracy: 0.9615 - val_loss: 1.4941 - val_accuracy: 0.7598\n",
      "Epoch 123/500\n",
      "30/30 [==============================] - 1s 48ms/step - loss: 0.0930 - accuracy: 0.9658 - val_loss: 1.3816 - val_accuracy: 0.7598\n",
      "Epoch 124/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1196 - accuracy: 0.9573 - val_loss: 1.1903 - val_accuracy: 0.7990\n",
      "Epoch 125/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1538 - accuracy: 0.9573 - val_loss: 1.2060 - val_accuracy: 0.7941\n",
      "Epoch 126/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1002 - accuracy: 0.9616 - val_loss: 1.1933 - val_accuracy: 0.7843\n",
      "Epoch 127/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1109 - accuracy: 0.9594 - val_loss: 1.2985 - val_accuracy: 0.8039\n",
      "Epoch 128/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1061 - accuracy: 0.9658 - val_loss: 1.1139 - val_accuracy: 0.7892\n",
      "Epoch 129/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0813 - accuracy: 0.9691 - val_loss: 1.2973 - val_accuracy: 0.7892\n",
      "Epoch 130/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.96 - 1s 29ms/step - loss: 0.0876 - accuracy: 0.9691 - val_loss: 1.5774 - val_accuracy: 0.7500\n",
      "Epoch 131/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1709 - accuracy: 0.9424 - val_loss: 1.3119 - val_accuracy: 0.7990\n",
      "Epoch 132/500\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.1910 - accuracy: 0.9392 - val_loss: 1.2736 - val_accuracy: 0.7892\n",
      "Epoch 133/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1480 - accuracy: 0.9531 - val_loss: 1.3822 - val_accuracy: 0.7647\n",
      "Epoch 134/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1806 - accuracy: 0.9392 - val_loss: 1.4261 - val_accuracy: 0.7500\n",
      "Epoch 135/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2067 - accuracy: 0.9317 - val_loss: 1.3400 - val_accuracy: 0.7402\n",
      "Epoch 136/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2197 - accuracy: 0.9285 - val_loss: 1.4232 - val_accuracy: 0.7549\n",
      "Epoch 137/500\n",
      "30/30 [==============================] - 1s 28ms/step - loss: 0.2011 - accuracy: 0.9360 - val_loss: 1.5608 - val_accuracy: 0.7500\n",
      "Epoch 138/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.2512 - accuracy: 0.9392 - val_loss: 1.2743 - val_accuracy: 0.7647\n",
      "Epoch 139/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2423 - accuracy: 0.9274 - val_loss: 1.7509 - val_accuracy: 0.7206\n",
      "Epoch 140/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.3099 - accuracy: 0.9114 - val_loss: 1.2160 - val_accuracy: 0.7353\n",
      "Epoch 141/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.2247 - accuracy: 0.9338 - val_loss: 1.3584 - val_accuracy: 0.7500\n",
      "Epoch 142/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1796 - accuracy: 0.9445 - val_loss: 1.2270 - val_accuracy: 0.7941\n",
      "Epoch 143/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1279 - accuracy: 0.9509 - val_loss: 1.2326 - val_accuracy: 0.7892\n",
      "Epoch 144/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0773 - accuracy: 0.9698 - val_loss: 1.3653 - val_accuracy: 0.8137\n",
      "Epoch 145/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0637 - accuracy: 0.9765 - val_loss: 1.5263 - val_accuracy: 0.7843\n",
      "Epoch 146/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1043 - accuracy: 0.9637 - val_loss: 1.4026 - val_accuracy: 0.7745\n",
      "Epoch 147/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1164 - accuracy: 0.9573 - val_loss: 1.1965 - val_accuracy: 0.7892\n",
      "Epoch 148/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0690 - accuracy: 0.9755 - val_loss: 1.3964 - val_accuracy: 0.7941\n",
      "Epoch 149/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0644 - accuracy: 0.9733 - val_loss: 1.4640 - val_accuracy: 0.7892\n",
      "Epoch 150/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0541 - accuracy: 0.9765 - val_loss: 1.4661 - val_accuracy: 0.7745\n",
      "Epoch 151/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0662 - accuracy: 0.9744 - val_loss: 1.4672 - val_accuracy: 0.7696\n",
      "Epoch 152/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0678 - accuracy: 0.9765 - val_loss: 1.3233 - val_accuracy: 0.7843\n",
      "Epoch 153/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1182 - accuracy: 0.9594 - val_loss: 1.2631 - val_accuracy: 0.7598\n",
      "Epoch 154/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.1162 - accuracy: 0.9626 - val_loss: 1.2721 - val_accuracy: 0.7892\n",
      "Epoch 155/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1139 - accuracy: 0.9677 - val_loss: 1.6745 - val_accuracy: 0.7402\n",
      "Epoch 156/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1523 - accuracy: 0.9520 - val_loss: 1.5832 - val_accuracy: 0.8088\n",
      "Epoch 157/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1228 - accuracy: 0.9658 - val_loss: 1.4726 - val_accuracy: 0.7696\n",
      "Epoch 158/500\n",
      "30/30 [==============================] - 1s 29ms/step - loss: 0.0922 - accuracy: 0.9658 - val_loss: 1.5023 - val_accuracy: 0.7745\n",
      "Epoch 159/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1041 - accuracy: 0.9691 - val_loss: 1.4634 - val_accuracy: 0.7696\n",
      "Epoch 160/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.2030 - accuracy: 0.9402 - val_loss: 1.4611 - val_accuracy: 0.7402\n",
      "Epoch 161/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1841 - accuracy: 0.9413 - val_loss: 1.4146 - val_accuracy: 0.7647\n",
      "Epoch 162/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.2423 - accuracy: 0.9253 - val_loss: 1.4766 - val_accuracy: 0.7549\n",
      "Epoch 163/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.3106 - accuracy: 0.8986 - val_loss: 1.1053 - val_accuracy: 0.7696\n",
      "Epoch 164/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1455 - accuracy: 0.9552 - val_loss: 1.2962 - val_accuracy: 0.8039\n",
      "Epoch 165/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1505 - accuracy: 0.9509 - val_loss: 1.1657 - val_accuracy: 0.7794\n",
      "Epoch 166/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.1891 - accuracy: 0.9427 - val_loss: 1.1570 - val_accuracy: 0.8088\n",
      "Epoch 167/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1143 - accuracy: 0.9616 - val_loss: 1.3784 - val_accuracy: 0.7647\n",
      "Epoch 168/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0604 - accuracy: 0.9797 - val_loss: 1.5457 - val_accuracy: 0.7843\n",
      "Epoch 169/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0835 - accuracy: 0.9701 - val_loss: 1.3791 - val_accuracy: 0.7647\n",
      "Epoch 170/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0719 - accuracy: 0.9776 - val_loss: 1.2911 - val_accuracy: 0.8137\n",
      "Epoch 171/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0692 - accuracy: 0.9701 - val_loss: 1.4172 - val_accuracy: 0.8137\n",
      "Epoch 172/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0490 - accuracy: 0.9829 - val_loss: 1.4370 - val_accuracy: 0.8039\n",
      "Epoch 173/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0576 - accuracy: 0.9776 - val_loss: 1.4689 - val_accuracy: 0.7892\n",
      "Epoch 174/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0399 - accuracy: 0.9829 - val_loss: 1.4635 - val_accuracy: 0.7990\n",
      "Epoch 175/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0547 - accuracy: 0.9819 - val_loss: 1.4904 - val_accuracy: 0.8088\n",
      "Epoch 176/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0446 - accuracy: 0.9765 - val_loss: 1.4939 - val_accuracy: 0.8088\n",
      "Epoch 177/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0473 - accuracy: 0.9844 - val_loss: 1.4225 - val_accuracy: 0.8137\n",
      "Epoch 178/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0365 - accuracy: 0.9819 - val_loss: 1.4192 - val_accuracy: 0.8137\n",
      "Epoch 179/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 1.4895 - val_accuracy: 0.8137\n",
      "Epoch 180/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0519 - accuracy: 0.9808 - val_loss: 1.5142 - val_accuracy: 0.7990\n",
      "Epoch 181/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0472 - accuracy: 0.9776 - val_loss: 1.5056 - val_accuracy: 0.7990\n",
      "Epoch 182/500\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0465 - accuracy: 0.9829 - val_loss: 1.5308 - val_accuracy: 0.8186\n",
      "Epoch 183/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0458 - accuracy: 0.9861 - val_loss: 1.4961 - val_accuracy: 0.8039\n",
      "Epoch 184/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0329 - accuracy: 0.9883 - val_loss: 1.5040 - val_accuracy: 0.8088\n",
      "Epoch 185/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0490 - accuracy: 0.9819 - val_loss: 1.4875 - val_accuracy: 0.8137\n",
      "Epoch 186/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0404 - accuracy: 0.9851 - val_loss: 1.4856 - val_accuracy: 0.7941\n",
      "Epoch 187/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0500 - accuracy: 0.9819 - val_loss: 1.4565 - val_accuracy: 0.8039\n",
      "Epoch 188/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0589 - accuracy: 0.9792 - val_loss: 1.4905 - val_accuracy: 0.8137\n",
      "Epoch 189/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0445 - accuracy: 0.9797 - val_loss: 1.5795 - val_accuracy: 0.7843\n",
      "Epoch 190/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0434 - accuracy: 0.9840 - val_loss: 1.5862 - val_accuracy: 0.8137\n",
      "Epoch 191/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0454 - accuracy: 0.9840 - val_loss: 1.4211 - val_accuracy: 0.8088\n",
      "Epoch 192/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0344 - accuracy: 0.9840 - val_loss: 1.4526 - val_accuracy: 0.8333\n",
      "Epoch 193/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0520 - accuracy: 0.9776 - val_loss: 1.4868 - val_accuracy: 0.8039\n",
      "Epoch 194/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0443 - accuracy: 0.9797 - val_loss: 1.4830 - val_accuracy: 0.8284\n",
      "Epoch 195/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0362 - accuracy: 0.9872 - val_loss: 1.5431 - val_accuracy: 0.8235\n",
      "Epoch 196/500\n",
      "30/30 [==============================] - 1s 30ms/step - loss: 0.0348 - accuracy: 0.9851 - val_loss: 1.5559 - val_accuracy: 0.7941\n",
      "Epoch 197/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0401 - accuracy: 0.9872 - val_loss: 1.5467 - val_accuracy: 0.7990\n",
      "Epoch 198/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0392 - accuracy: 0.9840 - val_loss: 1.5159 - val_accuracy: 0.7941\n",
      "Epoch 199/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0376 - accuracy: 0.9865 - val_loss: 1.5545 - val_accuracy: 0.8088\n",
      "Epoch 200/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0794 - accuracy: 0.9776 - val_loss: 1.5993 - val_accuracy: 0.7941\n",
      "Epoch 201/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2497 - accuracy: 0.9349 - val_loss: 1.5311 - val_accuracy: 0.7549\n",
      "Epoch 202/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3293 - accuracy: 0.9039 - val_loss: 1.5562 - val_accuracy: 0.7451\n",
      "Epoch 203/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3238 - accuracy: 0.9029 - val_loss: 1.6202 - val_accuracy: 0.7353\n",
      "Epoch 204/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3661 - accuracy: 0.9029 - val_loss: 1.5523 - val_accuracy: 0.7402\n",
      "Epoch 205/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.4527 - accuracy: 0.8666 - val_loss: 1.4237 - val_accuracy: 0.7059\n",
      "Epoch 206/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2946 - accuracy: 0.9093 - val_loss: 1.3481 - val_accuracy: 0.7647\n",
      "Epoch 207/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.2251 - accuracy: 0.9370 - val_loss: 1.7318 - val_accuracy: 0.7451\n",
      "Epoch 208/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1781 - accuracy: 0.9381 - val_loss: 1.7125 - val_accuracy: 0.7549\n",
      "Epoch 209/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1477 - accuracy: 0.9584 - val_loss: 1.5985 - val_accuracy: 0.7451\n",
      "Epoch 210/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1047 - accuracy: 0.9615 - val_loss: 1.7807 - val_accuracy: 0.7745\n",
      "Epoch 211/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1432 - accuracy: 0.9530 - val_loss: 1.6700 - val_accuracy: 0.7500\n",
      "Epoch 212/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1175 - accuracy: 0.9584 - val_loss: 1.7385 - val_accuracy: 0.7843\n",
      "Epoch 213/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1129 - accuracy: 0.9584 - val_loss: 1.8609 - val_accuracy: 0.7745\n",
      "Epoch 214/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.2201 - accuracy: 0.9338 - val_loss: 1.3284 - val_accuracy: 0.7843\n",
      "Epoch 215/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1248 - accuracy: 0.9562 - val_loss: 1.2901 - val_accuracy: 0.7794\n",
      "Epoch 216/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1773 - accuracy: 0.9477 - val_loss: 1.8192 - val_accuracy: 0.7549\n",
      "Epoch 217/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1743 - accuracy: 0.9466 - val_loss: 2.0438 - val_accuracy: 0.7647\n",
      "Epoch 218/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1912 - accuracy: 0.9413 - val_loss: 1.6777 - val_accuracy: 0.7843\n",
      "Epoch 219/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.1479 - accuracy: 0.9477 - val_loss: 1.4490 - val_accuracy: 0.7941\n",
      "Epoch 220/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0927 - accuracy: 0.9701 - val_loss: 1.5053 - val_accuracy: 0.7843\n",
      "Epoch 221/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0866 - accuracy: 0.9677 - val_loss: 1.6766 - val_accuracy: 0.7794\n",
      "Epoch 222/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1049 - accuracy: 0.9712 - val_loss: 1.6595 - val_accuracy: 0.7941\n",
      "Epoch 223/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0902 - accuracy: 0.9723 - val_loss: 1.6331 - val_accuracy: 0.7745\n",
      "Epoch 224/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0953 - accuracy: 0.9755 - val_loss: 1.6622 - val_accuracy: 0.7745\n",
      "Epoch 225/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0516 - accuracy: 0.9776 - val_loss: 1.6799 - val_accuracy: 0.7647\n",
      "Epoch 226/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0411 - accuracy: 0.9829 - val_loss: 1.7982 - val_accuracy: 0.7745\n",
      "Epoch 227/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0467 - accuracy: 0.9808 - val_loss: 1.7139 - val_accuracy: 0.8088\n",
      "Epoch 228/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0419 - accuracy: 0.9819 - val_loss: 1.6985 - val_accuracy: 0.8088\n",
      "Epoch 229/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0380 - accuracy: 0.9861 - val_loss: 1.7803 - val_accuracy: 0.8137\n",
      "Epoch 230/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0508 - accuracy: 0.9819 - val_loss: 1.9746 - val_accuracy: 0.7647\n",
      "Epoch 231/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0482 - accuracy: 0.9829 - val_loss: 1.9764 - val_accuracy: 0.7745\n",
      "Epoch 232/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0769 - accuracy: 0.9760 - val_loss: 1.8377 - val_accuracy: 0.7843\n",
      "Epoch 233/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0912 - accuracy: 0.9691 - val_loss: 1.7496 - val_accuracy: 0.7696\n",
      "Epoch 234/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1074 - accuracy: 0.9648 - val_loss: 1.5762 - val_accuracy: 0.7794\n",
      "Epoch 235/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0863 - accuracy: 0.9723 - val_loss: 1.6606 - val_accuracy: 0.7843\n",
      "Epoch 236/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0925 - accuracy: 0.9744 - val_loss: 1.6124 - val_accuracy: 0.7892\n",
      "Epoch 237/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0746 - accuracy: 0.9797 - val_loss: 1.6693 - val_accuracy: 0.7892\n",
      "Epoch 238/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1255 - accuracy: 0.9605 - val_loss: 1.9340 - val_accuracy: 0.7598\n",
      "Epoch 239/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.1291 - accuracy: 0.9552 - val_loss: 1.6399 - val_accuracy: 0.7598\n",
      "Epoch 240/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2544 - accuracy: 0.9360 - val_loss: 1.8823 - val_accuracy: 0.7500\n",
      "Epoch 241/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2113 - accuracy: 0.9338 - val_loss: 1.8350 - val_accuracy: 0.7892\n",
      "Epoch 242/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1659 - accuracy: 0.9541 - val_loss: 1.4519 - val_accuracy: 0.7843\n",
      "Epoch 243/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1001 - accuracy: 0.9677 - val_loss: 1.6825 - val_accuracy: 0.7549\n",
      "Epoch 244/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1539 - accuracy: 0.9605 - val_loss: 2.0456 - val_accuracy: 0.7745\n",
      "Epoch 245/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2044 - accuracy: 0.9466 - val_loss: 2.0406 - val_accuracy: 0.7353\n",
      "Epoch 246/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.1432 - accuracy: 0.9584 - val_loss: 1.7747 - val_accuracy: 0.7745\n",
      "Epoch 247/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0518 - accuracy: 0.9840 - val_loss: 1.5993 - val_accuracy: 0.7794\n",
      "Epoch 248/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1270 - accuracy: 0.9637 - val_loss: 1.6262 - val_accuracy: 0.7647\n",
      "Epoch 249/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0487 - accuracy: 0.9787 - val_loss: 1.6956 - val_accuracy: 0.7794\n",
      "Epoch 250/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0858 - accuracy: 0.9712 - val_loss: 1.9247 - val_accuracy: 0.7549\n",
      "Epoch 251/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0838 - accuracy: 0.9733 - val_loss: 1.5094 - val_accuracy: 0.7745\n",
      "Epoch 252/500\n",
      "30/30 [==============================] - 1s 44ms/step - loss: 0.0504 - accuracy: 0.9829 - val_loss: 1.5598 - val_accuracy: 0.7794\n",
      "Epoch 253/500\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.0406 - accuracy: 0.9840 - val_loss: 1.5353 - val_accuracy: 0.7990\n",
      "Epoch 254/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0546 - accuracy: 0.9802 - val_loss: 1.5908 - val_accuracy: 0.7794\n",
      "Epoch 255/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0532 - accuracy: 0.9808 - val_loss: 1.6463 - val_accuracy: 0.7941\n",
      "Epoch 256/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0782 - accuracy: 0.9755 - val_loss: 1.9226 - val_accuracy: 0.7598\n",
      "Epoch 257/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1053 - accuracy: 0.9637 - val_loss: 1.7697 - val_accuracy: 0.7500\n",
      "Epoch 258/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1452 - accuracy: 0.9648 - val_loss: 1.7573 - val_accuracy: 0.7549\n",
      "Epoch 259/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1242 - accuracy: 0.9616 - val_loss: 1.4982 - val_accuracy: 0.7794\n",
      "Epoch 260/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0645 - accuracy: 0.9744 - val_loss: 1.7362 - val_accuracy: 0.8039\n",
      "Epoch 261/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0356 - accuracy: 0.9861 - val_loss: 1.8776 - val_accuracy: 0.7941\n",
      "Epoch 262/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0551 - accuracy: 0.9819 - val_loss: 1.7205 - val_accuracy: 0.8088\n",
      "Epoch 263/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0393 - accuracy: 0.9829 - val_loss: 1.7168 - val_accuracy: 0.7892\n",
      "Epoch 264/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0610 - accuracy: 0.9797 - val_loss: 1.6960 - val_accuracy: 0.7794\n",
      "Epoch 265/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0292 - accuracy: 0.9854 - val_loss: 1.7394 - val_accuracy: 0.8039\n",
      "Epoch 266/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0376 - accuracy: 0.9883 - val_loss: 1.7651 - val_accuracy: 0.7941\n",
      "Epoch 267/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0424 - accuracy: 0.9861 - val_loss: 1.5612 - val_accuracy: 0.7892\n",
      "Epoch 268/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0721 - accuracy: 0.9808 - val_loss: 1.7107 - val_accuracy: 0.7745\n",
      "Epoch 269/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0512 - accuracy: 0.9861 - val_loss: 1.6318 - val_accuracy: 0.7990\n",
      "Epoch 270/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0182 - accuracy: 0.9936 - val_loss: 1.7203 - val_accuracy: 0.8039\n",
      "Epoch 271/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0218 - accuracy: 0.9925 - val_loss: 1.8198 - val_accuracy: 0.8088\n",
      "Epoch 272/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0180 - accuracy: 0.9936 - val_loss: 1.7191 - val_accuracy: 0.7892\n",
      "Epoch 273/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0257 - accuracy: 0.9893 - val_loss: 1.8439 - val_accuracy: 0.8039\n",
      "Epoch 274/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0277 - accuracy: 0.9872 - val_loss: 1.9688 - val_accuracy: 0.7892\n",
      "Epoch 275/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0340 - accuracy: 0.9872 - val_loss: 1.8502 - val_accuracy: 0.7941\n",
      "Epoch 276/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0347 - accuracy: 0.9865 - val_loss: 1.7783 - val_accuracy: 0.7892\n",
      "Epoch 277/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0235 - accuracy: 0.9883 - val_loss: 1.7498 - val_accuracy: 0.7990\n",
      "Epoch 278/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0171 - accuracy: 0.9936 - val_loss: 1.8014 - val_accuracy: 0.8039\n",
      "Epoch 279/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0164 - accuracy: 0.9947 - val_loss: 1.8708 - val_accuracy: 0.8088\n",
      "Epoch 280/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0096 - accuracy: 0.9979 - val_loss: 1.8797 - val_accuracy: 0.7990\n",
      "Epoch 281/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0270 - accuracy: 0.9883 - val_loss: 1.9629 - val_accuracy: 0.7990\n",
      "Epoch 282/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0326 - accuracy: 0.9872 - val_loss: 1.7717 - val_accuracy: 0.7892\n",
      "Epoch 283/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0197 - accuracy: 0.9904 - val_loss: 1.8754 - val_accuracy: 0.7941\n",
      "Epoch 284/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0229 - accuracy: 0.9904 - val_loss: 1.9059 - val_accuracy: 0.8039\n",
      "Epoch 285/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0241 - accuracy: 0.9893 - val_loss: 2.0093 - val_accuracy: 0.8186\n",
      "Epoch 286/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0351 - accuracy: 0.9872 - val_loss: 1.8422 - val_accuracy: 0.7990\n",
      "Epoch 287/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0286 - accuracy: 0.9875 - val_loss: 2.0457 - val_accuracy: 0.8137\n",
      "Epoch 288/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0250 - accuracy: 0.9893 - val_loss: 1.8267 - val_accuracy: 0.8137\n",
      "Epoch 289/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0265 - accuracy: 0.9904 - val_loss: 1.7971 - val_accuracy: 0.8137\n",
      "Epoch 290/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0490 - accuracy: 0.9861 - val_loss: 2.2466 - val_accuracy: 0.7794\n",
      "Epoch 291/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1875 - accuracy: 0.9530 - val_loss: 1.4074 - val_accuracy: 0.7696\n",
      "Epoch 292/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1799 - accuracy: 0.9562 - val_loss: 1.5481 - val_accuracy: 0.7500\n",
      "Epoch 293/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.4853 - accuracy: 0.8826 - val_loss: 1.6564 - val_accuracy: 0.7255\n",
      "Epoch 294/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.3728 - accuracy: 0.9029 - val_loss: 1.6225 - val_accuracy: 0.7402\n",
      "Epoch 295/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.3938 - accuracy: 0.8901 - val_loss: 2.0696 - val_accuracy: 0.7255\n",
      "Epoch 296/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.3014 - accuracy: 0.9157 - val_loss: 1.5186 - val_accuracy: 0.7549\n",
      "Epoch 297/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1916 - accuracy: 0.9338 - val_loss: 1.7283 - val_accuracy: 0.7647\n",
      "Epoch 298/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0776 - accuracy: 0.9688 - val_loss: 1.4924 - val_accuracy: 0.7696\n",
      "Epoch 299/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1590 - accuracy: 0.9573 - val_loss: 1.5624 - val_accuracy: 0.7892\n",
      "Epoch 300/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1206 - accuracy: 0.9658 - val_loss: 1.3721 - val_accuracy: 0.7892\n",
      "Epoch 301/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1055 - accuracy: 0.9637 - val_loss: 1.2652 - val_accuracy: 0.7843\n",
      "Epoch 302/500\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.1000 - accuracy: 0.9776 - val_loss: 1.4748 - val_accuracy: 0.7745\n",
      "Epoch 303/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1301 - accuracy: 0.9648 - val_loss: 1.3500 - val_accuracy: 0.8137\n",
      "Epoch 304/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1669 - accuracy: 0.9626 - val_loss: 2.0686 - val_accuracy: 0.7598\n",
      "Epoch 305/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0702 - accuracy: 0.9829 - val_loss: 1.6105 - val_accuracy: 0.7892\n",
      "Epoch 306/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0370 - accuracy: 0.9851 - val_loss: 1.7362 - val_accuracy: 0.8186\n",
      "Epoch 307/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0569 - accuracy: 0.9808 - val_loss: 1.6979 - val_accuracy: 0.8088\n",
      "Epoch 308/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0364 - accuracy: 0.9872 - val_loss: 1.7501 - val_accuracy: 0.8088\n",
      "Epoch 309/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0275 - accuracy: 0.9885 - val_loss: 1.8401 - val_accuracy: 0.8039\n",
      "Epoch 310/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0231 - accuracy: 0.9904 - val_loss: 1.8594 - val_accuracy: 0.7990\n",
      "Epoch 311/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0214 - accuracy: 0.9915 - val_loss: 1.9025 - val_accuracy: 0.8186\n",
      "Epoch 312/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0477 - accuracy: 0.9829 - val_loss: 1.8075 - val_accuracy: 0.8088\n",
      "Epoch 313/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0206 - accuracy: 0.9893 - val_loss: 1.7726 - val_accuracy: 0.8137\n",
      "Epoch 314/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0230 - accuracy: 0.9925 - val_loss: 1.9107 - val_accuracy: 0.8088\n",
      "Epoch 315/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0504 - accuracy: 0.9829 - val_loss: 1.6919 - val_accuracy: 0.8186\n",
      "Epoch 316/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0352 - accuracy: 0.9872 - val_loss: 2.1330 - val_accuracy: 0.7941\n",
      "Epoch 317/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0624 - accuracy: 0.9808 - val_loss: 2.0462 - val_accuracy: 0.7941\n",
      "Epoch 318/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0663 - accuracy: 0.9776 - val_loss: 2.0607 - val_accuracy: 0.7941\n",
      "Epoch 319/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0639 - accuracy: 0.9819 - val_loss: 1.9465 - val_accuracy: 0.7892\n",
      "Epoch 320/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2217 - accuracy: 0.9521 - val_loss: 1.8314 - val_accuracy: 0.7843\n",
      "Epoch 321/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1115 - accuracy: 0.9755 - val_loss: 1.7578 - val_accuracy: 0.7598\n",
      "Epoch 322/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2186 - accuracy: 0.9466 - val_loss: 1.4670 - val_accuracy: 0.7696\n",
      "Epoch 323/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0703 - accuracy: 0.9712 - val_loss: 1.4521 - val_accuracy: 0.8039\n",
      "Epoch 324/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0895 - accuracy: 0.9765 - val_loss: 2.0260 - val_accuracy: 0.8088\n",
      "Epoch 325/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0831 - accuracy: 0.9712 - val_loss: 2.1505 - val_accuracy: 0.7598\n",
      "Epoch 326/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1527 - accuracy: 0.9573 - val_loss: 1.9014 - val_accuracy: 0.7598\n",
      "Epoch 327/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1001 - accuracy: 0.9680 - val_loss: 1.8737 - val_accuracy: 0.8039\n",
      "Epoch 328/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1244 - accuracy: 0.9712 - val_loss: 1.6572 - val_accuracy: 0.7990\n",
      "Epoch 329/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0742 - accuracy: 0.9776 - val_loss: 1.5970 - val_accuracy: 0.8088\n",
      "Epoch 330/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0851 - accuracy: 0.9765 - val_loss: 1.4950 - val_accuracy: 0.7941\n",
      "Epoch 331/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1603 - accuracy: 0.9615 - val_loss: 1.5474 - val_accuracy: 0.7892\n",
      "Epoch 332/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0755 - accuracy: 0.9808 - val_loss: 1.9617 - val_accuracy: 0.7549\n",
      "Epoch 333/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0694 - accuracy: 0.9808 - val_loss: 1.5791 - val_accuracy: 0.7843\n",
      "Epoch 334/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0262 - accuracy: 0.9915 - val_loss: 1.7239 - val_accuracy: 0.7892\n",
      "Epoch 335/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0349 - accuracy: 0.9872 - val_loss: 1.6765 - val_accuracy: 0.8039\n",
      "Epoch 336/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0179 - accuracy: 0.9947 - val_loss: 1.6583 - val_accuracy: 0.8039\n",
      "Epoch 337/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0170 - accuracy: 0.9947 - val_loss: 1.6495 - val_accuracy: 0.8137\n",
      "Epoch 338/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0223 - accuracy: 0.9904 - val_loss: 1.7724 - val_accuracy: 0.8039\n",
      "Epoch 339/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0156 - accuracy: 0.9947 - val_loss: 1.8021 - val_accuracy: 0.7941\n",
      "Epoch 340/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0154 - accuracy: 0.9957 - val_loss: 1.8204 - val_accuracy: 0.8039\n",
      "Epoch 341/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0104 - accuracy: 0.9979 - val_loss: 1.9874 - val_accuracy: 0.8137\n",
      "Epoch 342/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0180 - accuracy: 0.9917 - val_loss: 2.0077 - val_accuracy: 0.7941\n",
      "Epoch 343/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0141 - accuracy: 0.9936 - val_loss: 2.0437 - val_accuracy: 0.8088\n",
      "Epoch 344/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0122 - accuracy: 0.9957 - val_loss: 2.0980 - val_accuracy: 0.8137\n",
      "Epoch 345/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0252 - accuracy: 0.9904 - val_loss: 2.1197 - val_accuracy: 0.8186\n",
      "Epoch 346/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0117 - accuracy: 0.9968 - val_loss: 2.1407 - val_accuracy: 0.8186\n",
      "Epoch 347/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0119 - accuracy: 0.9957 - val_loss: 2.2531 - val_accuracy: 0.8137\n",
      "Epoch 348/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0325 - accuracy: 0.9915 - val_loss: 1.9623 - val_accuracy: 0.7941\n",
      "Epoch 349/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0399 - accuracy: 0.9861 - val_loss: 2.0798 - val_accuracy: 0.7941\n",
      "Epoch 350/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0194 - accuracy: 0.9925 - val_loss: 2.1571 - val_accuracy: 0.8137\n",
      "Epoch 351/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0308 - accuracy: 0.9904 - val_loss: 2.1603 - val_accuracy: 0.7843\n",
      "Epoch 352/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1107 - accuracy: 0.9680 - val_loss: 2.2658 - val_accuracy: 0.7500\n",
      "Epoch 353/500\n",
      "30/30 [==============================] - 1s 40ms/step - loss: 0.2454 - accuracy: 0.9417 - val_loss: 1.3950 - val_accuracy: 0.7304\n",
      "Epoch 354/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.4335 - accuracy: 0.9029 - val_loss: 1.4974 - val_accuracy: 0.7353\n",
      "Epoch 355/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2762 - accuracy: 0.9381 - val_loss: 2.2370 - val_accuracy: 0.7304\n",
      "Epoch 356/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2152 - accuracy: 0.9370 - val_loss: 2.2085 - val_accuracy: 0.7745\n",
      "Epoch 357/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.2662 - accuracy: 0.9424 - val_loss: 1.5594 - val_accuracy: 0.7794\n",
      "Epoch 358/500\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.1194 - accuracy: 0.9669 - val_loss: 1.8772 - val_accuracy: 0.7990\n",
      "Epoch 359/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0472 - accuracy: 0.9819 - val_loss: 2.0667 - val_accuracy: 0.7892\n",
      "Epoch 360/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0579 - accuracy: 0.9797 - val_loss: 1.9992 - val_accuracy: 0.8039\n",
      "Epoch 361/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.1090 - accuracy: 0.9701 - val_loss: 1.9322 - val_accuracy: 0.7794\n",
      "Epoch 362/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1322 - accuracy: 0.9658 - val_loss: 2.1004 - val_accuracy: 0.7598\n",
      "Epoch 363/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.1766 - accuracy: 0.9562 - val_loss: 1.6000 - val_accuracy: 0.7745\n",
      "Epoch 364/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.1557 - accuracy: 0.9521 - val_loss: 1.8305 - val_accuracy: 0.7451\n",
      "Epoch 365/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.1809 - accuracy: 0.9541 - val_loss: 1.5258 - val_accuracy: 0.7843\n",
      "Epoch 366/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.2282 - accuracy: 0.9488 - val_loss: 1.6594 - val_accuracy: 0.7745\n",
      "Epoch 367/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.3619 - accuracy: 0.9274 - val_loss: 1.4034 - val_accuracy: 0.7696\n",
      "Epoch 368/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.1655 - accuracy: 0.9509 - val_loss: 1.3969 - val_accuracy: 0.7843\n",
      "Epoch 369/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1408 - accuracy: 0.9562 - val_loss: 1.9044 - val_accuracy: 0.7990\n",
      "Epoch 370/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0666 - accuracy: 0.9776 - val_loss: 1.8268 - val_accuracy: 0.7990\n",
      "Epoch 371/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0883 - accuracy: 0.98 - 1s 34ms/step - loss: 0.0858 - accuracy: 0.9808 - val_loss: 1.6541 - val_accuracy: 0.7892\n",
      "Epoch 372/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0613 - accuracy: 0.9851 - val_loss: 1.4165 - val_accuracy: 0.8039\n",
      "Epoch 373/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0332 - accuracy: 0.9872 - val_loss: 1.5128 - val_accuracy: 0.8235\n",
      "Epoch 374/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0371 - accuracy: 0.9840 - val_loss: 1.5738 - val_accuracy: 0.8039\n",
      "Epoch 375/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0235 - accuracy: 0.9937 - val_loss: 1.5489 - val_accuracy: 0.8088\n",
      "Epoch 376/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0210 - accuracy: 0.9925 - val_loss: 1.5246 - val_accuracy: 0.8284\n",
      "Epoch 377/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 1.5948 - val_accuracy: 0.8235\n",
      "Epoch 378/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0241 - accuracy: 0.9872 - val_loss: 1.5709 - val_accuracy: 0.8284\n",
      "Epoch 379/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 1.4588 - val_accuracy: 0.8039\n",
      "Epoch 380/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0117 - accuracy: 0.9957 - val_loss: 1.6012 - val_accuracy: 0.8088\n",
      "Epoch 381/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0242 - accuracy: 0.9893 - val_loss: 1.5926 - val_accuracy: 0.8039\n",
      "Epoch 382/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0173 - accuracy: 0.9936 - val_loss: 1.6086 - val_accuracy: 0.8137\n",
      "Epoch 383/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0287 - accuracy: 0.9904 - val_loss: 1.6214 - val_accuracy: 0.8137\n",
      "Epoch 384/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0162 - accuracy: 0.9936 - val_loss: 1.5967 - val_accuracy: 0.8186\n",
      "Epoch 385/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0225 - accuracy: 0.9904 - val_loss: 1.5847 - val_accuracy: 0.8284\n",
      "Epoch 386/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0190 - accuracy: 0.9927 - val_loss: 1.6477 - val_accuracy: 0.8235\n",
      "Epoch 387/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0257 - accuracy: 0.9925 - val_loss: 1.6516 - val_accuracy: 0.8186\n",
      "Epoch 388/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0165 - accuracy: 0.9925 - val_loss: 1.7261 - val_accuracy: 0.8186\n",
      "Epoch 389/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0235 - accuracy: 0.9925 - val_loss: 1.6888 - val_accuracy: 0.8137\n",
      "Epoch 390/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0197 - accuracy: 0.9893 - val_loss: 1.7360 - val_accuracy: 0.8137\n",
      "Epoch 391/500\n",
      "30/30 [==============================] - ETA: 0s - loss: 0.0185 - accuracy: 0.99 - 1s 34ms/step - loss: 0.0179 - accuracy: 0.9925 - val_loss: 1.7990 - val_accuracy: 0.8333\n",
      "Epoch 392/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0188 - accuracy: 0.9893 - val_loss: 1.7184 - val_accuracy: 0.8333\n",
      "Epoch 393/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0191 - accuracy: 0.9947 - val_loss: 1.8104 - val_accuracy: 0.8333\n",
      "Epoch 394/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0202 - accuracy: 0.9925 - val_loss: 1.8516 - val_accuracy: 0.8186\n",
      "Epoch 395/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0137 - accuracy: 0.9936 - val_loss: 1.7982 - val_accuracy: 0.8284\n",
      "Epoch 396/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0128 - accuracy: 0.9947 - val_loss: 1.8315 - val_accuracy: 0.8284\n",
      "Epoch 397/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0274 - accuracy: 0.9896 - val_loss: 1.8655 - val_accuracy: 0.8137\n",
      "Epoch 398/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0497 - accuracy: 0.9819 - val_loss: 1.6283 - val_accuracy: 0.8039\n",
      "Epoch 399/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0329 - accuracy: 0.9872 - val_loss: 1.7992 - val_accuracy: 0.7892\n",
      "Epoch 400/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0179 - accuracy: 0.9915 - val_loss: 1.8437 - val_accuracy: 0.7941\n",
      "Epoch 401/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0272 - accuracy: 0.9904 - val_loss: 1.9263 - val_accuracy: 0.7941\n",
      "Epoch 402/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0403 - accuracy: 0.9915 - val_loss: 1.9055 - val_accuracy: 0.7843\n",
      "Epoch 403/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0272 - accuracy: 0.9883 - val_loss: 1.7904 - val_accuracy: 0.7892\n",
      "Epoch 404/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0253 - accuracy: 0.9936 - val_loss: 2.0215 - val_accuracy: 0.7990\n",
      "Epoch 405/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0111 - accuracy: 0.9947 - val_loss: 2.1431 - val_accuracy: 0.8186\n",
      "Epoch 406/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0069 - accuracy: 0.9968 - val_loss: 2.1750 - val_accuracy: 0.8137\n",
      "Epoch 407/500\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0052 - accuracy: 0.9989 - val_loss: 2.3175 - val_accuracy: 0.8088\n",
      "Epoch 408/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0093 - accuracy: 0.9958 - val_loss: 2.1590 - val_accuracy: 0.8088\n",
      "Epoch 409/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0630 - accuracy: 0.9851 - val_loss: 2.0652 - val_accuracy: 0.8137\n",
      "Epoch 410/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0356 - accuracy: 0.9840 - val_loss: 1.6554 - val_accuracy: 0.8137\n",
      "Epoch 411/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0186 - accuracy: 0.9947 - val_loss: 1.6509 - val_accuracy: 0.8186\n",
      "Epoch 412/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0191 - accuracy: 0.9915 - val_loss: 1.7980 - val_accuracy: 0.8284\n",
      "Epoch 413/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0190 - accuracy: 0.9947 - val_loss: 1.8994 - val_accuracy: 0.8186\n",
      "Epoch 414/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0261 - accuracy: 0.9893 - val_loss: 1.8593 - val_accuracy: 0.8186\n",
      "Epoch 415/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0088 - accuracy: 0.9979 - val_loss: 1.9014 - val_accuracy: 0.8186\n",
      "Epoch 416/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.0113 - accuracy: 0.9968 - val_loss: 1.9692 - val_accuracy: 0.8186\n",
      "Epoch 417/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0087 - accuracy: 0.9979 - val_loss: 1.9798 - val_accuracy: 0.8284\n",
      "Epoch 418/500\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.0072 - accuracy: 0.9968 - val_loss: 2.0243 - val_accuracy: 0.8284\n",
      "Epoch 419/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 2.0403 - val_accuracy: 0.8284\n",
      "Epoch 420/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0536 - accuracy: 0.9893 - val_loss: 1.7177 - val_accuracy: 0.8235\n",
      "Epoch 421/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0616 - accuracy: 0.9925 - val_loss: 1.5817 - val_accuracy: 0.8137\n",
      "Epoch 422/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0601 - accuracy: 0.9776 - val_loss: 1.8620 - val_accuracy: 0.7941\n",
      "Epoch 423/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.3217 - accuracy: 0.9274 - val_loss: 1.7088 - val_accuracy: 0.7059\n",
      "Epoch 424/500\n",
      "30/30 [==============================] - 1s 37ms/step - loss: 0.5552 - accuracy: 0.8634 - val_loss: 1.8379 - val_accuracy: 0.7304\n",
      "Epoch 425/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.3621 - accuracy: 0.9018 - val_loss: 1.4205 - val_accuracy: 0.7451\n",
      "Epoch 426/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.5442 - accuracy: 0.8901 - val_loss: 1.6273 - val_accuracy: 0.6814\n",
      "Epoch 427/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.4600 - accuracy: 0.8709 - val_loss: 1.8961 - val_accuracy: 0.7451\n",
      "Epoch 428/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.2984 - accuracy: 0.9242 - val_loss: 1.4409 - val_accuracy: 0.7647\n",
      "Epoch 429/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.1729 - accuracy: 0.9424 - val_loss: 1.4094 - val_accuracy: 0.7794\n",
      "Epoch 430/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.1532 - accuracy: 0.9625 - val_loss: 1.4099 - val_accuracy: 0.8039\n",
      "Epoch 431/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0801 - accuracy: 0.9776 - val_loss: 1.5224 - val_accuracy: 0.7941\n",
      "Epoch 432/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0475 - accuracy: 0.9872 - val_loss: 1.8104 - val_accuracy: 0.7941\n",
      "Epoch 433/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0480 - accuracy: 0.9797 - val_loss: 1.8477 - val_accuracy: 0.7647\n",
      "Epoch 434/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0758 - accuracy: 0.9744 - val_loss: 1.9894 - val_accuracy: 0.7794\n",
      "Epoch 435/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0878 - accuracy: 0.9691 - val_loss: 1.7420 - val_accuracy: 0.7990\n",
      "Epoch 436/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.1417 - accuracy: 0.9744 - val_loss: 1.6142 - val_accuracy: 0.7941\n",
      "Epoch 437/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0832 - accuracy: 0.9797 - val_loss: 1.8629 - val_accuracy: 0.7843\n",
      "Epoch 438/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0757 - accuracy: 0.9797 - val_loss: 1.2851 - val_accuracy: 0.7745\n",
      "Epoch 439/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0941 - accuracy: 0.9776 - val_loss: 1.5127 - val_accuracy: 0.7990\n",
      "Epoch 440/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0578 - accuracy: 0.9840 - val_loss: 1.3749 - val_accuracy: 0.8137\n",
      "Epoch 441/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0552 - accuracy: 0.9823 - val_loss: 1.2057 - val_accuracy: 0.8039\n",
      "Epoch 442/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0298 - accuracy: 0.9872 - val_loss: 1.4272 - val_accuracy: 0.8137\n",
      "Epoch 443/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0242 - accuracy: 0.9904 - val_loss: 1.5037 - val_accuracy: 0.8137\n",
      "Epoch 444/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0137 - accuracy: 0.9936 - val_loss: 1.3397 - val_accuracy: 0.8186\n",
      "Epoch 445/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0131 - accuracy: 0.9947 - val_loss: 1.3109 - val_accuracy: 0.8186\n",
      "Epoch 446/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 1.4036 - val_accuracy: 0.8235\n",
      "Epoch 447/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0060 - accuracy: 0.9989 - val_loss: 1.4093 - val_accuracy: 0.8284\n",
      "Epoch 448/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0057 - accuracy: 0.9979 - val_loss: 1.4633 - val_accuracy: 0.8186\n",
      "Epoch 449/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0035 - accuracy: 0.9989 - val_loss: 1.4688 - val_accuracy: 0.8284\n",
      "Epoch 450/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0037 - accuracy: 0.9989 - val_loss: 1.5873 - val_accuracy: 0.8186\n",
      "Epoch 451/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0583 - accuracy: 0.9851 - val_loss: 1.2547 - val_accuracy: 0.7990\n",
      "Epoch 452/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0360 - accuracy: 0.9833 - val_loss: 1.2081 - val_accuracy: 0.8039\n",
      "Epoch 453/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0238 - accuracy: 0.9893 - val_loss: 1.2164 - val_accuracy: 0.8284\n",
      "Epoch 454/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0142 - accuracy: 0.9936 - val_loss: 1.2862 - val_accuracy: 0.8235\n",
      "Epoch 455/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0093 - accuracy: 0.9968 - val_loss: 1.2683 - val_accuracy: 0.8333\n",
      "Epoch 456/500\n",
      "30/30 [==============================] - 1s 36ms/step - loss: 0.0132 - accuracy: 0.9957 - val_loss: 1.3424 - val_accuracy: 0.8186\n",
      "Epoch 457/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0215 - accuracy: 0.9915 - val_loss: 1.3466 - val_accuracy: 0.8333\n",
      "Epoch 458/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0153 - accuracy: 0.9957 - val_loss: 1.2924 - val_accuracy: 0.8186\n",
      "Epoch 459/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0128 - accuracy: 0.9947 - val_loss: 1.4120 - val_accuracy: 0.8186\n",
      "Epoch 460/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0077 - accuracy: 0.9968 - val_loss: 1.4975 - val_accuracy: 0.8186\n",
      "Epoch 461/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0289 - accuracy: 0.9904 - val_loss: 1.4211 - val_accuracy: 0.7990\n",
      "Epoch 462/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.0625 - accuracy: 0.9808 - val_loss: 1.2572 - val_accuracy: 0.7892\n",
      "Epoch 463/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0474 - accuracy: 0.9802 - val_loss: 1.2159 - val_accuracy: 0.8088\n",
      "Epoch 464/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0340 - accuracy: 0.9861 - val_loss: 1.3374 - val_accuracy: 0.8333\n",
      "Epoch 465/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0169 - accuracy: 0.9936 - val_loss: 1.3363 - val_accuracy: 0.8186\n",
      "Epoch 466/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0132 - accuracy: 0.9947 - val_loss: 1.3405 - val_accuracy: 0.8137\n",
      "Epoch 467/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0115 - accuracy: 0.9947 - val_loss: 1.4445 - val_accuracy: 0.8137\n",
      "Epoch 468/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0231 - accuracy: 0.9925 - val_loss: 1.2860 - val_accuracy: 0.8137\n",
      "Epoch 469/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0078 - accuracy: 0.9989 - val_loss: 1.3151 - val_accuracy: 0.8333\n",
      "Epoch 470/500\n",
      "30/30 [==============================] - 1s 39ms/step - loss: 0.0069 - accuracy: 0.9989 - val_loss: 1.3661 - val_accuracy: 0.8235\n",
      "Epoch 471/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0179 - accuracy: 0.9936 - val_loss: 1.4044 - val_accuracy: 0.8186\n",
      "Epoch 472/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0227 - accuracy: 0.9936 - val_loss: 1.3512 - val_accuracy: 0.8137\n",
      "Epoch 473/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0129 - accuracy: 0.9968 - val_loss: 1.4491 - val_accuracy: 0.8284\n",
      "Epoch 474/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0141 - accuracy: 0.9958 - val_loss: 1.5140 - val_accuracy: 0.8088\n",
      "Epoch 475/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0057 - accuracy: 0.9989 - val_loss: 1.4880 - val_accuracy: 0.8235\n",
      "Epoch 476/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0051 - accuracy: 0.9989 - val_loss: 1.4913 - val_accuracy: 0.8137\n",
      "Epoch 477/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.0068 - accuracy: 0.9968 - val_loss: 1.6139 - val_accuracy: 0.8186\n",
      "Epoch 478/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0184 - accuracy: 0.9915 - val_loss: 1.4191 - val_accuracy: 0.8284\n",
      "Epoch 479/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0171 - accuracy: 0.9957 - val_loss: 1.5122 - val_accuracy: 0.8186\n",
      "Epoch 480/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0252 - accuracy: 0.9925 - val_loss: 1.8353 - val_accuracy: 0.7843\n",
      "Epoch 481/500\n",
      "30/30 [==============================] - 1s 35ms/step - loss: 0.1965 - accuracy: 0.9594 - val_loss: 1.6692 - val_accuracy: 0.7206\n",
      "Epoch 482/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.8208 - accuracy: 0.8303 - val_loss: 1.7937 - val_accuracy: 0.7206\n",
      "Epoch 483/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.4487 - accuracy: 0.8943 - val_loss: 1.4292 - val_accuracy: 0.7255\n",
      "Epoch 484/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1971 - accuracy: 0.9466 - val_loss: 1.6927 - val_accuracy: 0.7647\n",
      "Epoch 485/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.3288 - accuracy: 0.9438 - val_loss: 1.5999 - val_accuracy: 0.7108\n",
      "Epoch 486/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.2456 - accuracy: 0.9242 - val_loss: 1.7810 - val_accuracy: 0.7549\n",
      "Epoch 487/500\n",
      "30/30 [==============================] - 1s 38ms/step - loss: 0.1273 - accuracy: 0.9669 - val_loss: 2.0626 - val_accuracy: 0.7794\n",
      "Epoch 488/500\n",
      "30/30 [==============================] - 1s 31ms/step - loss: 0.1443 - accuracy: 0.9626 - val_loss: 2.1532 - val_accuracy: 0.7451\n",
      "Epoch 489/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2333 - accuracy: 0.9274 - val_loss: 1.7549 - val_accuracy: 0.7500\n",
      "Epoch 490/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.2226 - accuracy: 0.9466 - val_loss: 1.8052 - val_accuracy: 0.7794\n",
      "Epoch 491/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1626 - accuracy: 0.9562 - val_loss: 2.0297 - val_accuracy: 0.7500\n",
      "Epoch 492/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.1040 - accuracy: 0.9712 - val_loss: 2.9928 - val_accuracy: 0.7745\n",
      "Epoch 493/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0510 - accuracy: 0.9829 - val_loss: 2.8378 - val_accuracy: 0.7892\n",
      "Epoch 494/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0302 - accuracy: 0.9915 - val_loss: 2.6418 - val_accuracy: 0.7794\n",
      "Epoch 495/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0147 - accuracy: 0.9936 - val_loss: 2.6282 - val_accuracy: 0.7892\n",
      "Epoch 496/500\n",
      "30/30 [==============================] - 1s 34ms/step - loss: 0.0163 - accuracy: 0.9969 - val_loss: 2.6164 - val_accuracy: 0.7843\n",
      "Epoch 497/500\n",
      "30/30 [==============================] - 1s 33ms/step - loss: 0.0147 - accuracy: 0.9925 - val_loss: 2.6795 - val_accuracy: 0.7843\n",
      "Epoch 498/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0248 - accuracy: 0.9925 - val_loss: 2.7501 - val_accuracy: 0.7598\n",
      "Epoch 499/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0161 - accuracy: 0.9957 - val_loss: 2.8533 - val_accuracy: 0.7843\n",
      "Epoch 500/500\n",
      "30/30 [==============================] - 1s 32ms/step - loss: 0.0274 - accuracy: 0.9936 - val_loss: 2.7190 - val_accuracy: 0.7843\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x282fa305370>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the Model\n",
    "model.fit(x_train, y_train, epochs=500, steps_per_epoch=30, batch_size=32, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "stainless-blond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved into model_SIBI.h5\n"
     ]
    }
   ],
   "source": [
    "#Saving the model into H5 system file\n",
    "save_model = \"model_SIBI.h5\"\n",
    "model.save(save_model)\n",
    "print(\"Model Saved into\", save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving the model for TF-Serving Type\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"server_model_SIBI\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    print('\\nAlready saved a model, cleaning up\\n')\n",
    "    !rm -r {export_path}\n",
    "\n",
    "model.save(export_path, save_format=\"tf\")\n",
    "\n",
    "print('\\nexport_path = {}'.format(export_path))\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "liable-respondent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 42, 1)\n",
      "[[[1103.327]\n",
      "  [1540.716]\n",
      "  [ 855.426]\n",
      "  [1411.774]\n",
      "  [ 683.308]\n",
      "  [1153.357]\n",
      "  [ 558.997]\n",
      "  [ 959.797]\n",
      "  [ 387.358]\n",
      "  [ 854.781]\n",
      "  [ 934.849]\n",
      "  [ 893.382]\n",
      "  [ 925.604]\n",
      "  [ 639.385]\n",
      "  [ 920.373]\n",
      "  [ 475.725]\n",
      "  [ 924.981]\n",
      "  [ 324.413]\n",
      "  [1102.588]\n",
      "  [ 946.422]\n",
      "  [1137.946]\n",
      "  [ 746.217]\n",
      "  [1086.862]\n",
      "  [1009.154]\n",
      "  [1076.11 ]\n",
      "  [1152.22 ]\n",
      "  [1254.827]\n",
      "  [1021.595]\n",
      "  [1289.853]\n",
      "  [ 855.331]\n",
      "  [1201.73 ]\n",
      "  [1101.462]\n",
      "  [1185.479]\n",
      "  [1216.182]\n",
      "  [1399.912]\n",
      "  [1116.804]\n",
      "  [1421.615]\n",
      "  [ 964.66 ]\n",
      "  [1333.269]\n",
      "  [1121.019]\n",
      "  [1310.715]\n",
      "  [1211.711]]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "input_test = [[[1103.32715511], [1540.71593285],\n",
    "               [ 855.42565584],[1411.77392006],[ 683.30776691],[1153.35738659],[ 558.99703503],[ 959.79690552],[ 387.35830784],[ 854.78103161],\n",
    "               [ 934.84920263],[ 893.38219166],[ 925.60398579],[ 639.38450813],[ 920.37254572],[ 475.72478652],[ 924.98147488],[ 324.41276312],\n",
    "               [1102.58769989],[ 946.42174244],[1137.94636726],[ 746.21725082],[1086.86184883],[1009.15443897],[1076.11000538],[1152.21977234],\n",
    "               [1254.82654572],[1021.59452438],[1289.85261917],[ 855.33124208],[1201.73048973],[1101.46188736],[1185.47868729],[1216.18199348],\n",
    "               [1399.91247654],[1116.8037653 ],[1421.61536217],[ 964.65975046],[1333.26935768],[1121.01900578],[1310.71531773],[1211.71069145]]]\n",
    "input_test = np.array(input_test)\n",
    "input = np.reshape(input_test, (input_test.shape[0], input_test.shape[1], 1))\n",
    "print(input_test.shape)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bronze-pizza",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "WARNING:tensorflow:From <ipython-input-16-ce52a49e73f2>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[11]\n"
     ]
    }
   ],
   "source": [
    "#Print the Prediction\n",
    "print(model.predict(input_test))\n",
    "print(model.predict_classes(input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "alone-blogger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L\n"
     ]
    }
   ],
   "source": [
    "classes = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6,\n",
    "    'H': 7,\n",
    "    'I': 8,\n",
    "    'J': 9,\n",
    "    'K': 10,\n",
    "    'L': 11,\n",
    "    'M': 12,\n",
    "    'N': 13,\n",
    "    'O': 14,\n",
    "    'P': 15,\n",
    "    'Q': 16,\n",
    "    'R': 17,\n",
    "    'S': 18,\n",
    "    'T': 19,\n",
    "    'U': 20,\n",
    "    'V': 21,\n",
    "    'W': 22,\n",
    "    'X': 23,\n",
    "    'Y': 24,\n",
    "    'Z': 25\n",
    "}\n",
    "\n",
    "predictions = model.predict_classes(input_test)\n",
    "for alphabets, values in classes.items():\n",
    "    if values == predictions[0] :\n",
    "        print(alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technical-criticism",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
