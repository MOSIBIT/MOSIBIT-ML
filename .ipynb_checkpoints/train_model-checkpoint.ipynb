{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "subtle-blood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the much needed stuff for training\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Make numpy values easier to read.\n",
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "specific-argument",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>wristZ</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_CmcZ</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_McpZ</th>\n",
       "      <th>...</th>\n",
       "      <th>pinky_McpZ</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_PipZ</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_DipZ</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "      <th>pinky_TipZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>633.079380</td>\n",
       "      <td>1031.794727</td>\n",
       "      <td>-1.029254e-06</td>\n",
       "      <td>552.742794</td>\n",
       "      <td>1014.596709</td>\n",
       "      <td>-0.016872</td>\n",
       "      <td>488.761693</td>\n",
       "      <td>950.070965</td>\n",
       "      <td>-0.015831</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014530</td>\n",
       "      <td>625.414029</td>\n",
       "      <td>841.904812</td>\n",
       "      <td>-0.030691</td>\n",
       "      <td>621.901855</td>\n",
       "      <td>885.472149</td>\n",
       "      <td>-0.008086</td>\n",
       "      <td>634.784594</td>\n",
       "      <td>893.365762</td>\n",
       "      <td>0.015983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>1591.784213</td>\n",
       "      <td>2099.707924</td>\n",
       "      <td>-1.857942e-06</td>\n",
       "      <td>1318.607858</td>\n",
       "      <td>1917.100941</td>\n",
       "      <td>-0.034396</td>\n",
       "      <td>1120.267701</td>\n",
       "      <td>1682.855012</td>\n",
       "      <td>-0.059530</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074243</td>\n",
       "      <td>1683.593624</td>\n",
       "      <td>1443.143311</td>\n",
       "      <td>-0.114158</td>\n",
       "      <td>1637.900345</td>\n",
       "      <td>1626.554544</td>\n",
       "      <td>-0.080055</td>\n",
       "      <td>1703.259765</td>\n",
       "      <td>1651.284102</td>\n",
       "      <td>-0.043401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>1583.454045</td>\n",
       "      <td>1993.154838</td>\n",
       "      <td>-1.932337e-06</td>\n",
       "      <td>1328.071508</td>\n",
       "      <td>1836.134789</td>\n",
       "      <td>-0.023637</td>\n",
       "      <td>1136.303083</td>\n",
       "      <td>1590.329483</td>\n",
       "      <td>-0.039298</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.075024</td>\n",
       "      <td>1680.113028</td>\n",
       "      <td>1334.217393</td>\n",
       "      <td>-0.118665</td>\n",
       "      <td>1636.022493</td>\n",
       "      <td>1532.326854</td>\n",
       "      <td>-0.082507</td>\n",
       "      <td>1701.786802</td>\n",
       "      <td>1574.371294</td>\n",
       "      <td>-0.043058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>1593.135340</td>\n",
       "      <td>2003.049824</td>\n",
       "      <td>-1.917325e-06</td>\n",
       "      <td>1329.626807</td>\n",
       "      <td>1846.083207</td>\n",
       "      <td>-0.025695</td>\n",
       "      <td>1135.570690</td>\n",
       "      <td>1601.084763</td>\n",
       "      <td>-0.042078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.071267</td>\n",
       "      <td>1678.341120</td>\n",
       "      <td>1337.785852</td>\n",
       "      <td>-0.110229</td>\n",
       "      <td>1631.677766</td>\n",
       "      <td>1530.720809</td>\n",
       "      <td>-0.074036</td>\n",
       "      <td>1692.048390</td>\n",
       "      <td>1576.065932</td>\n",
       "      <td>-0.036066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>1615.659772</td>\n",
       "      <td>1899.973072</td>\n",
       "      <td>-1.697942e-06</td>\n",
       "      <td>1366.676348</td>\n",
       "      <td>1755.681619</td>\n",
       "      <td>-0.028110</td>\n",
       "      <td>1173.298271</td>\n",
       "      <td>1514.212327</td>\n",
       "      <td>-0.040185</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.053522</td>\n",
       "      <td>1706.582711</td>\n",
       "      <td>1260.215963</td>\n",
       "      <td>-0.096525</td>\n",
       "      <td>1669.381164</td>\n",
       "      <td>1456.343893</td>\n",
       "      <td>-0.064412</td>\n",
       "      <td>1727.129797</td>\n",
       "      <td>1498.052231</td>\n",
       "      <td>-0.027742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1051</th>\n",
       "      <td>Z</td>\n",
       "      <td>472.966313</td>\n",
       "      <td>1540.549994</td>\n",
       "      <td>1.156611e-06</td>\n",
       "      <td>522.023559</td>\n",
       "      <td>1514.556885</td>\n",
       "      <td>-0.276131</td>\n",
       "      <td>711.459756</td>\n",
       "      <td>1421.978235</td>\n",
       "      <td>-0.379206</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.041158</td>\n",
       "      <td>1253.675699</td>\n",
       "      <td>1319.325686</td>\n",
       "      <td>-0.154479</td>\n",
       "      <td>1103.553176</td>\n",
       "      <td>1493.587255</td>\n",
       "      <td>-0.163301</td>\n",
       "      <td>963.837624</td>\n",
       "      <td>1532.385945</td>\n",
       "      <td>-0.143554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1052</th>\n",
       "      <td>Z</td>\n",
       "      <td>511.085093</td>\n",
       "      <td>1538.953304</td>\n",
       "      <td>1.230672e-06</td>\n",
       "      <td>485.632837</td>\n",
       "      <td>1530.085564</td>\n",
       "      <td>-0.252264</td>\n",
       "      <td>638.235927</td>\n",
       "      <td>1447.455406</td>\n",
       "      <td>-0.371612</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.123394</td>\n",
       "      <td>1241.890192</td>\n",
       "      <td>1443.705082</td>\n",
       "      <td>-0.246548</td>\n",
       "      <td>1078.716993</td>\n",
       "      <td>1598.790646</td>\n",
       "      <td>-0.245476</td>\n",
       "      <td>946.605921</td>\n",
       "      <td>1613.921404</td>\n",
       "      <td>-0.219498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1053</th>\n",
       "      <td>Z</td>\n",
       "      <td>289.586902</td>\n",
       "      <td>1389.938593</td>\n",
       "      <td>1.943169e-07</td>\n",
       "      <td>592.706800</td>\n",
       "      <td>1311.617374</td>\n",
       "      <td>-0.152483</td>\n",
       "      <td>900.285482</td>\n",
       "      <td>1261.919737</td>\n",
       "      <td>-0.125441</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223034</td>\n",
       "      <td>1054.138541</td>\n",
       "      <td>1293.740630</td>\n",
       "      <td>0.159143</td>\n",
       "      <td>914.128363</td>\n",
       "      <td>1380.347967</td>\n",
       "      <td>0.114000</td>\n",
       "      <td>786.959291</td>\n",
       "      <td>1369.079351</td>\n",
       "      <td>0.106502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1054</th>\n",
       "      <td>Z</td>\n",
       "      <td>265.467346</td>\n",
       "      <td>1484.784365</td>\n",
       "      <td>2.655130e-07</td>\n",
       "      <td>447.429001</td>\n",
       "      <td>1383.484006</td>\n",
       "      <td>-0.221616</td>\n",
       "      <td>732.457399</td>\n",
       "      <td>1315.951228</td>\n",
       "      <td>-0.233599</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175156</td>\n",
       "      <td>1110.132456</td>\n",
       "      <td>1363.412261</td>\n",
       "      <td>0.080964</td>\n",
       "      <td>984.978139</td>\n",
       "      <td>1486.548662</td>\n",
       "      <td>0.038428</td>\n",
       "      <td>876.057625</td>\n",
       "      <td>1501.790762</td>\n",
       "      <td>0.041642</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1055</th>\n",
       "      <td>Z</td>\n",
       "      <td>413.485765</td>\n",
       "      <td>1470.117807</td>\n",
       "      <td>2.890906e-07</td>\n",
       "      <td>521.474183</td>\n",
       "      <td>1376.033545</td>\n",
       "      <td>-0.238790</td>\n",
       "      <td>766.105592</td>\n",
       "      <td>1297.483921</td>\n",
       "      <td>-0.279074</td>\n",
       "      <td>...</td>\n",
       "      <td>0.094886</td>\n",
       "      <td>1289.921880</td>\n",
       "      <td>1314.355969</td>\n",
       "      <td>-0.003062</td>\n",
       "      <td>1141.063213</td>\n",
       "      <td>1440.321207</td>\n",
       "      <td>-0.032022</td>\n",
       "      <td>1006.802082</td>\n",
       "      <td>1456.267476</td>\n",
       "      <td>-0.022182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1056 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     class_type       wristX       wristY        wristZ   thumb_CmcX  \\\n",
       "0             A   633.079380  1031.794727 -1.029254e-06   552.742794   \n",
       "1             A  1591.784213  2099.707924 -1.857942e-06  1318.607858   \n",
       "2             A  1583.454045  1993.154838 -1.932337e-06  1328.071508   \n",
       "3             A  1593.135340  2003.049824 -1.917325e-06  1329.626807   \n",
       "4             A  1615.659772  1899.973072 -1.697942e-06  1366.676348   \n",
       "...         ...          ...          ...           ...          ...   \n",
       "1051          Z   472.966313  1540.549994  1.156611e-06   522.023559   \n",
       "1052          Z   511.085093  1538.953304  1.230672e-06   485.632837   \n",
       "1053          Z   289.586902  1389.938593  1.943169e-07   592.706800   \n",
       "1054          Z   265.467346  1484.784365  2.655130e-07   447.429001   \n",
       "1055          Z   413.485765  1470.117807  2.890906e-07   521.474183   \n",
       "\n",
       "       thumb_CmcY  thumb_CmcZ   thumb_McpX   thumb_McpY  thumb_McpZ  ...  \\\n",
       "0     1014.596709   -0.016872   488.761693   950.070965   -0.015831  ...   \n",
       "1     1917.100941   -0.034396  1120.267701  1682.855012   -0.059530  ...   \n",
       "2     1836.134789   -0.023637  1136.303083  1590.329483   -0.039298  ...   \n",
       "3     1846.083207   -0.025695  1135.570690  1601.084763   -0.042078  ...   \n",
       "4     1755.681619   -0.028110  1173.298271  1514.212327   -0.040185  ...   \n",
       "...           ...         ...          ...          ...         ...  ...   \n",
       "1051  1514.556885   -0.276131   711.459756  1421.978235   -0.379206  ...   \n",
       "1052  1530.085564   -0.252264   638.235927  1447.455406   -0.371612  ...   \n",
       "1053  1311.617374   -0.152483   900.285482  1261.919737   -0.125441  ...   \n",
       "1054  1383.484006   -0.221616   732.457399  1315.951228   -0.233599  ...   \n",
       "1055  1376.033545   -0.238790   766.105592  1297.483921   -0.279074  ...   \n",
       "\n",
       "      pinky_McpZ   pinky_PipX   pinky_PipY  pinky_PipZ   pinky_DipX  \\\n",
       "0      -0.014530   625.414029   841.904812   -0.030691   621.901855   \n",
       "1      -0.074243  1683.593624  1443.143311   -0.114158  1637.900345   \n",
       "2      -0.075024  1680.113028  1334.217393   -0.118665  1636.022493   \n",
       "3      -0.071267  1678.341120  1337.785852   -0.110229  1631.677766   \n",
       "4      -0.053522  1706.582711  1260.215963   -0.096525  1669.381164   \n",
       "...          ...          ...          ...         ...          ...   \n",
       "1051   -0.041158  1253.675699  1319.325686   -0.154479  1103.553176   \n",
       "1052   -0.123394  1241.890192  1443.705082   -0.246548  1078.716993   \n",
       "1053    0.223034  1054.138541  1293.740630    0.159143   914.128363   \n",
       "1054    0.175156  1110.132456  1363.412261    0.080964   984.978139   \n",
       "1055    0.094886  1289.921880  1314.355969   -0.003062  1141.063213   \n",
       "\n",
       "       pinky_DipY  pinky_DipZ   pinky_TipX   pinky_TipY  pinky_TipZ  \n",
       "0      885.472149   -0.008086   634.784594   893.365762    0.015983  \n",
       "1     1626.554544   -0.080055  1703.259765  1651.284102   -0.043401  \n",
       "2     1532.326854   -0.082507  1701.786802  1574.371294   -0.043058  \n",
       "3     1530.720809   -0.074036  1692.048390  1576.065932   -0.036066  \n",
       "4     1456.343893   -0.064412  1727.129797  1498.052231   -0.027742  \n",
       "...           ...         ...          ...          ...         ...  \n",
       "1051  1493.587255   -0.163301   963.837624  1532.385945   -0.143554  \n",
       "1052  1598.790646   -0.245476   946.605921  1613.921404   -0.219498  \n",
       "1053  1380.347967    0.114000   786.959291  1369.079351    0.106502  \n",
       "1054  1486.548662    0.038428   876.057625  1501.790762    0.041642  \n",
       "1055  1440.321207   -0.032022  1006.802082  1456.267476   -0.022182  \n",
       "\n",
       "[1056 rows x 64 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Training the model using Pandas\n",
    "df_train = pd.read_csv(\"hands_SIBI_training.csv\", header=0)\n",
    "\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "constitutional-steam",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>class_type</th>\n",
       "      <th>wristX</th>\n",
       "      <th>wristY</th>\n",
       "      <th>wristZ</th>\n",
       "      <th>thumb_CmcX</th>\n",
       "      <th>thumb_CmcY</th>\n",
       "      <th>thumb_CmcZ</th>\n",
       "      <th>thumb_McpX</th>\n",
       "      <th>thumb_McpY</th>\n",
       "      <th>thumb_McpZ</th>\n",
       "      <th>...</th>\n",
       "      <th>pinky_McpZ</th>\n",
       "      <th>pinky_PipX</th>\n",
       "      <th>pinky_PipY</th>\n",
       "      <th>pinky_PipZ</th>\n",
       "      <th>pinky_DipX</th>\n",
       "      <th>pinky_DipY</th>\n",
       "      <th>pinky_DipZ</th>\n",
       "      <th>pinky_TipX</th>\n",
       "      <th>pinky_TipY</th>\n",
       "      <th>pinky_TipZ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>1610.007216</td>\n",
       "      <td>1784.244035</td>\n",
       "      <td>-1.749235e-06</td>\n",
       "      <td>1369.551107</td>\n",
       "      <td>1709.763939</td>\n",
       "      <td>-0.042585</td>\n",
       "      <td>1148.568022</td>\n",
       "      <td>1496.684943</td>\n",
       "      <td>-0.062479</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.052158</td>\n",
       "      <td>1657.078684</td>\n",
       "      <td>1124.223124</td>\n",
       "      <td>-0.091841</td>\n",
       "      <td>1640.837864</td>\n",
       "      <td>1317.483665</td>\n",
       "      <td>-0.057763</td>\n",
       "      <td>1681.879869</td>\n",
       "      <td>1367.191519</td>\n",
       "      <td>-0.020736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>721.901596</td>\n",
       "      <td>1655.649900</td>\n",
       "      <td>-1.420789e-06</td>\n",
       "      <td>530.846596</td>\n",
       "      <td>1427.740693</td>\n",
       "      <td>-0.034975</td>\n",
       "      <td>442.550361</td>\n",
       "      <td>1126.330376</td>\n",
       "      <td>-0.054130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051886</td>\n",
       "      <td>1116.067886</td>\n",
       "      <td>1094.982624</td>\n",
       "      <td>-0.091441</td>\n",
       "      <td>1040.838242</td>\n",
       "      <td>1238.941789</td>\n",
       "      <td>-0.066161</td>\n",
       "      <td>998.332918</td>\n",
       "      <td>1323.050618</td>\n",
       "      <td>-0.035342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>749.511778</td>\n",
       "      <td>1684.732795</td>\n",
       "      <td>-1.402203e-06</td>\n",
       "      <td>563.877583</td>\n",
       "      <td>1453.747034</td>\n",
       "      <td>-0.036049</td>\n",
       "      <td>486.337095</td>\n",
       "      <td>1149.076819</td>\n",
       "      <td>-0.054778</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.049362</td>\n",
       "      <td>1145.679951</td>\n",
       "      <td>1128.144860</td>\n",
       "      <td>-0.089420</td>\n",
       "      <td>1065.986991</td>\n",
       "      <td>1271.077037</td>\n",
       "      <td>-0.065419</td>\n",
       "      <td>1017.943382</td>\n",
       "      <td>1357.578039</td>\n",
       "      <td>-0.034874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>626.763403</td>\n",
       "      <td>1036.431742</td>\n",
       "      <td>-1.010617e-06</td>\n",
       "      <td>557.352796</td>\n",
       "      <td>1013.014567</td>\n",
       "      <td>-0.019210</td>\n",
       "      <td>502.974451</td>\n",
       "      <td>940.376868</td>\n",
       "      <td>-0.019415</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.026157</td>\n",
       "      <td>646.708667</td>\n",
       "      <td>850.894585</td>\n",
       "      <td>-0.045292</td>\n",
       "      <td>638.892889</td>\n",
       "      <td>895.978625</td>\n",
       "      <td>-0.022630</td>\n",
       "      <td>649.126634</td>\n",
       "      <td>910.227203</td>\n",
       "      <td>0.002203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>303.739804</td>\n",
       "      <td>3094.464386</td>\n",
       "      <td>2.659254e-07</td>\n",
       "      <td>373.745911</td>\n",
       "      <td>3178.373840</td>\n",
       "      <td>-0.009913</td>\n",
       "      <td>384.860658</td>\n",
       "      <td>3259.081604</td>\n",
       "      <td>-0.005684</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074704</td>\n",
       "      <td>270.859761</td>\n",
       "      <td>3269.004730</td>\n",
       "      <td>0.101987</td>\n",
       "      <td>289.082896</td>\n",
       "      <td>3289.683472</td>\n",
       "      <td>0.118750</td>\n",
       "      <td>303.885542</td>\n",
       "      <td>3296.251648</td>\n",
       "      <td>0.130978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>Z</td>\n",
       "      <td>225.790128</td>\n",
       "      <td>1833.904896</td>\n",
       "      <td>-2.789371e-08</td>\n",
       "      <td>222.669122</td>\n",
       "      <td>1967.997986</td>\n",
       "      <td>-0.246023</td>\n",
       "      <td>430.404808</td>\n",
       "      <td>2049.844986</td>\n",
       "      <td>-0.340677</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046414</td>\n",
       "      <td>1051.457326</td>\n",
       "      <td>1762.525051</td>\n",
       "      <td>-0.136801</td>\n",
       "      <td>910.432521</td>\n",
       "      <td>1864.261826</td>\n",
       "      <td>-0.133323</td>\n",
       "      <td>791.947807</td>\n",
       "      <td>1792.989178</td>\n",
       "      <td>-0.108027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>Z</td>\n",
       "      <td>2572.288327</td>\n",
       "      <td>2967.599804</td>\n",
       "      <td>-1.137866e-07</td>\n",
       "      <td>2394.056368</td>\n",
       "      <td>2864.046263</td>\n",
       "      <td>-0.004316</td>\n",
       "      <td>2233.504243</td>\n",
       "      <td>2798.140757</td>\n",
       "      <td>-0.023914</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.079279</td>\n",
       "      <td>2248.028226</td>\n",
       "      <td>2977.749338</td>\n",
       "      <td>-0.090497</td>\n",
       "      <td>2285.416975</td>\n",
       "      <td>3009.534792</td>\n",
       "      <td>-0.083654</td>\n",
       "      <td>2350.825109</td>\n",
       "      <td>2980.246215</td>\n",
       "      <td>-0.078822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>Z</td>\n",
       "      <td>1742.245886</td>\n",
       "      <td>2565.417858</td>\n",
       "      <td>2.051919e-07</td>\n",
       "      <td>1691.130518</td>\n",
       "      <td>2520.568737</td>\n",
       "      <td>-0.055253</td>\n",
       "      <td>1672.879678</td>\n",
       "      <td>2425.837498</td>\n",
       "      <td>-0.085446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.039117</td>\n",
       "      <td>1978.539119</td>\n",
       "      <td>2383.158772</td>\n",
       "      <td>-0.081821</td>\n",
       "      <td>1975.439507</td>\n",
       "      <td>2466.163536</td>\n",
       "      <td>-0.087077</td>\n",
       "      <td>1954.306755</td>\n",
       "      <td>2521.870468</td>\n",
       "      <td>-0.081235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>Z</td>\n",
       "      <td>607.798517</td>\n",
       "      <td>1321.071744</td>\n",
       "      <td>7.056732e-07</td>\n",
       "      <td>662.470937</td>\n",
       "      <td>1287.229657</td>\n",
       "      <td>-0.190705</td>\n",
       "      <td>824.859679</td>\n",
       "      <td>1257.505655</td>\n",
       "      <td>-0.242309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023399</td>\n",
       "      <td>1208.418965</td>\n",
       "      <td>1214.795113</td>\n",
       "      <td>-0.046823</td>\n",
       "      <td>1095.283389</td>\n",
       "      <td>1289.289713</td>\n",
       "      <td>-0.056086</td>\n",
       "      <td>1015.160561</td>\n",
       "      <td>1262.510061</td>\n",
       "      <td>-0.042613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Z</td>\n",
       "      <td>687.228024</td>\n",
       "      <td>1365.019202</td>\n",
       "      <td>6.465051e-07</td>\n",
       "      <td>734.231889</td>\n",
       "      <td>1342.226267</td>\n",
       "      <td>-0.180457</td>\n",
       "      <td>910.643280</td>\n",
       "      <td>1326.208472</td>\n",
       "      <td>-0.231182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.019234</td>\n",
       "      <td>1308.938861</td>\n",
       "      <td>1289.016008</td>\n",
       "      <td>-0.054561</td>\n",
       "      <td>1201.678514</td>\n",
       "      <td>1363.672256</td>\n",
       "      <td>-0.063278</td>\n",
       "      <td>1112.614751</td>\n",
       "      <td>1330.185056</td>\n",
       "      <td>-0.049138</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>212 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    class_type       wristX       wristY        wristZ   thumb_CmcX  \\\n",
       "0            A  1610.007216  1784.244035 -1.749235e-06  1369.551107   \n",
       "1            A   721.901596  1655.649900 -1.420789e-06   530.846596   \n",
       "2            A   749.511778  1684.732795 -1.402203e-06   563.877583   \n",
       "3            A   626.763403  1036.431742 -1.010617e-06   557.352796   \n",
       "4            A   303.739804  3094.464386  2.659254e-07   373.745911   \n",
       "..         ...          ...          ...           ...          ...   \n",
       "207          Z   225.790128  1833.904896 -2.789371e-08   222.669122   \n",
       "208          Z  2572.288327  2967.599804 -1.137866e-07  2394.056368   \n",
       "209          Z  1742.245886  2565.417858  2.051919e-07  1691.130518   \n",
       "210          Z   607.798517  1321.071744  7.056732e-07   662.470937   \n",
       "211          Z   687.228024  1365.019202  6.465051e-07   734.231889   \n",
       "\n",
       "      thumb_CmcY  thumb_CmcZ   thumb_McpX   thumb_McpY  thumb_McpZ  ...  \\\n",
       "0    1709.763939   -0.042585  1148.568022  1496.684943   -0.062479  ...   \n",
       "1    1427.740693   -0.034975   442.550361  1126.330376   -0.054130  ...   \n",
       "2    1453.747034   -0.036049   486.337095  1149.076819   -0.054778  ...   \n",
       "3    1013.014567   -0.019210   502.974451   940.376868   -0.019415  ...   \n",
       "4    3178.373840   -0.009913   384.860658  3259.081604   -0.005684  ...   \n",
       "..           ...         ...          ...          ...         ...  ...   \n",
       "207  1967.997986   -0.246023   430.404808  2049.844986   -0.340677  ...   \n",
       "208  2864.046263   -0.004316  2233.504243  2798.140757   -0.023914  ...   \n",
       "209  2520.568737   -0.055253  1672.879678  2425.837498   -0.085446  ...   \n",
       "210  1287.229657   -0.190705   824.859679  1257.505655   -0.242309  ...   \n",
       "211  1342.226267   -0.180457   910.643280  1326.208472   -0.231182  ...   \n",
       "\n",
       "     pinky_McpZ   pinky_PipX   pinky_PipY  pinky_PipZ   pinky_DipX  \\\n",
       "0     -0.052158  1657.078684  1124.223124   -0.091841  1640.837864   \n",
       "1     -0.051886  1116.067886  1094.982624   -0.091441  1040.838242   \n",
       "2     -0.049362  1145.679951  1128.144860   -0.089420  1065.986991   \n",
       "3     -0.026157   646.708667   850.894585   -0.045292   638.892889   \n",
       "4      0.074704   270.859761  3269.004730    0.101987   289.082896   \n",
       "..          ...          ...          ...         ...          ...   \n",
       "207   -0.046414  1051.457326  1762.525051   -0.136801   910.432521   \n",
       "208   -0.079279  2248.028226  2977.749338   -0.090497  2285.416975   \n",
       "209   -0.039117  1978.539119  2383.158772   -0.081821  1975.439507   \n",
       "210    0.023399  1208.418965  1214.795113   -0.046823  1095.283389   \n",
       "211    0.019234  1308.938861  1289.016008   -0.054561  1201.678514   \n",
       "\n",
       "      pinky_DipY  pinky_DipZ   pinky_TipX   pinky_TipY  pinky_TipZ  \n",
       "0    1317.483665   -0.057763  1681.879869  1367.191519   -0.020736  \n",
       "1    1238.941789   -0.066161   998.332918  1323.050618   -0.035342  \n",
       "2    1271.077037   -0.065419  1017.943382  1357.578039   -0.034874  \n",
       "3     895.978625   -0.022630   649.126634   910.227203    0.002203  \n",
       "4    3289.683472    0.118750   303.885542  3296.251648    0.130978  \n",
       "..           ...         ...          ...          ...         ...  \n",
       "207  1864.261826   -0.133323   791.947807  1792.989178   -0.108027  \n",
       "208  3009.534792   -0.083654  2350.825109  2980.246215   -0.078822  \n",
       "209  2466.163536   -0.087077  1954.306755  2521.870468   -0.081235  \n",
       "210  1289.289713   -0.056086  1015.160561  1262.510061   -0.042613  \n",
       "211  1363.672256   -0.063278  1112.614751  1330.185056   -0.049138  \n",
       "\n",
       "[212 rows x 64 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read CSV file for Validation or Testing the Model using Pandas\n",
    "df_test = pd.read_csv(\"hands_SIBI_validation.csv\", header=0)\n",
    "\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "nasty-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put Categorical using Pandas\n",
    "df_train[\"class_type\"] = pd.Categorical(df_train[\"class_type\"])\n",
    "df_train[\"class_type\"] = df_train.class_type.cat.codes\n",
    "\n",
    "df_test[\"class_type\"] = pd.Categorical(df_test[\"class_type\"])\n",
    "df_test[\"class_type\"] = df_test.class_type.cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "indirect-dialogue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy Label and Feature for training\n",
    "y_train = df_train.pop(\"class_type\")\n",
    "x_train = df_train.copy()\n",
    "\n",
    "y_test = df_test.pop(\"class_type\")\n",
    "x_test = df_test.copy()\n",
    "\n",
    "# Copied Features turn to Array by using NumPy\n",
    "x_train = np.array(x_train)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Normalize the Datasets\n",
    "x_train, x_test = x_train/255.0, x_test/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "phantom-shower",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1056, 63)\n",
      "(212, 63)\n",
      "(1056, 63, 1)\n",
      "(212, 63, 1)\n"
     ]
    }
   ],
   "source": [
    "# Check Array Shape before transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "\n",
    "# Since the array shape is 1x1, we must turn it into 1x10x1 so we can feed it into the model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Check Array Shape after transformation\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "significant-worse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.483]\n",
      " [ 4.046]\n",
      " [-0.   ]\n",
      " [ 2.168]\n",
      " [ 3.979]\n",
      " [-0.   ]\n",
      " [ 1.917]\n",
      " [ 3.726]\n",
      " [-0.   ]\n",
      " [ 1.785]\n",
      " [ 3.51 ]\n",
      " [-0.   ]\n",
      " [ 1.706]\n",
      " [ 3.333]\n",
      " [-0.   ]\n",
      " [ 2.006]\n",
      " [ 3.451]\n",
      " [ 0.   ]\n",
      " [ 1.888]\n",
      " [ 3.332]\n",
      " [-0.   ]\n",
      " [ 1.971]\n",
      " [ 3.553]\n",
      " [-0.   ]\n",
      " [ 2.048]\n",
      " [ 3.638]\n",
      " [-0.   ]\n",
      " [ 2.176]\n",
      " [ 3.398]\n",
      " [ 0.   ]\n",
      " [ 2.051]\n",
      " [ 3.301]\n",
      " [-0.   ]\n",
      " [ 2.123]\n",
      " [ 3.565]\n",
      " [-0.   ]\n",
      " [ 2.194]\n",
      " [ 3.584]\n",
      " [-0.   ]\n",
      " [ 2.364]\n",
      " [ 3.379]\n",
      " [ 0.   ]\n",
      " [ 2.242]\n",
      " [ 3.276]\n",
      " [-0.   ]\n",
      " [ 2.281]\n",
      " [ 3.527]\n",
      " [-0.   ]\n",
      " [ 2.352]\n",
      " [ 3.554]\n",
      " [ 0.   ]\n",
      " [ 2.559]\n",
      " [ 3.385]\n",
      " [-0.   ]\n",
      " [ 2.453]\n",
      " [ 3.302]\n",
      " [-0.   ]\n",
      " [ 2.439]\n",
      " [ 3.472]\n",
      " [-0.   ]\n",
      " [ 2.489]\n",
      " [ 3.503]\n",
      " [ 0.   ]]\n",
      "[[11.362]\n",
      " [ 8.714]\n",
      " [-0.   ]\n",
      " [10.547]\n",
      " [ 7.91 ]\n",
      " [-0.   ]\n",
      " [10.144]\n",
      " [ 7.111]\n",
      " [ 0.   ]\n",
      " [10.096]\n",
      " [ 6.262]\n",
      " [ 0.   ]\n",
      " [10.108]\n",
      " [ 5.641]\n",
      " [ 0.   ]\n",
      " [10.686]\n",
      " [ 6.474]\n",
      " [ 0.   ]\n",
      " [10.615]\n",
      " [ 5.83 ]\n",
      " [ 0.   ]\n",
      " [10.546]\n",
      " [ 6.443]\n",
      " [ 0.   ]\n",
      " [10.569]\n",
      " [ 6.712]\n",
      " [ 0.   ]\n",
      " [11.125]\n",
      " [ 6.564]\n",
      " [ 0.   ]\n",
      " [11.013]\n",
      " [ 5.979]\n",
      " [ 0.   ]\n",
      " [10.872]\n",
      " [ 6.715]\n",
      " [ 0.   ]\n",
      " [10.898]\n",
      " [ 6.891]\n",
      " [ 0.   ]\n",
      " [11.596]\n",
      " [ 6.668]\n",
      " [ 0.   ]\n",
      " [11.497]\n",
      " [ 6.144]\n",
      " [ 0.   ]\n",
      " [11.266]\n",
      " [ 6.858]\n",
      " [ 0.   ]\n",
      " [11.269]\n",
      " [ 7.077]\n",
      " [ 0.   ]\n",
      " [12.082]\n",
      " [ 6.828]\n",
      " [ 0.   ]\n",
      " [11.994]\n",
      " [ 6.333]\n",
      " [ 0.   ]\n",
      " [11.708]\n",
      " [ 6.801]\n",
      " [ 0.   ]\n",
      " [11.649]\n",
      " [ 7.046]\n",
      " [ 0.   ]]\n"
     ]
    }
   ],
   "source": [
    "# Check sample train and test features\n",
    "print(x_train[0])\n",
    "print(x_test[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "entire-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of classes according standard Alphabets\n",
    "num_classes = 26\n",
    "\n",
    "# Using the Keras.Utils to put the label categorically \n",
    "y_train = to_categorical(y_train, num_classes)\n",
    "y_test = to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "hungarian-general",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_16 (Conv1D)           (None, 63, 32)            192       \n",
      "_________________________________________________________________\n",
      "conv1d_17 (Conv1D)           (None, 63, 32)            5152      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1 (None, 31, 32)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_18 (Conv1D)           (None, 31, 64)            10304     \n",
      "_________________________________________________________________\n",
      "conv1d_19 (Conv1D)           (None, 31, 64)            20544     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 15, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_20 (Conv1D)           (None, 15, 128)           41088     \n",
      "_________________________________________________________________\n",
      "conv1d_21 (Conv1D)           (None, 15, 128)           82048     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_22 (Conv1D)           (None, 7, 256)            164096    \n",
      "_________________________________________________________________\n",
      "conv1d_23 (Conv1D)           (None, 7, 256)            327936    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 3, 256)            0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 768)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 512)               393728    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 26)                13338     \n",
      "=================================================================\n",
      "Total params: 1,058,426\n",
      "Trainable params: 1,058,426\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# One Dimensional Convolutional Neural Network model, Train will be feed to 1 Dimension Convolutional Neural Network\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\", input_shape=x_train.shape[1:3]),\n",
    "    tf.keras.layers.Conv1D(filters=32, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=64, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=128, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.Conv1D(filters=256, kernel_size=5, strides=1, padding=\"causal\", activation=\"relu\"),\n",
    "    tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    # Flatten the results to feed into a DNN\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # 512 neuron hidden layer\n",
    "    tf.keras.layers.Dense(512, activation='relu'), \n",
    "    tf.keras.layers.Dense(num_classes, activation='softmax')])\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "smoking-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs={}):\n",
    "    if(logs.get('val_accuracy')>0.95):\n",
    "      print(\"\\nReached 95% accuracy so cancelling training!\")\n",
    "      self.model.stop_training = True\n",
    "\n",
    "callbacks = myCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "occupational-container",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      " 2/33 [>.............................] - ETA: 1s - loss: 0.3486 - accuracy: 0.8750WARNING:tensorflow:Callbacks method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0150s vs `on_train_batch_end` time: 0.0700s). Check your callbacks.\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.3963 - accuracy: 0.8466 - val_loss: 0.5400 - val_accuracy: 0.7877\n",
      "Epoch 2/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.3924 - accuracy: 0.8523 - val_loss: 0.6510 - val_accuracy: 0.7830\n",
      "Epoch 3/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.4029 - accuracy: 0.8409 - val_loss: 0.5981 - val_accuracy: 0.7689\n",
      "Epoch 4/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.3995 - accuracy: 0.8428 - val_loss: 0.5220 - val_accuracy: 0.8255\n",
      "Epoch 5/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2842 - accuracy: 0.8968 - val_loss: 0.4576 - val_accuracy: 0.8302\n",
      "Epoch 6/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2803 - accuracy: 0.9034 - val_loss: 0.3843 - val_accuracy: 0.8726\n",
      "Epoch 7/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.3315 - accuracy: 0.8873 - val_loss: 0.5644 - val_accuracy: 0.8208\n",
      "Epoch 8/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2416 - accuracy: 0.9110 - val_loss: 0.4243 - val_accuracy: 0.8868\n",
      "Epoch 9/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1983 - accuracy: 0.9252 - val_loss: 0.5302 - val_accuracy: 0.8255\n",
      "Epoch 10/1000\n",
      "33/33 [==============================] - 1s 39ms/step - loss: 0.2633 - accuracy: 0.8930 - val_loss: 0.4235 - val_accuracy: 0.8868\n",
      "Epoch 11/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2307 - accuracy: 0.9186 - val_loss: 0.5287 - val_accuracy: 0.7972\n",
      "Epoch 12/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2237 - accuracy: 0.9157 - val_loss: 0.4218 - val_accuracy: 0.8443\n",
      "Epoch 13/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2316 - accuracy: 0.9148 - val_loss: 0.4173 - val_accuracy: 0.8443\n",
      "Epoch 14/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.3598 - accuracy: 0.8674 - val_loss: 0.5310 - val_accuracy: 0.8255\n",
      "Epoch 15/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2836 - accuracy: 0.8977 - val_loss: 0.3524 - val_accuracy: 0.8915\n",
      "Epoch 16/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.2323 - accuracy: 0.9186 - val_loss: 0.4295 - val_accuracy: 0.8868\n",
      "Epoch 17/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1772 - accuracy: 0.9384 - val_loss: 0.3299 - val_accuracy: 0.8868\n",
      "Epoch 18/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1812 - accuracy: 0.9318 - val_loss: 0.4435 - val_accuracy: 0.8491\n",
      "Epoch 19/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.2167 - accuracy: 0.9176 - val_loss: 0.4196 - val_accuracy: 0.9009\n",
      "Epoch 20/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1593 - accuracy: 0.9422 - val_loss: 0.5869 - val_accuracy: 0.8443\n",
      "Epoch 21/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.3273 - accuracy: 0.8911 - val_loss: 0.4631 - val_accuracy: 0.8302\n",
      "Epoch 22/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1776 - accuracy: 0.9347 - val_loss: 0.4112 - val_accuracy: 0.9057\n",
      "Epoch 23/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1897 - accuracy: 0.9290 - val_loss: 0.4972 - val_accuracy: 0.8821\n",
      "Epoch 24/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.2261 - accuracy: 0.9119 - val_loss: 0.3703 - val_accuracy: 0.9009\n",
      "Epoch 25/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1812 - accuracy: 0.9347 - val_loss: 0.3221 - val_accuracy: 0.9009\n",
      "Epoch 26/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1529 - accuracy: 0.9536 - val_loss: 0.3070 - val_accuracy: 0.9057\n",
      "Epoch 27/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1972 - accuracy: 0.9337 - val_loss: 0.3393 - val_accuracy: 0.9151\n",
      "Epoch 28/1000\n",
      "33/33 [==============================] - 1s 35ms/step - loss: 0.1311 - accuracy: 0.9479 - val_loss: 0.2740 - val_accuracy: 0.9245\n",
      "Epoch 29/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1671 - accuracy: 0.9337 - val_loss: 0.3494 - val_accuracy: 0.8868\n",
      "Epoch 30/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1955 - accuracy: 0.9403 - val_loss: 0.3224 - val_accuracy: 0.8774\n",
      "Epoch 31/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2145 - accuracy: 0.9138 - val_loss: 0.3519 - val_accuracy: 0.9198\n",
      "Epoch 32/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.2151 - accuracy: 0.9299 - val_loss: 0.2970 - val_accuracy: 0.9151\n",
      "Epoch 33/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1446 - accuracy: 0.9489 - val_loss: 0.2489 - val_accuracy: 0.9245\n",
      "Epoch 34/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0967 - accuracy: 0.9650 - val_loss: 0.3427 - val_accuracy: 0.8868\n",
      "Epoch 35/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.1328 - accuracy: 0.9508 - val_loss: 0.3509 - val_accuracy: 0.8726\n",
      "Epoch 36/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1346 - accuracy: 0.9536 - val_loss: 0.4229 - val_accuracy: 0.8962\n",
      "Epoch 37/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0878 - accuracy: 0.9678 - val_loss: 0.3455 - val_accuracy: 0.9057\n",
      "Epoch 38/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1035 - accuracy: 0.9621 - val_loss: 0.2604 - val_accuracy: 0.9245\n",
      "Epoch 39/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0998 - accuracy: 0.9621 - val_loss: 0.3699 - val_accuracy: 0.9198\n",
      "Epoch 40/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1978 - accuracy: 0.9328 - val_loss: 0.3403 - val_accuracy: 0.9009\n",
      "Epoch 41/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1166 - accuracy: 0.9545 - val_loss: 0.2761 - val_accuracy: 0.9198\n",
      "Epoch 42/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1628 - accuracy: 0.9422 - val_loss: 0.3540 - val_accuracy: 0.8915\n",
      "Epoch 43/1000\n",
      "33/33 [==============================] - 1s 37ms/step - loss: 0.1459 - accuracy: 0.9451 - val_loss: 0.3214 - val_accuracy: 0.8962\n",
      "Epoch 44/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0911 - accuracy: 0.9631 - val_loss: 0.3183 - val_accuracy: 0.8915\n",
      "Epoch 45/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0700 - accuracy: 0.9688 - val_loss: 0.4243 - val_accuracy: 0.8821\n",
      "Epoch 46/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1186 - accuracy: 0.9574 - val_loss: 0.3263 - val_accuracy: 0.9009\n",
      "Epoch 47/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1065 - accuracy: 0.9593 - val_loss: 0.4017 - val_accuracy: 0.9104\n",
      "Epoch 48/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.2167 - accuracy: 0.9290 - val_loss: 0.2843 - val_accuracy: 0.9104\n",
      "Epoch 49/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1279 - accuracy: 0.9564 - val_loss: 0.3182 - val_accuracy: 0.9151\n",
      "Epoch 50/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1195 - accuracy: 0.9583 - val_loss: 0.3324 - val_accuracy: 0.9104\n",
      "Epoch 51/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0963 - accuracy: 0.9640 - val_loss: 0.4077 - val_accuracy: 0.8821\n",
      "Epoch 52/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0885 - accuracy: 0.9678 - val_loss: 0.2886 - val_accuracy: 0.9292\n",
      "Epoch 53/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1340 - accuracy: 0.9508 - val_loss: 0.5794 - val_accuracy: 0.8491\n",
      "Epoch 54/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.2360 - accuracy: 0.9167 - val_loss: 0.3492 - val_accuracy: 0.9057\n",
      "Epoch 55/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1034 - accuracy: 0.9669 - val_loss: 0.3329 - val_accuracy: 0.9057\n",
      "Epoch 56/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0765 - accuracy: 0.9659 - val_loss: 0.3721 - val_accuracy: 0.9340\n",
      "Epoch 57/1000\n",
      "33/33 [==============================] - 2s 70ms/step - loss: 0.1584 - accuracy: 0.9460 - val_loss: 0.4030 - val_accuracy: 0.9245\n",
      "Epoch 58/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.2313 - accuracy: 0.9261 - val_loss: 0.3964 - val_accuracy: 0.8868\n",
      "Epoch 59/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1223 - accuracy: 0.9545 - val_loss: 0.3804 - val_accuracy: 0.9292\n",
      "Epoch 60/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0836 - accuracy: 0.9688 - val_loss: 0.4268 - val_accuracy: 0.9057\n",
      "Epoch 61/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0996 - accuracy: 0.9640 - val_loss: 0.2868 - val_accuracy: 0.9292\n",
      "Epoch 62/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0608 - accuracy: 0.9744 - val_loss: 0.2972 - val_accuracy: 0.9245\n",
      "Epoch 63/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0486 - accuracy: 0.9801 - val_loss: 0.2878 - val_accuracy: 0.9292\n",
      "Epoch 64/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0891 - accuracy: 0.9706 - val_loss: 0.4454 - val_accuracy: 0.9009\n",
      "Epoch 65/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1122 - accuracy: 0.9602 - val_loss: 0.4365 - val_accuracy: 0.9009\n",
      "Epoch 66/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0768 - accuracy: 0.9716 - val_loss: 0.3614 - val_accuracy: 0.9104\n",
      "Epoch 67/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0679 - accuracy: 0.9763 - val_loss: 0.3035 - val_accuracy: 0.9151\n",
      "Epoch 68/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0480 - accuracy: 0.9830 - val_loss: 0.3713 - val_accuracy: 0.9292\n",
      "Epoch 69/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1000 - accuracy: 0.9631 - val_loss: 0.3959 - val_accuracy: 0.9104\n",
      "Epoch 70/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1187 - accuracy: 0.9527 - val_loss: 0.3675 - val_accuracy: 0.9057\n",
      "Epoch 71/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0701 - accuracy: 0.9801 - val_loss: 0.2534 - val_accuracy: 0.9340\n",
      "Epoch 72/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0932 - accuracy: 0.9697 - val_loss: 0.2502 - val_accuracy: 0.9387\n",
      "Epoch 73/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0589 - accuracy: 0.9782 - val_loss: 0.3521 - val_accuracy: 0.9151\n",
      "Epoch 74/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0733 - accuracy: 0.9782 - val_loss: 0.3726 - val_accuracy: 0.9151\n",
      "Epoch 75/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0864 - accuracy: 0.9735 - val_loss: 0.3363 - val_accuracy: 0.9340\n",
      "Epoch 76/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1057 - accuracy: 0.9669 - val_loss: 0.3396 - val_accuracy: 0.9245\n",
      "Epoch 77/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1306 - accuracy: 0.9574 - val_loss: 0.3908 - val_accuracy: 0.9104\n",
      "Epoch 78/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1334 - accuracy: 0.9621 - val_loss: 0.2677 - val_accuracy: 0.9481\n",
      "Epoch 79/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0913 - accuracy: 0.9678 - val_loss: 0.3551 - val_accuracy: 0.9245\n",
      "Epoch 80/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0936 - accuracy: 0.9744 - val_loss: 0.3731 - val_accuracy: 0.9198\n",
      "Epoch 81/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0941 - accuracy: 0.9602 - val_loss: 0.3041 - val_accuracy: 0.9292\n",
      "Epoch 82/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1025 - accuracy: 0.9659 - val_loss: 0.3534 - val_accuracy: 0.9104\n",
      "Epoch 83/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1216 - accuracy: 0.9508 - val_loss: 0.6057 - val_accuracy: 0.8774\n",
      "Epoch 84/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.2247 - accuracy: 0.9299 - val_loss: 0.3826 - val_accuracy: 0.8962\n",
      "Epoch 85/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1624 - accuracy: 0.9489 - val_loss: 0.2821 - val_accuracy: 0.9057\n",
      "Epoch 86/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1100 - accuracy: 0.9574 - val_loss: 0.3302 - val_accuracy: 0.9198\n",
      "Epoch 87/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0780 - accuracy: 0.9754 - val_loss: 0.3499 - val_accuracy: 0.9387\n",
      "Epoch 88/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0726 - accuracy: 0.9754 - val_loss: 0.2815 - val_accuracy: 0.9198\n",
      "Epoch 89/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0828 - accuracy: 0.9678 - val_loss: 0.4120 - val_accuracy: 0.8915\n",
      "Epoch 90/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0892 - accuracy: 0.9697 - val_loss: 0.3066 - val_accuracy: 0.9387\n",
      "Epoch 91/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0863 - accuracy: 0.9697 - val_loss: 0.3292 - val_accuracy: 0.9198\n",
      "Epoch 92/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0896 - accuracy: 0.9725 - val_loss: 0.3983 - val_accuracy: 0.9292\n",
      "Epoch 93/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1857 - accuracy: 0.9460 - val_loss: 0.3118 - val_accuracy: 0.9151\n",
      "Epoch 94/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0574 - accuracy: 0.9820 - val_loss: 0.3310 - val_accuracy: 0.9387\n",
      "Epoch 95/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0469 - accuracy: 0.9858 - val_loss: 0.3560 - val_accuracy: 0.9198\n",
      "Epoch 96/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0372 - accuracy: 0.9839 - val_loss: 0.3919 - val_accuracy: 0.9292\n",
      "Epoch 97/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1027 - accuracy: 0.9602 - val_loss: 0.3318 - val_accuracy: 0.9387\n",
      "Epoch 98/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1912 - accuracy: 0.9432 - val_loss: 0.3886 - val_accuracy: 0.9198\n",
      "Epoch 99/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1328 - accuracy: 0.9593 - val_loss: 0.4415 - val_accuracy: 0.8821\n",
      "Epoch 100/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0602 - accuracy: 0.9735 - val_loss: 0.3340 - val_accuracy: 0.9104\n",
      "Epoch 101/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0470 - accuracy: 0.9839 - val_loss: 0.3362 - val_accuracy: 0.9198\n",
      "Epoch 102/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0455 - accuracy: 0.9848 - val_loss: 0.2978 - val_accuracy: 0.9104\n",
      "Epoch 103/1000\n",
      "33/33 [==============================] - 3s 98ms/step - loss: 0.0253 - accuracy: 0.9934 - val_loss: 0.3217 - val_accuracy: 0.9292\n",
      "Epoch 104/1000\n",
      "33/33 [==============================] - 1s 40ms/step - loss: 0.0259 - accuracy: 0.9896 - val_loss: 0.3531 - val_accuracy: 0.9387\n",
      "Epoch 105/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0860 - accuracy: 0.9763 - val_loss: 0.3268 - val_accuracy: 0.9198\n",
      "Epoch 106/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1033 - accuracy: 0.9621 - val_loss: 0.3003 - val_accuracy: 0.9104\n",
      "Epoch 107/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0879 - accuracy: 0.9650 - val_loss: 0.4074 - val_accuracy: 0.9151\n",
      "Epoch 108/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1074 - accuracy: 0.9631 - val_loss: 0.3654 - val_accuracy: 0.9198\n",
      "Epoch 109/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1058 - accuracy: 0.9631 - val_loss: 0.4088 - val_accuracy: 0.8679\n",
      "Epoch 110/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0911 - accuracy: 0.9669 - val_loss: 0.2807 - val_accuracy: 0.9387\n",
      "Epoch 111/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0699 - accuracy: 0.9754 - val_loss: 0.3943 - val_accuracy: 0.9104\n",
      "Epoch 112/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0674 - accuracy: 0.9754 - val_loss: 0.4182 - val_accuracy: 0.9057\n",
      "Epoch 113/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0744 - accuracy: 0.9744 - val_loss: 0.3327 - val_accuracy: 0.9292\n",
      "Epoch 114/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0409 - accuracy: 0.9867 - val_loss: 0.3226 - val_accuracy: 0.9198\n",
      "Epoch 115/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0278 - accuracy: 0.9877 - val_loss: 0.3197 - val_accuracy: 0.9292\n",
      "Epoch 116/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0270 - accuracy: 0.9886 - val_loss: 0.2637 - val_accuracy: 0.9387\n",
      "Epoch 117/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0224 - accuracy: 0.9934 - val_loss: 0.3912 - val_accuracy: 0.9245\n",
      "Epoch 118/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0149 - accuracy: 0.9943 - val_loss: 0.3564 - val_accuracy: 0.9340\n",
      "Epoch 119/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0246 - accuracy: 0.9905 - val_loss: 0.3273 - val_accuracy: 0.9340\n",
      "Epoch 120/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0229 - accuracy: 0.9915 - val_loss: 0.3836 - val_accuracy: 0.9340\n",
      "Epoch 121/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0360 - accuracy: 0.9858 - val_loss: 0.4043 - val_accuracy: 0.9292\n",
      "Epoch 122/1000\n",
      "33/33 [==============================] - 1s 42ms/step - loss: 0.1133 - accuracy: 0.9640 - val_loss: 0.5253 - val_accuracy: 0.9340\n",
      "Epoch 123/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.1598 - accuracy: 0.9527 - val_loss: 0.4388 - val_accuracy: 0.8915\n",
      "Epoch 124/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1249 - accuracy: 0.9602 - val_loss: 0.3570 - val_accuracy: 0.9057\n",
      "Epoch 125/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1300 - accuracy: 0.9640 - val_loss: 0.3910 - val_accuracy: 0.8915\n",
      "Epoch 126/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0824 - accuracy: 0.9782 - val_loss: 0.3096 - val_accuracy: 0.9340\n",
      "Epoch 127/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0482 - accuracy: 0.9830 - val_loss: 0.4330 - val_accuracy: 0.9151\n",
      "Epoch 128/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1180 - accuracy: 0.9564 - val_loss: 0.4317 - val_accuracy: 0.9057\n",
      "Epoch 129/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.2760 - accuracy: 0.9167 - val_loss: 0.5023 - val_accuracy: 0.8726\n",
      "Epoch 130/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.1979 - accuracy: 0.9422 - val_loss: 0.3234 - val_accuracy: 0.9151\n",
      "Epoch 131/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1032 - accuracy: 0.9612 - val_loss: 0.4276 - val_accuracy: 0.9057\n",
      "Epoch 132/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1005 - accuracy: 0.9754 - val_loss: 0.3852 - val_accuracy: 0.9340\n",
      "Epoch 133/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0600 - accuracy: 0.9773 - val_loss: 0.4217 - val_accuracy: 0.9292\n",
      "Epoch 134/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0801 - accuracy: 0.9763 - val_loss: 0.3814 - val_accuracy: 0.9387\n",
      "Epoch 135/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0728 - accuracy: 0.9754 - val_loss: 0.4156 - val_accuracy: 0.9245\n",
      "Epoch 136/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0269 - accuracy: 0.9943 - val_loss: 0.4377 - val_accuracy: 0.9198\n",
      "Epoch 137/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0229 - accuracy: 0.9915 - val_loss: 0.4460 - val_accuracy: 0.9292\n",
      "Epoch 138/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0307 - accuracy: 0.9886 - val_loss: 0.4863 - val_accuracy: 0.9057\n",
      "Epoch 139/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0263 - accuracy: 0.9924 - val_loss: 0.4914 - val_accuracy: 0.9198\n",
      "Epoch 140/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0217 - accuracy: 0.9943 - val_loss: 0.4733 - val_accuracy: 0.9292\n",
      "Epoch 141/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0291 - accuracy: 0.9896 - val_loss: 0.3516 - val_accuracy: 0.9387\n",
      "Epoch 142/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0216 - accuracy: 0.9915 - val_loss: 0.3556 - val_accuracy: 0.9387\n",
      "Epoch 143/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0874 - accuracy: 0.9792 - val_loss: 0.3159 - val_accuracy: 0.9340\n",
      "Epoch 144/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1507 - accuracy: 0.9612 - val_loss: 0.3021 - val_accuracy: 0.9292\n",
      "Epoch 145/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0565 - accuracy: 0.9839 - val_loss: 0.3025 - val_accuracy: 0.9340\n",
      "Epoch 146/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0709 - accuracy: 0.9858 - val_loss: 0.4014 - val_accuracy: 0.9198\n",
      "Epoch 147/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0602 - accuracy: 0.9811 - val_loss: 0.3777 - val_accuracy: 0.9292\n",
      "Epoch 148/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0361 - accuracy: 0.9896 - val_loss: 0.3081 - val_accuracy: 0.9292\n",
      "Epoch 149/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0290 - accuracy: 0.9905 - val_loss: 0.3470 - val_accuracy: 0.9245\n",
      "Epoch 150/1000\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0214 - accuracy: 0.9905 - val_loss: 0.3680 - val_accuracy: 0.9340\n",
      "Epoch 151/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.4458 - val_accuracy: 0.9340\n",
      "Epoch 152/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0528 - accuracy: 0.9848 - val_loss: 0.4331 - val_accuracy: 0.9104\n",
      "Epoch 153/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0685 - accuracy: 0.9811 - val_loss: 0.3901 - val_accuracy: 0.9151\n",
      "Epoch 154/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.1085 - accuracy: 0.9650 - val_loss: 0.4051 - val_accuracy: 0.9057\n",
      "Epoch 155/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0889 - accuracy: 0.9678 - val_loss: 0.3574 - val_accuracy: 0.9245\n",
      "Epoch 156/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1087 - accuracy: 0.9706 - val_loss: 0.3379 - val_accuracy: 0.9292\n",
      "Epoch 157/1000\n",
      "33/33 [==============================] - 1s 31ms/step - loss: 0.0830 - accuracy: 0.9678 - val_loss: 0.3383 - val_accuracy: 0.9104\n",
      "Epoch 158/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0454 - accuracy: 0.9867 - val_loss: 0.4879 - val_accuracy: 0.9198\n",
      "Epoch 159/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0282 - accuracy: 0.9924 - val_loss: 0.3854 - val_accuracy: 0.9245\n",
      "Epoch 160/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0529 - accuracy: 0.9839 - val_loss: 0.5379 - val_accuracy: 0.9057\n",
      "Epoch 161/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0766 - accuracy: 0.9725 - val_loss: 0.4900 - val_accuracy: 0.9198\n",
      "Epoch 162/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0841 - accuracy: 0.9678 - val_loss: 0.5423 - val_accuracy: 0.9104\n",
      "Epoch 163/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.1253 - accuracy: 0.9612 - val_loss: 0.4152 - val_accuracy: 0.9151\n",
      "Epoch 164/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0792 - accuracy: 0.9706 - val_loss: 0.2832 - val_accuracy: 0.9292\n",
      "Epoch 165/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0498 - accuracy: 0.9820 - val_loss: 0.4018 - val_accuracy: 0.9387\n",
      "Epoch 166/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1259 - accuracy: 0.9602 - val_loss: 0.3373 - val_accuracy: 0.9151\n",
      "Epoch 167/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0914 - accuracy: 0.9697 - val_loss: 0.3710 - val_accuracy: 0.9057\n",
      "Epoch 168/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0589 - accuracy: 0.9820 - val_loss: 0.2897 - val_accuracy: 0.9387\n",
      "Epoch 169/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0461 - accuracy: 0.9858 - val_loss: 0.3071 - val_accuracy: 0.9387\n",
      "Epoch 170/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0330 - accuracy: 0.9915 - val_loss: 0.2660 - val_accuracy: 0.9387\n",
      "Epoch 171/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.0263 - accuracy: 0.9877 - val_loss: 0.2833 - val_accuracy: 0.9481\n",
      "Epoch 172/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0254 - accuracy: 0.9905 - val_loss: 0.2522 - val_accuracy: 0.9481\n",
      "Epoch 173/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0273 - accuracy: 0.9896 - val_loss: 0.2943 - val_accuracy: 0.9387\n",
      "Epoch 174/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0280 - accuracy: 0.9915 - val_loss: 0.3724 - val_accuracy: 0.9340\n",
      "Epoch 175/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0224 - accuracy: 0.9915 - val_loss: 0.3044 - val_accuracy: 0.9387\n",
      "Epoch 176/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0247 - accuracy: 0.9896 - val_loss: 0.3062 - val_accuracy: 0.9340\n",
      "Epoch 177/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0349 - accuracy: 0.9905 - val_loss: 0.3204 - val_accuracy: 0.9387\n",
      "Epoch 178/1000\n",
      "33/33 [==============================] - 1s 34ms/step - loss: 0.0936 - accuracy: 0.9782 - val_loss: 0.2980 - val_accuracy: 0.9387\n",
      "Epoch 179/1000\n",
      "33/33 [==============================] - 1s 32ms/step - loss: 0.0620 - accuracy: 0.9773 - val_loss: 0.3611 - val_accuracy: 0.9104\n",
      "Epoch 180/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0918 - accuracy: 0.9688 - val_loss: 0.4059 - val_accuracy: 0.9057\n",
      "Epoch 181/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0522 - accuracy: 0.9792 - val_loss: 0.3996 - val_accuracy: 0.9340\n",
      "Epoch 182/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1172 - accuracy: 0.9631 - val_loss: 0.3957 - val_accuracy: 0.9340\n",
      "Epoch 183/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1073 - accuracy: 0.9621 - val_loss: 0.3025 - val_accuracy: 0.9292\n",
      "Epoch 184/1000\n",
      "33/33 [==============================] - 1s 30ms/step - loss: 0.1205 - accuracy: 0.9593 - val_loss: 0.3246 - val_accuracy: 0.9198\n",
      "Epoch 185/1000\n",
      "33/33 [==============================] - 1s 28ms/step - loss: 0.0629 - accuracy: 0.9754 - val_loss: 0.4961 - val_accuracy: 0.8962\n",
      "Epoch 186/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1616 - accuracy: 0.9498 - val_loss: 0.3260 - val_accuracy: 0.9057\n",
      "Epoch 187/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.1245 - accuracy: 0.9612 - val_loss: 0.4364 - val_accuracy: 0.9151\n",
      "Epoch 188/1000\n",
      "33/33 [==============================] - 1s 29ms/step - loss: 0.0692 - accuracy: 0.9792 - val_loss: 0.3426 - val_accuracy: 0.9292\n",
      "Epoch 189/1000\n",
      "32/33 [============================>.] - ETA: 0s - loss: 0.0355 - accuracy: 0.9893\n",
      "Reached 99% accuracy so cancelling training!\n",
      "33/33 [==============================] - 1s 33ms/step - loss: 0.0348 - accuracy: 0.9896 - val_loss: 0.3223 - val_accuracy: 0.9575\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f5c01bfca0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Train the Model\n",
    "model.fit(x_train, y_train, epochs=1000, batch_size=32, validation_data=(x_test, y_test), callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "hearing-target",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Saved into model_SIBI.h5\n"
     ]
    }
   ],
   "source": [
    "#Saving the model into H5 system file\n",
    "save_model = \"model_SIBI.h5\"\n",
    "model.save(save_model)\n",
    "print(\"Model Saved into\", save_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "narrow-accent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Already saved a model, cleaning up\n",
      "\n",
      "INFO:tensorflow:Assets written to: server_model_SIBI\\1\\assets\n",
      "\n",
      "export_path = server_model_SIBI\\1\n",
      "total 340\n",
      "drwxr-xr-x 1 afkaa afkaa      0 Dec 21 12:35 assets\n",
      "-rw-r--r-- 1 afkaa afkaa 342057 Dec 21 12:35 saved_model.pb\n",
      "drwxr-xr-x 1 afkaa afkaa      0 Dec 21 12:35 variables\n"
     ]
    }
   ],
   "source": [
    "#Saving the model for TF-Serving Type\n",
    "import os\n",
    "\n",
    "MODEL_DIR = \"server_model_SIBI\"\n",
    "\n",
    "version = 1\n",
    "\n",
    "export_path = os.path.join(MODEL_DIR, str(version))\n",
    "\n",
    "if os.path.isdir(export_path):\n",
    "    print('\\nAlready saved a model, cleaning up\\n')\n",
    "    !rm -r {export_path}\n",
    "\n",
    "model.save(export_path, save_format=\"tf\")\n",
    "\n",
    "print('\\nexport_path = {}'.format(export_path))\n",
    "!ls -l {export_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "emotional-feelings",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 63, 1)\n",
      "[[[ 2.991]\n",
      "  [ 6.479]\n",
      "  [ 0.   ]\n",
      "  [ 2.221]\n",
      "  [ 5.938]\n",
      "  [-0.   ]\n",
      "  [ 1.711]\n",
      "  [ 4.989]\n",
      "  [-0.   ]\n",
      "  [ 1.977]\n",
      "  [ 4.178]\n",
      "  [-0.001]\n",
      "  [ 2.819]\n",
      "  [ 3.851]\n",
      "  [-0.001]\n",
      "  [ 2.324]\n",
      "  [ 3.676]\n",
      "  [-0.   ]\n",
      "  [ 2.196]\n",
      "  [ 2.853]\n",
      "  [-0.   ]\n",
      "  [ 2.278]\n",
      "  [ 3.302]\n",
      "  [-0.001]\n",
      "  [ 2.429]\n",
      "  [ 3.784]\n",
      "  [-0.001]\n",
      "  [ 2.97 ]\n",
      "  [ 3.499]\n",
      "  [-0.   ]\n",
      "  [ 3.102]\n",
      "  [ 2.235]\n",
      "  [-0.   ]\n",
      "  [ 3.286]\n",
      "  [ 1.507]\n",
      "  [-0.   ]\n",
      "  [ 3.471]\n",
      "  [ 0.808]\n",
      "  [-0.001]\n",
      "  [ 3.516]\n",
      "  [ 3.659]\n",
      "  [-0.   ]\n",
      "  [ 3.673]\n",
      "  [ 2.502]\n",
      "  [-0.   ]\n",
      "  [ 3.765]\n",
      "  [ 1.799]\n",
      "  [-0.   ]\n",
      "  [ 3.843]\n",
      "  [ 1.136]\n",
      "  [-0.001]\n",
      "  [ 4.08 ]\n",
      "  [ 4.077]\n",
      "  [-0.   ]\n",
      "  [ 4.168]\n",
      "  [ 3.108]\n",
      "  [-0.   ]\n",
      "  [ 4.199]\n",
      "  [ 2.529]\n",
      "  [-0.   ]\n",
      "  [ 4.226]\n",
      "  [ 1.994]\n",
      "  [-0.   ]]]\n"
     ]
    }
   ],
   "source": [
    "#Testing the Model\n",
    "input_test = [[[762.6636624336243], [1652.198314666748], [1.0169689090844258e-07],\n",
    "               [566.4011240005493], [1514.1772031784058], [-0.06466735899448395],\n",
    "               [436.24624609947205], [1272.1819877624512], [-0.10941988229751587],\n",
    "               [504.03743982315063], [1065.2660131454468], [-0.1551685929298401],\n",
    "               [718.9680337905884], [981.9985628128052], [-0.19297298789024353],\n",
    "               [592.5631523132324], [937.2789859771729], [-0.024738160893321037],\n",
    "               [559.9795579910278], [727.5906205177307], [-0.1174304261803627],\n",
    "               [581.0156464576721], [841.9082164764404], [-0.19315676391124725],\n",
    "               [619.5127964019775], [965.0120735168457], [-0.23132655024528503],\n",
    "               [757.310152053833], [892.2686576843262], [-0.030217956751585007],\n",
    "               [790.9350991249084], [569.8509216308594], [-0.07613588869571686],\n",
    "               [838.0098938941956], [384.23940539360046], [-0.11438910663127899],\n",
    "               [885.1863145828247], [206.1316967010498], [-0.14229716360569], \n",
    "               [896.6857194900513], [933.0331087112427], [-0.053754180669784546],\n",
    "               [936.7329478263855], [638.064980506897], [-0.09413756430149078], \n",
    "               [960.0444436073303], [458.6908519268036], [-0.11900665611028671],\n",
    "               [980.0054430961609], [289.8055911064148], [-0.1342657208442688], \n",
    "               [1040.4841899871826], [1039.6770238876343], [-0.08633966743946075],\n",
    "               [1062.9222393035889], [792.4231886863708], [-0.10331644117832184],\n",
    "               [1070.7426071166992], [644.8165774345398], [-0.10464464873075485],\n",
    "               [1077.575922012329], [508.50069522857666], [-0.10636621713638306]]]\n",
    "input_test = np.array(input_test)\n",
    "input_test = input_test/255.0\n",
    "input = np.reshape(input_test, (input_test.shape[0], input_test.shape[1], 1))\n",
    "print(input_test.shape)\n",
    "print(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "flush-intent",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0.]]\n",
      "WARNING:tensorflow:From <ipython-input-48-ce52a49e73f2>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
      "[5]\n"
     ]
    }
   ],
   "source": [
    "#Print the Prediction\n",
    "print(model.predict(input_test))\n",
    "print(model.predict_classes(input_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "systematic-warrior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n"
     ]
    }
   ],
   "source": [
    "classes = {\n",
    "    'A': 0,\n",
    "    'B': 1,\n",
    "    'C': 2,\n",
    "    'D': 3,\n",
    "    'E': 4,\n",
    "    'F': 5,\n",
    "    'G': 6,\n",
    "    'H': 7,\n",
    "    'I': 8,\n",
    "    'J': 9,\n",
    "    'K': 10,\n",
    "    'L': 11,\n",
    "    'M': 12,\n",
    "    'N': 13,\n",
    "    'O': 14,\n",
    "    'P': 15,\n",
    "    'Q': 16,\n",
    "    'R': 17,\n",
    "    'S': 18,\n",
    "    'T': 19,\n",
    "    'U': 20,\n",
    "    'V': 21,\n",
    "    'W': 22,\n",
    "    'X': 23,\n",
    "    'Y': 24,\n",
    "    'Z': 25\n",
    "}\n",
    "\n",
    "predictions = model.predict_classes(input_test)\n",
    "for alphabets, values in classes.items():\n",
    "    if values == predictions[0] :\n",
    "        print(alphabets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liked-measurement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
